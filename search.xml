<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Python3.5.6 multiprocessing 用户文档]]></title>
    <url>%2F2018%2F11%2F16%2FPython3-5-6-multiprocessing-%E7%94%A8%E6%88%B7%E6%96%87%E6%A1%A3%2F</url>
    <content type="text"><![CDATA[最近在做 Python 内存优化时，发现 multiprocessing 可以充分利用 CPU 的多核特性，且可以在执行完任务后释放所有的资源，只保留需要的结果即可，可以避免 Python 的垃圾回收过慢的弊病，同时带来执行效率的提升。同时，之前的 Python 资料书籍更关注于 Python 多线程部分，而对多进程的讲解应用较少。于是开始阅读该部分文档和源码，本篇博客是其官网文档 multiprocessing 的翻译（原文地址）。 Introductionmultiprocessing 是一个支持产生新进程的 package，API 使用方式与 threading 模块相似。multiprocessing 可以提供本地/远程的线程并发，通过使用子进程而不是线程绕开 GIL 的约束（充分利用 CPU 多核提升效率）。因此，multiprocessing 模块可以支持程序充分利用给定计算机上的多个处理器。Unix/Windows 环境均可以运行。 multiprocessing同样引入了 threading中没有的 APIs。一个主要的例子如 Pool，提供了便捷的方法实现跨多个输入值的函数并行执行，可以跨进程分配输入的数据（数据并行）。下面的例子演示Pool在模块中定义这样一个函数的常见用法，以便子进程可以成功导入该模块，实现数据的并行性。 12345678from multiprocessing import Pooldef f(x): return x*xif __name__ == '__main__': with Pool(5) as p: print(p.map(f, [1, 2, 3])) 打印结果如下 1[1, 4, 9] Process 类multiprocessing通过创建一个 Process 对象调用其 start() 方法产生子进程。Process 的 API 与 threading.Thread一致，一个多进程程序的例子如下所示。 123456789from multiprocessing import Processdef f(name): print('hello', name)if __name__ == '__main__': p = Process(target=f, args=('bob',)) p.start() p.join() 为了显示每个进程的独立 ID，一个扩展的程序实例如下。 123456789101112131415161718from multiprocessing import Processimport osdef info(title): print(title) print('module name:', __name__) print('parent process:', os.getppid()) print('process id:', os.getpid())def f(name): info('function f') print('hello', name)if __name__ == '__main__': info('main line') p = Process(target=f, args=('bob',)) p.start() p.join() 如果读者需要了解 if __name__ == &#39;__main__&#39;，查看 Programming Guidlines。 Context 与 start 方法依据运行环境的不同（Unix/Windows），multiprocessing 支持三种方式创建子进程，这三种方式是 spawn 父进程开启一个全新的 Python 解释器进程。子进程仅继承运行进程对象的 run() 方法所必须的资源。特别的是，不会从父进程中继承不需要的文件描述和文件句柄。与下面两种方法相比，此方法启动比较慢。 Unix 与 Windows 环境均存在该方法，Windows 下默认使用该方法。 fork 父进程使用 os.fork() 复制 Python 解释器。子进程开始运行时实际是与父进程一样的。父进程的所有资源均会被子进程继承。注意拷贝一个多线程的进程可能会有问题。 仅 Unix 环境存在该方法，Unix 默认使用该方法。 forkserver 当程序开始运行并选择了 forkserver 方法， 仅支持 Unix 管道通信传输文件描述符的 Unix 平台支持该方法。 在3.4版本之后，所有的 Unix 环境支持了spawn 方法，部分 Unix 环境支持了 forkserver 方法，Windows 环境下子进程不再继承父进程的所有可继承的句柄。 Unix 环境下使用 spawn 或 forkserver 方法同样会开启一个信号量跟踪器进程跟踪由程序创建的未连接的命名信号量。当所有的进程退出时，信号量跟踪器取消链接任何剩余的信号量。一般情况下是没有的，但如果一个进程被信号量杀死，可能会有信号量泄漏。（没有链接的命名信号量是一个严重的问题，因为操作系统只允许有限数量的信号量且直至下次重启不会主动取消链接。） 猜测：该方式只拷贝了父进程的信号量而没有拷贝文件描述符，因此是未连接的命名信号量，文件描述符或句柄由父进程的信号量连接。正常情况下，进程结束时，信号跟踪器会负责销毁该信号量，当意外退出或被 kill -9 杀死时，未能完成信号量的回收操作导致信号量泄漏。 可以在主函数的 if __name__ == &#39;__main__&#39; 从句中使用 set_start_method() 方法选择启动方法，如下实例。 123456789101112import multiprocessing as mpdef foo(q): q.put('hello')if __name__ == '__main__': mp.set_start_method('spawn') q = mp.Queue() p = mp.Process(target=foo, args=(q,)) p.start() print(q.get()) p.join() 程序中 set_start_method() 方法至多被使用一次，如像上述实例中，使用错误会报错 1RuntimeError: context has already been set. 或者，可以使用 get_context() 获取一个 context 对象，context 对象作为 multiprocessing 模块的一元，具有同样的设置开启子进程方式的方法，并允许在同一程序中使用多个启动方法。 123456789101112import multiprocessing as mpdef foo(q): q.put('hello')if __name__ == '__main__': ctx = mp.get_context('spawn') q = ctx.Queue() p = ctx.Process(target=foo, args=(q,)) p.start() print(q.get()) p.join() 注：与 context 相关的对象可能与不同 context 的进程不兼容。特别是使用 fork() 方法的 context 创建的锁不能传递给使用 spawn 或 forkserver 方法启动的进程。 使用特别启动方法的库应该使用 get_context() 避免干扰库使用者对启动方法的选择。 进程间信息交换进程间 multiprocessing 支持两种类型的交流通道：Queue 和 Pipe 。 Queue 是差不多克隆自 queue.Queue。代码实例如下 1234567891011from multiprocessing import Process, Queuedef f(q): q.put([42, None, 'hello'])if __name__ == '__main__': q = Queue() p = Process(target=f, args=(q,)) p.start() print(q.get()) # prints "[42, None, 'hello']" p.join() Queue 是线程/进程安全的。 Pipe() 函数返回一对由管道连接的连接对象（默认是双工的）。代码实例如下 123456789101112from multiprocessing import Process, Pipedef f(conn): conn.send([42, None, 'hello']) conn.close()if __name__ == '__main__': parent_conn, child_conn = Pipe() p = Process(target=f, args=(child_conn,)) p.start() print(parent_conn.recv()) # prints "[42, None, 'hello']" p.join() 两个连接对象代表了管道的两端。每一个连接对象都有 send() 和 recv() （彼此间）。 注：如果两个进程（或线程）同时读取或写入管道同一端的数据，则数据可能被损毁。当然，进程使用管道不同端的不存在数据损坏的风险。 进程间同步multiprocessing 模块包含 threading 所有同步原语的等价语句。例如我们可以使用 lock 保证同一时刻只有一个进程在进行标准输出。 1234567891011121314from multiprocessing import Process, Lockdef f(l, i): l.acquire() try: print('hello world', i) finally: l.release()if __name__ == '__main__': lock = Lock() for num in range(10): Process(target=f, args=(lock, num)).start() 如果不使用 lock ，来自不同进程的输出可能会混淆不清。 进程间状态共享如上所述，在进行并发编程时，尽量不适用状态共享，多线程编程时尤其如此。 然而，如果你确实需要共享数据，multiprocessing 提供了两种方式实现。 内存共享数据可以以 Value 或者 Array 的方式在内存中共享。代码实例如下 1234567891011121314151617from multiprocessing import Process, Value, Arraydef f(n, a): n.value = 3.1415927 for i in range(len(a)): a[i] = -a[i]if __name__ == '__main__': num = Value('d', 0.0) arr = Array('i', range(10)) p = Process(target=f, args=(num, arr)) p.start() p.join() print(num.value) print(arr[:]) 打印结果如下 123.1415927[0, -1, -2, -3, -4, -5, -6, -7, -8, -9] 创建变量 num 和 arr 时实用的参数 ‘d’ 和 ‘i’ 指定数据类型的代码：’d’ 表示双精度浮点型，’i’ 表示有符号整型。共享对象是进程/线程安全的。 为了更灵活地使用内存共享，可以使用 multiprocessing.sharedctypes 模块，该模块支持从共享内存分配出来的任意 ctypes 对象。 服务进程Manager() 返回的 manager 对象管理一个服务器进程持有 Python 对象且允许其他进程使用代理操作这些对象，支持 list, dict, Namespace, Lock, RLock, Semaphore, BoundedSemaphore, Condition, Event, Barrier, Queue, Value 和 Array。代码示例如下 12345678910111213141516171819from multiprocessing import Process, Managerdef f(d, l): d[1] = '1' d['2'] = 2 d[0.25] = None l.reverse()if __name__ == '__main__': with Manager() as manager: d = manager.dict() l = manager.list(range(10)) p = Process(target=f, args=(d, l)) p.start() p.join() print(d) print(l) 打印结果如下 12&#123;0.25: None, 1: &apos;1&apos;, &apos;2&apos;: 2&#125;[9, 8, 7, 6, 5, 4, 3, 2, 1, 0] 使用服务进程 manager 比使用内存共享更灵活，因为 manager 可以支持任意对象类型。此外，单个 manager 可以通过网络在不同计算机上实现进程共享。但是，manager 方式比使用内存共享慢。 使用 Pool workersPool 对象表示一个进程池，其允许以几种不同的方式将任务装载到进程池。代码示例如下 1234567891011121314151617181920212223242526272829303132333435363738394041from multiprocessing import Pool, TimeoutErrorimport timeimport osdef f(x): return x*xif __name__ == '__main__': # start 4 worker processes with Pool(processes=4) as pool: # print "[0, 1, 4,..., 81]" print(pool.map(f, range(10))) # print same numbers in arbitrary order for i in pool.imap_unordered(f, range(10)): print(i) # evaluate "f(20)" asynchronously res = pool.apply_async(f, (20,)) # runs in *only* one process print(res.get(timeout=1)) # prints "400" # evaluate "os.getpid()" asynchronously res = pool.apply_async(os.getpid, ()) # runs in *only* one process print(res.get(timeout=1)) # prints the PID of that process # launching multiple evaluations asynchronously *may* use more processes multiple_results = [pool.apply_async(os.getpid, ()) for i in range(4)] print([res.get(timeout=1) for res in multiple_results]) # make a single worker sleep for 10 secs res = pool.apply_async(time.sleep, (10,)) try: print(res.get(timeout=1)) except TimeoutError: print("We lacked patience and got a multiprocessing.TimeoutError") print("For the moment, the pool remains available for more work") # exiting the 'with'-block has stopped the pool print("Now the pool is closed and no longer available") 注：Pool 的方法只能由创建它的进程使用。 Tips: 该包中的功能要求 __main__ 模块可由子项导入。在 Programming Guidelines 中有所涉及，但值得在此指出。这意味着某些例子，如下 multiprocessing.pool.Pool 的实例在交互式解释器中不起作用。 1234567891011121314&gt; &gt;&gt;&gt; from multiprocessing import Pool&gt; &gt;&gt;&gt; p = Pool(5)&gt; &gt;&gt;&gt; def f(x):&gt; ... return x*x&gt; ...&gt; &gt;&gt;&gt; p.map(f, [1,2,3])&gt; Process PoolWorker-1:&gt; Process PoolWorker-2:&gt; Process PoolWorker-3:&gt; Traceback (most recent call last):&gt; AttributeError: 'module' object has no attribute 'f'&gt; AttributeError: 'module' object has no attribute 'f'&gt; AttributeError: 'module' object has no attribute 'f'&gt; 如果你尝试运行该代码，会以半随机的方式输出三个完整的回溯，然后不能不以某种方式停止主进程。 Referencemultiprocessing 模块主要复制了 threading 模块的 API。 Process &amp;&amp; Exceptionsmultiprocessing.Process(group=None, target=None, name=None, args=(), kwargs={}, *, daemon=None) Process 对象表示另一个进程中的活动。Process 类具有 threading.Thread 的所有方法的等价方法。 应该始终使用关键字参数调用构造函数。参数 group 应该始终是 None，它仅为了与 threading.Thread 构造参数一致。参数 target 是 run() 方法的调用函数，默认为 None，即不调用任何内容。参数 name 是进程的名字，详细情况见下参数部分。kwargs 是目标函数调用的参数关键字字典，若提供了该参数，将关键字参数 daemon 设置为 True/False，若为默认值 None，则从创建该进程的进程继承该参数。 默认情况下，不会将任何参数传递给 target。 如果子类重写构造函数，必须确保在对进程执行任何动作之前先调用 Process.__init__() 方法。 3.3版本后添加了 daemon 参数。 构造函数源码如下所示。 1234567891011121314151617# cpython 3.5.2 /Lib/multiprocessing/process BaseProcess 类def __init__(self, group=None, target=None, name=None, args=(), kwargs=&#123;&#125;, *, daemon=None): assert group is None, 'group argument must be None for now' count = next(_process_counter) self._identity = _current_process._identity + (count,) self._config = _current_process._config.copy() self._parent_pid = os.getpid() self._popen = None self._target = target self._args = tuple(args) self._kwargs = dict(kwargs) self._name = name or type(self).__name__ + '-' + \ ':'.join(str(i) for i in self._identity) if daemon is not None: self.daemon = daemon _dangling.add(self) run() 表示进程动作（即进程的执行任务）的方法。 可以在子类中重写该方法。标准 run() 方法会调用构造函数参数 target 传递进来的目标函数（如果有），分别按顺序使用 args 和 kwargs 参数中的关键字参数。 start() 进程启动的方法。 该方法在每一个进程中至多被调用一次，其在一个单独进程中调用 Process 对象的 run() 方法。 join([timeout]) 若可选参数 timeout 为 None （默认为 None ），该方法会阻塞进程直至调用该方法的进程终止； 若可选参数 timeout 为整数，该方法会阻塞进程 timeout 秒； 注：若方法进程终止或超时，返回 None。可以通过检查进程的 exitcode 属性值确认进程是否终止。 name 进程名称。name 是字符串类型，仅用于识别目的，没有语义。多个进程可以使用相同的名字。 初识名字由构造器设置，若构造器没有提供显示名字，按照 “Process-{N1:N2:…:Nk}” 形式构造名字，其中 “Nk“ 表示父进程的第 Nk 个子进程。 is_alive() 返回进程是否活着。 粗略的说，从进程调用 start() 方法到进程终止之间进程状态时活着的。 daemon 进程后台运行标志，Boolean 类型。必须在调用 start() 方法之前设置 - 这样可以后台运行进程。 初始值继承自父进程（即创建进程）。 当进程退出时，它会尝试其终止其进程的所有子进程。 注：后台运行的进程不允许创建子进程，否则子进程会变成孤儿进程。此外，后台进程不能是 Unix 守护进程或服务，当非守护进程退出时，它们可以正常退出。 除了 threading.Thread API 之外，Process 还支持如下属性或方法 pid 返回进程 ID，进程生成之前返回 None。 exitcode 子进程退出码。 None 表示进程尚未终止；返回负整数 -N 表示进程被信号 N 终止；返回0表示没有错误正常退出；返回正数表示进程有错误，并以错误码为状态码退出。 authkey 程序的身份认证秘钥是一个 byte 类型的字符串。 multiprocessing 初始化时，会使用 os.urandom() 方法为主进程分配一个随机字符串。 当创建 Process 对象时，它会继承父进程的 authkey，可以通过设置 authkey 来改变它。 sentinel 系统对象的数字句柄，当进程结束时状态变为 ready。 一次等待多个事件时可以使用 multiprocessing.connection.wait()，否则使用 join() 方法会更简单。 Windows 环境下这是一个系统句柄调用 WaitForSingleObject 和 WaitForMultipleObjects API 族；Unix 环境下是一个文件描述符，与 select 模块中的原语配合使用。 terminate() 终止进程。Unix 环境下使用 SIGTERM 信号量完成；Windows 环境下调用 TerminatedProcess() 方法实现。但不会执行退出句柄和剩余子句。 注：进程的后裔进程不会被终止，后裔进程将称为孤儿进程。 警告：如果相互关联的进程正在使用管道或队列时，调用了该方法，管道或队列可能会被损毁且无法被其他进程使用。类似的，如果进程已获得了锁或者信号量，终止它可能导致其他进程的死锁。 注：start()，join()，is_alive()，terminate() 和 exitcode 这些方法应该仅由父进程对象调用。 如下是一个 Process 方法的使用实例。 12345678910111213&gt;&gt;&gt; import multiprocessing, time, signal&gt;&gt;&gt; p = multiprocessing.Process(target=time.sleep, args=(1000,))&gt;&gt;&gt; print(p, p.is_alive())&lt;Process(Process-1, initial)&gt; False&gt;&gt;&gt; p.start()&gt;&gt;&gt; print(p, p.is_alive())&lt;Process(Process-1, started)&gt; True&gt;&gt;&gt; p.terminate()&gt;&gt;&gt; time.sleep(0.1)&gt;&gt;&gt; print(p, p.is_alive())&lt;Process(Process-1, stopped[SIGTERM])&gt; False&gt;&gt;&gt; p.exitcode == -signal.SIGTERMTrue exception multiprocessing.ProcessError multiprocessing 异常类型的基类。 exception multiprocessing.BufferTooShort 当提供的缓存对读取的消息来说太小时，由 Connection.recv_bytes_into() 方法抛出的异常。 exception multiprocessing.AuthenticationError 认证失败抛出的异常。 exception multiprocessing.TimeoutError 超时抛出的异常。 Pipes &amp;&amp; Queues通常多进程之间通信应该使用消息传递，尽量避免使用任何同步原语（如锁）。 消息传递可以使用 Pipe() （两个进程间通信）或 queue（多个生产者和消费者之间通信）。 Queue、SimpleQueue 和 JoinableQueue 类型是基于标准库中 queue.Queue 类的基础上构建的多生产者，多消费者 FIFO 队列。不同之处在于 Queue 与 queue.Queue 相比缺少 task_done() 和 join() 方法（该方法在 Python 2.5 之后的版本引入）。 如果使用 JoinableQueue 则一定要为从队列中删除的每个任务调用 JoinableQueue.task_done()，否则可能导致用于统计未完成任务数量的信号量溢出，从而引发异常。 注：也可以通过 Manager 对象创建一个共享队列。 Tip：multiprocessing 使用 queue.Empty 和 queue.Full 异常发出超时信号。这些不能在 multiprocessing 的命名空间中找到，因此需要从 queue 中引用进来。 Tip：当一个对象被放入队列时，对象是被序列化了的，之后后台线程会把序列化之后的数据缓冲到一个底层管道。这可能是一个令人惊讶的结果，但不会造成任何困难。如果这个操作会影响到你的任务，可以使用 manager 。 将一个对象放入到空队列后，可能有一个无限小的延迟队列的 empty() 方法返回 True 。get_nowait() 方法可以在不抛出 queue.Empty 异常的情况下返回队列状态。 123456789101112131415161718192021222324252627282930313233&gt; # CPython 3.5.2 /Lib/multiprocessing/queues.py&gt; #&gt; # Queue type using a pipe, buffer and thread&gt; #&gt; class Queue(object):&gt; # ... ignore other methods&gt; def get(self, block=True, timeout=None):&gt; if block and timeout is None:&gt; with self._rlock:&gt; res = self._recv_bytes()&gt; self._sem.release()&gt; else:&gt; if block:&gt; deadline = time.time() + timeout&gt; if not self._rlock.acquire(block, timeout):&gt; raise Empty&gt; try:&gt; if block:&gt; timeout = deadline - time.time()&gt; if timeout &lt; 0 or not self._poll(timeout):&gt; raise Empty&gt; elif not self._poll():&gt; raise Empty&gt; res = self._recv_bytes()&gt; self._sem.release()&gt; finally:&gt; self._rlock.release()&gt; # unserialize the data after having released the lock&gt; return ForkingPickler.loads(res)&gt; &gt; def get_nowait(self):&gt; return self.get(False)&gt; 如果多个进程同时向队列中放入对象，在另一端接收到的对象很可能是无序的。但是，由同一个进程放入队列中的对象始终按预期的顺序相互关联。 警告： 警告： 有关使用队列在进程间通信的代码实例，可以查看 Examples 部分。 On the way! MiscellaneousOn the way! Connection ObjectsOn the way! Synchronization primitivesOn the way! Shared ctypes ObjectsOn the way! ManagerManager 提供了一种在不同进程间共享创建的数据的方式，包括在不同计算机运行的进程之间通过网络共享。Manager 对象管理一个共享对象的服务进程，其他进程通过使用代理获取共享对象。 Multiprocessing.Manager() 返回一个 SyncManager 对象，可以用来在进程间共享对象。返回的 manager 对象对应一个生成的子进程，该子进程会创建共享对象并返回相应的代理方法。 manager 进程一旦被垃圾回收或其父进程退出就会关闭。manager 类的定义位于 multiprocessing.managers 模块。 multiprocessing.managers.BaseManager([address[, authkey]]) 创建 BaseManager 对象。 创建后应该调用 start() 或 get_server().serve_forever() 方法保证 manager 对象引用的进程开启。 address 是 manager 进程监听的新连接的地址，若地址为 None 则 ProxyOn the way! Process PoolOn the way! Listener &amp;&amp; ClientOn the way! Authentication keysOn the way! LoggingOn the way! multiprocessing.dummy 模块On the way! Programming Guidelines如下是使用 multiprocessing 模块时的一些指导原则和习惯用法。 Start()如下原则适用于所有的 start() 方法。 避免内存共享 尽可能的避免在进程之间传递大量数据。 进程间通信最好严格使用 queue 或 pipe 而不是使用级别较低的同步原语。 序列化 保证代理方法的参数是可以序列化的。（可以被 pickle） 代理是线程安全的 若代理对象来自多个线程，应该使用锁进行保护。（确保使用同一个代理的不同进程不会出现问题） 僵尸进程的 join() 方法 Unix 中，当一个子进程完成退出没有调用 join() 方法而父进程在继续运行时，子进程成为僵尸进程。因为每一个新进程 start() （或调用 active_children() 方法），所有没有调用 join() 方法的终止进程将会 join()，所以僵尸进程的数量不会特别多。同事，调用一个已终止进程的 Process.is_alive 将会调用 join() 方法。即使如此，开始时明确进程的执行流程也是一个好习惯。 个人解读 join() 方法具有清除僵尸进程的作用。通过下述源码我们可以发现，该方法通过子父进程的串行进程执行方式清除僵尸进程。join() 方法主要做了两件事情：① 通知父进程调用 wait() 方法；② 等待子进程终止或超时后，移除子进程。不带参数的 join() 方法 若调用带有参数的 join(timeout) 方法，在代码中父进程等待 timeout 时长后，开始唤醒父进程，此时子父进程开始同时执行，父进程首先执行join(timeout) 方法，若此时子进程还未结束，则变量 res 获取的进程退出信息为空，即不会清除子进程，然后继续执行父进程的后续逻辑，此时子父进程在并行执行。若子进程再次终止后父任务还未结束，则父进程因无法获取到子进程的退出信息而导致子进程沦为僵尸进程。 12345678910111213141516171819&gt; # CPython 3.5.2 /Lib/multiprocessing/process.py BaseProcess 类&gt; class BaseProcess(object):&gt; '''&gt; Process objects represent activity that is run in a separate process&gt; The class is analogous to `threading.Thread`&gt; '''&gt; # ... ignore other methods&gt; def _Popen(self):&gt; raise NotImplementedError&gt; def join(self, timeout=None):&gt; '''&gt; Wait until child process terminates&gt; '''&gt; assert self._parent_pid == os.getpid(), 'can only join a child process'&gt; assert self._popen is not None, 'can only join a started process'&gt; res = self._popen.wait(timeout)&gt; if res is not None:&gt; _children.discard(self)&gt; 123456789101112131415161718192021222324252627282930313233&gt; # CPython 3.5.2 /Lib/multiprocessing/popen_fork.py Popen 类&gt; class Popen(object):&gt; method = 'fork'&gt; # ... ignore other methods&gt; def poll(self, flag=os.WNOHANG):&gt; if self.returncode is None:&gt; while True:&gt; try:&gt; pid, sts = os.waitpid(self.pid, flag)&gt; except OSError as e:&gt; # Child process not yet created. See #1731717&gt; # e.errno == errno.ECHILD == 10&gt; return None&gt; else:&gt; break&gt; if pid == self.pid:&gt; if os.WIFSIGNALED(sts):&gt; self.returncode = -os.WTERMSIG(sts)&gt; else:&gt; assert os.WIFEXITED(sts)&gt; self.returncode = os.WEXITSTATUS(sts)&gt; return self.returncode&gt; &gt; def wait(self, timeout=None):&gt; if self.returncode is None:&gt; if timeout is not None:&gt; from multiprocessing.connection import wait&gt; if not wait([self.sentinel], timeout):&gt; return None&gt; # This shouldn't block if wait() returned successfully.&gt; return self.poll(os.WNOHANG if timeout == 0.0 else 0)&gt; return self.returncode&gt; 消除僵尸进程的方法： 创建两次子进程 即将父进程创建子进程的方式改为父进程先创建子进程代理进程，由代理进程创建子进程执行任务。代理进程在完成子进程的创建任务后，调用代理进程的 join() 方法消除（因为代理进程的执行时间很短）；同时，执行任务的子进程变成了孤儿进程，无论执行多久，最终被 init 进程（即进程号为 1 的进程）所收养，由 init 进程对其完成状态收集工作，避免了僵尸进程的产生。 利用系统信号清除僵尸进程 基于 Linux 信号方式添加代码 signal.signal(signal.SIGCHLD, signal.SIG_IGN)。signal.signal() 函数定义在收到信号时执行自定义的处理程序。此处是指忽略创建的子进程的退出信号， signal.SIGCHLD 在子进程状态改变后会产生此信号。 sinal.SIG_IGN 是标准信号处理程序，简单忽略给定信号；默认使用 sinal.SIG_DFL，代表不理会给定信号，但也不丢弃。修改后不保存子进程的状态使之成为孤儿进程被 init 进程回收。 此外，父进程退出时创建的僵尸进程也会被清除。 继承比使用 pickle/unpickle 更好 使用 spawn 或者 forkserver 方式调用 start() 方法时，multiprocessing 中的许多类型需要被序列化给子进程使用它们。但是，通常应该避免使用 pipe 或 queue 将共享对象发送给其他进程。应该合理安排程序，使得需要访问其他进程创建的共享资源的进程通过从祖先进程继承的方式去访问。 避免主动终止进程 使用 Process.terminate 方法可以终止进程，但可能导致进程当前使用的资源（如锁、信号量、管道和队列）被破坏或者不能被其他进程使用。因此，最好只有没有使用任何共享资源的进程上调用 Process.terminate 方法。 使用 queue 通信的进程调用 join() 请记住，当一个进程调用 join() 方法时，Python 会检测被放入到 queue 中数据是否已经被全部删除（如 queue.get()，若没有被完全删除，进程将会一直等待，直到所有缓冲的数据由“feeder”线程将数据提供给底层管道。（子进程可以通过调用 Queue.cancel_join_thread 方法取消该行为。）这意味着，无论在什么时候使用 queue 都需要确保进程调用 join() 方法前放入到队列中的数据被全部清除。否则，无法确保将数据放入到队列中的进程已经终止。切记，非后台进程将会自动调用 join() 方法。 下面是一个会造成死锁的代码示例。 123456789101112&gt; from multiprocessing import Process, Queue&gt; &gt; def f(q):&gt; q.put('X' * 1000000)&gt; &gt; if __name__ == '__main__':&gt; queue = Queue()&gt; p = Process(target=f, args=(queue,))&gt; p.start()&gt; p.join() # this deadlocks&gt; obj = queue.get()&gt; 一个改进办法是交换上述代码的最后两行（或简单删除 p.join() ） 将资源明确的传递给子进程 在使用 fork 方式调用 start() 方法的 Unix 环境中，子进程可以使用父进程创建的全局资源中的所有共享资源，但最好将共享的对象作为参数传递给子进程的构造函数。 这样除了可以使得代码（可能）兼容 Windows 平台和其他调用 start() 方法的方式外，同时还可以确保共享的对象在子进程仍处于活动状态时不会在父进程被垃圾回收。如果父进程垃圾回收时释放某些共享资源，这种方式就非常重要。如下代码示例应该被改写为后者。 12345678910111213141516171819202122&gt; # 有问题的方式&gt; from multiprocessing import Process, Lock&gt; &gt; def f():&gt; ... do something using "lock" ...&gt; &gt; if __name__ == '__main__':&gt; lock = Lock()&gt; for i in range(10):&gt; Process(target=f).start()&gt; &gt; # 更好的写法&gt; from multiprocessing import Process, Lock&gt; &gt; def f(l):&gt; ... do something using "l" ...&gt; &gt; if __name__ == '__main__':&gt; lock = Lock()&gt; for i in range(10):&gt; Process(target=f, args=(lock,)).start()&gt; 用文件对象取代 sys.stdin 时要小心 multiprocessing 模块最初是无约束的调用 os.close(sys.stdin.fileno())。 在 multiprocessing.Process._bootstrap() 方法中这样会导致子进程出现问题，修改如下 123&gt; sys.stdin.close()&gt; sys.stdin = open(os.open(os.devnull, os.O_RDONLY), closefd=False)&gt; 这样解决了进程相互冲突导致的文件描述符错误的基本问题，但对于带有输出缓冲的文件对象取代 sys.stdin() 的应用程序引入了潜在危险。危险是：如果在多个进程中调用同一个文件对象的 close() 方法多次，可能导致相同的数据被多次刷新到文件对象中，从而导致了数据损毁。 如果需要自己实现文件对象及其缓存，则可以通过对缓存附加进程 PID 并在 PID 发生改变时丢弃缓存使其成为拷贝安全的。一个实现实例如下所示。 12345678&gt; @property&gt; def cache(self):&gt; pid = os.getpid()&gt; if pid != self._pid:&gt; self._pid = pid&gt; self._cache = []&gt; return self._cache&gt; 更详细的信息可以阅读 bpo-5155，pbo-5313 和 bpo-5331。 spawn方式 &amp;&amp; forkserver方式此外还有些不适用于 fork 方式 start() 的其他约束。 更多序列化要求 保证传给 Process.__init__() 方法的参数都是可以序列化的。同时也应该保证 Process 子类调用 Process.start() 方法时其实例也可以被序列化。 全局变量 请记住，如果在子进程中尝试访问全局变量，则子进程看到的变量值（如果有）可能与父进程中调用 Process.start() 方法时的值不同。 main 模块的引用安全 确保 Python 解释器可以安全的导入主模块而不会导致意外发生（如启动一个新进程时）。例如，使用 spawn 或 forkserver 方式调用 start() 方法运行下面的代码实例会导致 RuntimeError。 1234567from multiprocessing import Processdef foo(): print('hello')p = Process(target=foo)p.start() 应使用 if __name__ == &#39;__main__&#39;: 来包含程序入口，如下代码所示。 12345678910from multiprocessing import Process, freeze_support, set_start_methoddef foo(): print('hello')if __name__ == '__main__': freeze_support() set_start_method('spawn') p = Process(target=foo) p.start() （若程序实体正常运行而不需要冻结可以省略 freeze_support() 行代码。）这允许新生成的 Python 解释器可以安全导入模块，然后运行模块中 foo() 函数。在主模块中创建了 Pool 或 Manager 具有类似约束。 Examplesmanager如下是一个创建使用 manager 及其代理的实例。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889from multiprocessing import freeze_supportfrom multiprocessing.managers import BaseManager, BaseProxyimport operator##class Foo: def f(self): print('you called Foo.f()') def g(self): print('you called Foo.g()') def _h(self): print('you called Foo._h()')# A simple generator functiondef baz(): for i in range(10): yield i*i# Proxy type for generator objectsclass GeneratorProxy(BaseProxy): _exposed_ = ['__next__'] def __iter__(self): return self def __next__(self): return self._callmethod('__next__')# Function to return the operator moduledef get_operator_module(): return operator##class MyManager(BaseManager): pass# register the Foo class; make `f()` and `g()` accessible via proxyMyManager.register('Foo1', Foo)# register the Foo class; make `g()` and `_h()` accessible via proxyMyManager.register('Foo2', Foo, exposed=('g', '_h'))# register the generator function baz; use `GeneratorProxy` to make proxiesMyManager.register('baz', baz, proxytype=GeneratorProxy)# register get_operator_module(); make public functions accessible via proxyMyManager.register('operator', get_operator_module)##def test(): manager = MyManager() manager.start() print('-' * 20) f1 = manager.Foo1() f1.f() f1.g() assert not hasattr(f1, '_h') assert sorted(f1._exposed_) == sorted(['f', 'g']) print('-' * 20) f2 = manager.Foo2() f2.g() f2._h() assert not hasattr(f2, 'f') assert sorted(f2._exposed_) == sorted(['g', '_h']) print('-' * 20) it = manager.baz() for i in it: print('&lt;%d&gt;' % i, end=' ') print() print('-' * 20) op = manager.operator() print('op.add(23, 45) =', op.add(23, 45)) print('op.pow(2, 94) =', op.pow(2, 94)) print('op._exposed_ =', op._exposed_)##if __name__ == '__main__': freeze_support() test() Pool使用 Pool 的实例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153import multiprocessingimport timeimport randomimport sys## Functions used by test code#def calculate(func, args): result = func(*args) return '%s says that %s%s = %s' % ( multiprocessing.current_process().name, func.__name__, args, result )def calculatestar(args): return calculate(*args)def mul(a, b): time.sleep(0.5 * random.random()) return a * bdef plus(a, b): time.sleep(0.5 * random.random()) return a + bdef f(x): return 1.0 / (x - 5.0)def pow3(x): return x ** 3def noop(x): pass## Test code#def test(): PROCESSES = 4 print('Creating pool with %d processes\n' % PROCESSES) with multiprocessing.Pool(PROCESSES) as pool: # # Tests # TASKS = [(mul, (i, 7)) for i in range(10)] + \ [(plus, (i, 8)) for i in range(10)] results = [pool.apply_async(calculate, t) for t in TASKS] imap_it = pool.imap(calculatestar, TASKS) imap_unordered_it = pool.imap_unordered(calculatestar, TASKS) print('Ordered results using pool.apply_async():') for r in results: print('\t', r.get()) print() print('Ordered results using pool.imap():') for x in imap_it: print('\t', x) print() print('Unordered results using pool.imap_unordered():') for x in imap_unordered_it: print('\t', x) print() print('Ordered results using pool.map() --- will block till complete:') for x in pool.map(calculatestar, TASKS): print('\t', x) print() # # Test error handling # print('Testing error handling:') try: print(pool.apply(f, (5,))) except ZeroDivisionError: print('\tGot ZeroDivisionError as expected from pool.apply()') else: raise AssertionError('expected ZeroDivisionError') try: print(pool.map(f, list(range(10)))) except ZeroDivisionError: print('\tGot ZeroDivisionError as expected from pool.map()') else: raise AssertionError('expected ZeroDivisionError') try: print(list(pool.imap(f, list(range(10))))) except ZeroDivisionError: print('\tGot ZeroDivisionError as expected from list(pool.imap())') else: raise AssertionError('expected ZeroDivisionError') it = pool.imap(f, list(range(10))) for i in range(10): try: x = next(it) except ZeroDivisionError: if i == 5: pass except StopIteration: break else: if i == 5: raise AssertionError('expected ZeroDivisionError') assert i == 9 print('\tGot ZeroDivisionError as expected from IMapIterator.next()') print() # # Testing timeouts # print('Testing ApplyResult.get() with timeout:', end=' ') res = pool.apply_async(calculate, TASKS[0]) while 1: sys.stdout.flush() try: sys.stdout.write('\n\t%s' % res.get(0.02)) break except multiprocessing.TimeoutError: sys.stdout.write('.') print() print() print('Testing IMapIterator.next() with timeout:', end=' ') it = pool.imap(calculatestar, TASKS) while 1: sys.stdout.flush() try: sys.stdout.write('\n\t%s' % it.next(0.02)) except StopIteration: break except multiprocessing.TimeoutError: sys.stdout.write('.') print() print()if __name__ == '__main__': multiprocessing.freeze_support() test() queue 下面的实例介绍了如何使用 queue 将任务分配给多个任务进程并收集任务结果。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677import timeimport randomfrom multiprocessing import Process, Queue, current_process, freeze_support## Function run by worker processes#def worker(input, output): for func, args in iter(input.get, 'STOP'): result = calculate(func, args) output.put(result)## Function used to calculate result#def calculate(func, args): result = func(*args) return '%s says that %s%s = %s' % \ (current_process().name, func.__name__, args, result)## Functions referenced by tasks#def mul(a, b): time.sleep(0.5*random.random()) return a * bdef plus(a, b): time.sleep(0.5*random.random()) return a + b###def test(): NUMBER_OF_PROCESSES = 4 TASKS1 = [(mul, (i, 7)) for i in range(20)] TASKS2 = [(plus, (i, 8)) for i in range(10)] # Create queues task_queue = Queue() done_queue = Queue() # Submit tasks for task in TASKS1: task_queue.put(task) # Start worker processes for i in range(NUMBER_OF_PROCESSES): Process(target=worker, args=(task_queue, done_queue)).start() # Get and print results print('Unordered results:') for i in range(len(TASKS1)): print('\t', done_queue.get()) # Add more tasks using `put()` for task in TASKS2: task_queue.put(task) # Get and print some more results for i in range(len(TASKS2)): print('\t', done_queue.get()) # Tell child processes to stop for i in range(NUMBER_OF_PROCESSES): task_queue.put('STOP')if __name__ == '__main__': freeze_support() test() 后记On the way! 注该篇博客参考文档为 Python 3.5.6 版本，源码版本为 CPython 3.5.2。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>documentation</tag>
        <tag>Translation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GitHub Pages + Hexo 搭建博客]]></title>
    <url>%2F2018%2F11%2F01%2FGitHub-Pages-Hexo-%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[在决定写博客之后，我们首要面临的问题就是如何完成个人博客的初始化工作。此处，个人选择了 GitHub Pages + Hexo 的博客搭建方案。对博客方案选择过程有兴趣的或者有自身明确需求的读客可以阅读 博客的需求 和 博客搭建的选择 两部分以决定是否阅读本博客的方案，若决定使用与本博客同样的解决方案，可以直接阅读本博客的方案。 博客的需求对个人博客而言，我将需要具备的能力按重要性排列如下： 方便快捷编辑修改的能力 数据的存放 内容获取的便利性 其他方面 方便快捷编辑修改的能力我将方便快捷编辑修改的能力放置于个人博客需求的首位。 因为与其他网站不同，个人博客在追求形式美的同时，平时更多关注于内容的撰写与发表。具有较为理想的快速编辑与修改能力就显的很有必要，也减少了编写博客的阻力（编写能力弱无形中增加了写博客的惰性）。 其次，博客的内容在于他人的沟通交流或者自身理解程度的加深，需要回去修改或增加内容，所以快速定位博客及编辑修改的能力也要突出。 最后，编写功能操作越简单越好，推送发布的操作越少越好。降低每次写新博客或者编辑旧博客的复杂程度。 数据的存放对个人博客而言，数据存放是一件严肃认真的事情。数据的存放会包含很多方面，比如数据存放的位置，存放的平台，存放的格式等等；存放完还有数据是否自己可以完全掌控，是否容易迁移等等各种各样的问题。 对个人博客而言，一般而言，以文字为主，佐以少量图片或语音调色（可使用外链），因此一般数据量不大。访问的频率也不会太高。但写博客是一件长期的事情，所以要考虑数据存储的稳定性，数据备份迁移等行为不宜频率太高。 此处，我个人倾向于选择数据可以完全自己掌握的存储方式，如博客园，CSDN等博客网站，或如知乎，公众号等平台，首先会对你发表的内容进行审核，因此发表修改编辑有环节，不能所写所见；其次，数据导出或迁移甚至在无网络情况下，本地无法浏览更改；最后，这些数据的所有权归属于平台，因此在博客选择时我排查了上述类型的博客撰写方式。 内容获取的便利性个人博客的撰写目的当然是为了方便他人的阅读与交流，因此要方便他人的阅读，方便交流。当然，也可以通过获取方式对阅读人群进行初步筛选，如常见的新浪微博偏娱乐碎片化的信息记录交流；公众号稍微长一点，也是碎片化的信息；各种课程网站就比较长且完整。 其他方面除上述之外，可能还有很多原因或考虑促成你选择适合自己的博客。比如美观时尚要求，比如内容要求，经常分享自己的拍照视频等等各种各样的需求。按需选择适合自己的博客是促使可以长期坚持下去的首要条件，若是不合适的博客，在几次尝试撰写博客，或者在各大流行的博客间转换几次之后失去了兴趣，也就失去了写博客的初衷。因此，愿每个勇于尝试博客搭建的人都能很快选出适合的博客并坚持下去。 博客搭建的选择该部分大部分内容会参考自ONEGEE 博客 - 怎么选择和快速搭建个人博客 ，本人尝试过的方式会进行额外说明与补充。 该博主将博客按发布形式分为了三种：个人主页注册、静态网站生成与内容管理系统。 个人主页注册个人主页注册是指在现有的博客网站、论坛或社区上注册个人主页。优点是没有技术门槛，注册即用；拥有成熟的平台支撑，方便推广。缺点是风格单一，自定义程度低，还有许多形式与内容的限制。适合嫌麻烦不喜折腾而又不反感条条框框，对数据存储无感且不轻易迁移数据的人。 SegmentFault中文领域最大的编程问答交流社区平台，其所属杭州堆栈科技有限公司创立于2012年，目标是覆盖和服务中国软件开发者和 IT 信息从业者，充分利用在各个平台所能获得的各种技术创新机会为其开发产品应用和服务。 可以理解为中文的 StackOverFlow 社区，技术交流平台成熟。网站提供了文章专栏板块，有审核机制，支持 Markdown 文法、标签、评论、智能目录等。颜值中等的简洁风格。 因此，整体上 SegmentFault 平台计算机相关专业为主要阅读者。 简书简书自身定位为国内最优质的创作社区，2013年4月上线公测版本，正式开放注册。任何人可以在其上进行创作，相互交流。内容偏重于文字。同支持 Markdown 文法、标签、评论功能。颜值中等干净。 简书创始人简书个人主页，知乎ID简叔。对简书或其CEO个人有兴趣的可以去关注。 知乎中文互联网知名知识社交平台，创立于2011年1月26日，产品形态模仿自美国类似网站 Quora。用户通过问答等交流方式建立连接，偏娱乐化和专业知识，目前该公司包括知乎、知乎群组、知乎日报与公益壹点通四款应用。 提供的文章版本可以用作于博客撰写，提供 Markdown 文法、标签、评论功能。颜值中等偏大气的风格。 对知乎用户获取较为方便，但非知乎用户获取困难。大部分互联网人士了解该平台。 CSDNCSDN 全称是 Chinese Software Developer Network，创建于1999年，自身定位是中国专业 IT 社区，为中国的软件开发者提供知识传播、在线学习、职业发展等全生命周期服务。官方数据，截止2018年6月，CSDN 拥有2500+ 万技术会员，论坛发帖数 1000+ 万，技术资源 700+ 万，博客文章1300+ 万，新媒体矩阵粉丝数量 430+ 万。 老牌技术论坛，支持 Markdown 文法、标签、评论功能，文章管理方式传统。但目前 CSDN 的交互体验比较差，常被吐槽。颜值正常偏下水平。 博客园博客园创立于2004年1月，面向开发者的知识分享社区。自身定位为开发者打造纯净的技术交流区，推动并帮助开发者通过互联网分享知识。 老牌技术论坛，申请博客需人工审核，上班时间10分钟左右。支持 Markdown 文法、标签、评论、RSS、相册、文件等功能，文章管理方式传统。Logo 有多种选择，颜值正常，老式网站风格。 其他注册形式的博客还有很多，如网易(很快将迁移至 LOFTER)、新浪、搜狐博客等，甚至还有已经停止维护的博客网站，且大多数定位也不是技术类博客，此处没有介绍。此外，如微信订阅号、公众号，知乎问答、StackOverFlow或Quora、甚至百度贴吧等以问答形式完成博客的撰写也是不错的形式。 静态网站生成通常是指由 Jekyll、Hugo 或 Hexo 等技术生成静态网站，然后上传至 GitHub Pages、Coding Pages 等托管平台免费展示。具有一定的技术门槛，需要了解 Markdown 文法，简单 Linux 命令，域名解析，对要托管的平台如GitHub 或 Coding 有一定了解。 该类型的博客撰写或修改流程大致如下 本地以特定表头格式写博客，放于指定文件夹中 执行命令快速生成完整的静态网站 通过 Git 管理工具将文件上传至代码托管平台 该种博客搭建方式具有搭建快速、自定义程度高、主题丰富、技术更新迭代快、社区活跃的优势，同时具有一定的入坑门槛，适合有一定技术基础或喜欢折腾的用户，不同技术配置间迁移成本低。 Hexo一种基于 Node.js 的快速、简洁、高效的博客框架，GitHub代码库有 24k+ 的 Star（截止2018年11月12日）。安装过程顺利，配置、发布人性化，社区活跃，对技术不熟英文不好的人同样友善。主题多，选择空间大。可以通过插件形式支持博客所需功能。 Hugo一种基于 Go 语言实现的站点生成器，GitHub代码库有30K+的 Star（截止2018年11月12日）。安装过程较为顺利，中文社区不是很活跃。主题多，适合有一定技术基础有更高品味要求的用户。 JekyllGitHub 官方推荐的将纯文本转化为静态网站和博客的站点生成器。GitHub 联合创始人 Tom Preston-Werner 使用 Ruby 语言编写，在 GitHub代码库有35K+的 Star（截止2018年11月12日）。官方的加持，使得可以不依赖本地环境配置，直接在网站生成，本地环境配置较为麻烦。主题较好，相较于 Hexo 与 Hugo 较少。 内容管理系统内容管理系统指带有后台管理的博客系统，需要配置服务器、数据库以及域名管理，在此基础上安装内容管理系统。相较于静态网站生成而言，是动态博客，有前台后台之分，后台负责写作、发布、系统配置等。 这种方式具有贴心的后台管理功能，意味着具有出色的文章管理，相册管理，文件管理，而且在数据库基础上可以实现用户管理以及高清大图上传等，可以内置搜索、评论等常用功能。但同时，丰富的管理功能背后需要用户较高的技术基础，如 Web 相关的服务器知识，数据库知识等。与当下用户体验当道和扁平化时代相比，丰富又臃肿。 因此，对个人用户而言，使用该方式搭建博客，安全稳定，一次上手之后基本无需迁移，但可能需要有服务器的开销花费。若有多人维护，频繁更新的需求可以考虑该方式搭建博客。 WordPress一个开源的基于 PHP 和 MySQL 的个人发布系统。根源和开发可以追溯到2001年，社区活跃，遵循 GPL 协议。 具有较高的市场占有率，博客只是其功能之一，可以搭建企业级网站。中文友好，中文社区活跃。 ghost基于Node.js 实现的开源，旨在为新闻媒体构建开源解决方案。社区活跃，相比于 WordPress 而言，简洁大气，专为写作生产力的极致博客系统，便捷，可以随时随地撰写编辑博客，尤其在不同电脑上。WordPress 良好替换品，有一定搭建门槛。颜值在所有例子中最高（为颜值牺牲了一些功能）。 建议新手村指南：如果是新手，对于以上的技术门槛一窍不通，但是又想要主题精美的个人博客网站，建议从Markdown语言开始学起（半天入门，一天出师）。之后可以选择现有平台，简单上手，也可以稍微了解一些基本的命令行知识和 Git 操作，跟随各种教程，从生成静态网站入门快速搭建博客，完全不花钱。 对与我情况类似的读者，出身计算机相关专业或从事相关工作，可以考虑自己动手搭建。 首推 hexo。性价比最高，中文友好，快速上线，贴心配置，免费高颜值。 其次 WordPress。满足多人维护，资料繁多等需求，虽然门槛高较高，体量较大，且有额外花销，但稳定，可以对网址数据全掌握。 最后，内容高于形式，入坑需谨慎 。 个人博客最终选择了GitHub Pages + Hexo + NexT 的博客解决方案，博客地址：码农驿站 本博客的方案个人搭建技术博客，对颜值要求中等，期望数据完全掌握，由于一个人维护，又不希望维护的成本过高，因此选定了 Hexo 技术方案。其丰富的插件支持足以满足我对个人博客的需求。 在托管平台选取时，天然选择了 GitHub，即程序员交友网站，方便阅读和沟通交流。 GitHub Pages + Hexo + NexT 的方案搭建过程如下，其中可能遇到的问题我会在相应位置提及，但整体而言，较少遇到，搭起来很快。 参考了较多他人的博客与官方文档，资料如下 GitHub + Hexo 搭建个人网站详细教程 - From 知乎 Hexo 官网 环境准备这里的环境准备包括两部分，GitHub 的托管平台配置与本地环境的配置。与博客相关度不大的部分在这里介绍将会较为简略。 GitHub 配置GitHub 是目前最流行的代码仓库，得到了很多大公司与项目的青睐，为使得项目更方便的被人理解，需要项目的介绍页面甚至完整的技术文档，于是 GitHub Pages 服务应运而生，其不仅可以方便的为项目建立介绍站点，也可以用来建立个人博客。 GitHub Pages 属于轻量级的博客系统，配置简单；支持 Markdown 文法，编辑简单迅速；无需自己搭建服务器，GitHub 给每个站免费提供了 300MB 的空间（对文字而言足够）；可以绑定自己的域名。 配置流程大致如下： 购买、绑定独立域名 配置和使用 GitHub 注册账号 本地安装 Git Bash 配置 SSH Keys 设置实现免密登陆 测试联通成功并添加账号等相关信息 GitHub Pages 建立博客 - 注个人博客必须使用与 GitHub 用户名一样的名字，格式为 GitHubName.github.io 绑定域名到GitHub Pages 详细 GitHub Pages 配置过程可参考 使用 GitHub Pages 建立独立博客 - beiyunyun的博客，若与我情况类似，重新建立了新的 GitHub 账号以搭建博客，可能需要了解多个 GitHub 账号配置 SSH Key。 Hexo 本地环境配置Hexo 是基于 Node.js 的博客框架，因此需要首先安装 Node.js，Node.js 的安装包下载地址 安装成功后，通过命令行测试结果如下 1234~ $ node -vv8.4.0~ $ npm -v6.1.0 Hexo 安装较为简单，命令如下所示 1~ $ npm install hexo-cli -g 如上操作成功后，本地的环境基本准备完成，后续进行 Hexo 相关配置说明。 Hexo 配置Hexo 官网技术文档详细说明了如何上手，强烈推荐快速浏览一遍，可以对 Hexo 使用有一个大概了解，知道有哪些功能，方便后续功能的添加维护和更新。 此处基本按照 Hexo 搭建的流程进行配置。 建站安装 Hexo 完成后，执行如下命令，Hexo 将在指定的文件夹中新建所需文件 123~ $ hexo init &lt;blog-folder&gt; # 初始化名为 blog 的博客，可自行设置博客名~ $ cd &lt;blog-folder&gt;&lt;blog-folder&gt; $ npm install 新建完成后，指定文件夹下的目录结构如下所示 12345678.├── _config.yml # 网站的配置信息├── package.json # 应用程序信息，默认安装了 EJS，Stylus 和 Markdown Renderer，可自由移除├── scaffolds # 模板文件夹，Hexo 根据该文件建立文件├── source # 存放用户资源的地方，Markdown 和 HTML 文件被解析并放到 public 文件夹，其他被拷贝过去| ├── _drafts| └── _posts└── themes # 主题文件夹 网站的配置在 _config.yml 文件中，在此可以配置大部分的参数，常用及此次修改位置有网站、网址、目录、文章、分类&amp;标签、日期时间格式、分页、扩展（包括主题）等几个部分，配置文件中对应修改如下。 网站123456789# Sitetitle: 码农驿站 # 网站名，会在标签页上显示subtitle: 一枚码农的自述以供交流娱乐 # 副标题，网站名下面description: Just For Fun # 描述，主要用于SEO，告诉搜索引擎一个关于您站点的简单描述keywords: Codeauthor: 陈文嘉language: zh-Hans# 应填写 Asia/Shanghai，填写 CN 会报错 TypeError: Cannot read property 'utcOffset' of nulltimezone: Asia/Shanghai 网址123456# URL## If your site is put in a subdirectory, set url as 'http://yoursite.com/child' and root as '/child/'url: https://chenwenjia1991.github.ioroot: /permalink: :year/:month/:day/:title/permalink_defaults: 目录123456789# Directorysource_dir: source # 资源文件夹public_dir: public # 公共文件夹，用于存放生成的站点文件tag_dir: tags # 标签文件夹archive_dir: archives # 归档文件夹category_dir: categories # 分类文件夹code_dir: downloads/code # Include Code 文件夹i18n_dir: :lang # 国际化文件夹skip_render: # 跳过指定文件的渲染，可使用 glob表达式来匹配路径 Tips: 刚接触 Hexo，此部分一般不做修改。 分类 &amp; 标签博客添加分类和标签页，参考了 Hexo 使用攻略 - 添加分类及标签 From linlif 博客，添加新的页面也是如此，此处以添加 “categories” 页面为例，主要流程为 创建 “categories” 页面并添加 type 属性 1&lt;blog_folder&gt; $ hexo new page categories 成功后提示 1INFO Created: ~/Documents/blog/source/categories/index.md 正该 index.md 文件中，添加字段 type: “categories” 1234567&lt;blog_folder&gt; $ cat source/categories/index.md---title: categoriesdate: 2018-10-25 23:22:17comments: falsetype: "categories"--- 文章中添加 “categories” 属性即可，如下 123456---title: blog_namedate: ****categories:- category_name--- 对于今后的文章，基本都会在撰写时填写分类&amp;标签，可以通过修改 scaffolds/post.md 文件，在 tags: 上添加 categories: 后保存，之后执行 hexo new ** 产生的新文件就有分类选项了。 该文件是产生新博客时的模板，可以通过此文件设置默认的博客页面。 可能产生的问题 问题描述：GitHub Pages 结构混乱，而本地正常 解决方案：tag、category. 在主题的配置中必须选至少一个，即文档中存在的页面需要在配置文件打开，未打开出现上述异常。 扩展这里主要指主题，此处选择了 NexT 主题 NexT 主题配置NexT 主题现已支持十种语言，有四种外观，五套代码高亮主题，配置简单而丰富，已支持多种常见第三方服务，社区较为活跃，GitHub代码库有13K+的 Star（截止2018年11月13日）。官网的文档足够全面，基本可以满足个人博客的所有需求。 NexT 安装Hexo 安装主题的方式简单粗暴，只需将主题文件拷贝至站点目录 themes 目录下，然后修改配置文件中的 theme 配置即可。 从刚才的 GitHub代码库 位置拉取最新代码，或下载稳定版代码解压缩到站点 themes 目录下，并将解压后的文件夹名改为 next。 NexT 启用与其他 Hexo 主题启用方式一致，在 Hexo 配置文件中，theme 字段值修改为 next。 1234# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: next 切换主题后验证主题是否正确启用之前，最好使用 hexo clean 命令清除 Hexo 缓存。 验证主题启动 Hexo 本地站点并开启调试模式，命令是 hexo s --debug 。 服务启动过程中，注意观察命令行输出是否有任何异常信息，若碰到问题，这些信息可帮助更好的定位问题。 当命令行输出如下提示时，可以使用浏览器访问http://localhost:4000检查站点是否正常运行。 1INFO Hexo is running at http://0.0.0.0:4000/. Press Ctrl+C to stop. 当看到的站点外观与下图所示类似时说明已成功安装 NexT 主题。这是默认的 Schema – Muse。 选择 SchemaSchema 是 NexT 提供的一种特性，以提供多种不同的外观，几乎所有配置均可以在不同 Scheme 之间功用。目前支持了四种 Scheme，在希望启用的 scheme 前去掉注释 #即可。 12345678# ---------------------------------------------------------------# Scheme Settings# ---------------------------------------------------------------# Schemes#scheme: Muse # 默认主题，NexT 的最初版，黑白主调，大量留白#scheme: Mist # Muse 的紧凑版本，整洁有序的单栏外观scheme: Pisces # 双栏，小家碧玉似的清新，此处我采用的外观#scheme: Gemini # 与 Pisces 类似 菜单设置 设定菜单内容，对应 menu 字段，设置格式为 item name: link || icon name。其中item name是名称，并不会直接显示在页面上，而是用于匹配图标及翻译。link目标连接。icon name是 FontAwesome Icon 的名字。 通过下述我的配置，即打开了home、tags、categories 与 archives，其他部分待打开进行设置。 1234567891011121314151617# ---------------------------------------------------------------# Menu Settings# ---------------------------------------------------------------# When running the site in a subdirectory (e.g. domain.tld/blog), remove the leading slash from link value (/archives -&gt; archives).# Usage: `Key: /link/ || icon`# Key is the name of menu item. If translate for this menu will find in languages - this translate will be loaded; if not - Key name will be used. Key is case-senstive.# Value before `||` delimeter is the target link.# Value after `||` delimeter is the name of FontAwesome icon. If icon (with or without delimeter) is not specified, question icon will be loaded.menu: home: / || home # 主页 # about: /about/ || user # 关于页面 tags: /tags/ || tags # 标签页 categories: /categories/ || th # 分类页 archives: /archives/ || archive # 归档页 # schedule: /schedule/ || calendar # sitemap: /sitemap.xml || sitemap # 站点地图 # commonweal: /404/ || heartbeat # 公益 404 设置菜单的显示文本。上述步骤1中的名称并不会用于界面上的展示。Hexo 在生成时实用该名字查找对应的翻译，并提取显示文本。这些翻译文本放置在 NexT 主题目录下的 languages/{language}.yml（{language}为自己所使用的语言）。 12345678910menu: home: 首页 archives: 归档 categories: 分类 tags: 标签 about: 关于 search: 搜索 schedule: 日程表 sitemap: 站点地图 commonweal: 公益 404 设定菜单项的图标，对应字段是menu_settings。 1234# Enable/Disable menu icons / item badges.menu_settings: icons: true badges: false 注：在菜单图标开启的情况下，如果菜单项与菜单未匹配（没有设置或者无效的 Font Awesome 图标名字） 的情况下，NexT 将会使用 ? 作为图标。 图标配置&amp;头像修改主题配置文件中的favicon 中，内容如下 12345678910111213141516# ---------------------------------------------------------------# Site Information Settings# ---------------------------------------------------------------# To get or check favicons visit: https://realfavicongenerator.net# Put your favicons into `hexo-site/source/` (recommend) or `hexo-site/themes/next/source/images/` directory.# Default NexT favicons placed in `hexo-site/themes/next/source/images/` directory.# And if you want to place your icons in `hexo-site/source/` root directory, you must remove `/images` prefix from pathes.# For example, you put your favicons into `hexo-site/source/images` directory.# Then need to rename &amp; redefine they on any other names, otherwise icons from Next will rewrite your custom icons in Hexo.favicon: small: /images/blog_logos/16x16.png medium: /images/blog_logos/32x32.png apple_touch_icon: /images/blog_logos/apple-icon-180x180.png safari_pinned_tab: /images/blog_logos/logo.svg android_manifest: /images/blog_logos/manifest.json ms_browserconfig: /images/blog_logos/browserconfig.xml 主题配置文件中的avatar 中，内容如下 123456789101112# Sidebar Avataravatar: # in theme directory(source/images): /images/avatar.gif # in site directory(source/uploads): /uploads/avatar.gif # You can also use other linking images. url: /images/blog_logos/avatar.gif # If true, the avatar would be dispalyed in circle. rounded: true # The value of opacity should be choose from 0 to 1 to set the opacity of the avatar. opacity: 1 # If true, the avatar would be rotated with the cursor. rotated: true 其图像建议放入博客的 source/images 中，这样将来修改或更换该图片均不会失效。 网站 Logo、头像等的制作网上有很多工具，此处不做过多说明。 集成评论模块Hexo NexT 主题下，支持较多的评论系统，具体可参看NexT 第三方服务集成。除评论系统外，还可以通过第三方服务增加数据统计与分析功能，内容分享服务，搜索服务，数学公式显示，Facebook SDK 支持，Google 站点管理工具等服务。 官方推荐的评论模块有 DISQUES、Facebook Comments、HyperComments、网易云跟帖、LiveRe几种。参考了知乎问题Hexo(NexT主题)评论系统哪个好？，根据其推荐，选择了 Gitalk 作为此处评论系统。 Gitalk 是一个基于 GitHub Issue 和 Preact 开发的评论插件，使用 GitHub 登陆，支持多语言，支持个人或组织，无干扰模式（设置 distractionFreeMode 为 true 开启），支持快捷键（cmd|ctrl + enter）提交。 此处工作参考了博客Hexo NexT 主题中集成 Gitalk 评论系统 - From asdfv1929‘s Home，配置流程如下 GitHub 中注册新应用，注册链接，填写内容如下 1234Application name # 应用名称Homepage URL # 网址，此处填 https://chenwenjia1991.github.io/Application description # 应用描述~Authorization callback URL # 授权回调网址， https://chenwenjia1991.github.io/ 点击注册后，保存 Client ID 和 Client Secret，在后面的配置中会用到 主题配置文件中配置 Gitalk，具体参数含义可参考详细参数列表 1234567891011# Gitalk# Introduction: https://github.com/gitalk/gitalk/blob/master/readme-cn.md gitalk: enable: true githubID: chenwenjia1991 # GitHub 账号 repo: Gitalk-Comment # 存储评论的仓库，可新建或使用旧项目，只写项目名称即可，项目需开启 issue ClientID: ** # 上述记录的 Client ID ClientSecret: ** # 上述 Client Secret adminUser: chenwenjia1991 #指定可初始化评论账户 perPage: 15 # 每页显示的最大评论数 distractionFreeMode: true # 全屏遮罩 通过 MD5 加密 ID 以解决 Label 长度不能超过50的问题，出现的具体问题描述及解决方案参考 Gitalk Issues 115，将JS脚本文件拷贝至 source/js/src/md5.min.js。 主题中进行相关配置 主题目录下/layout/_third-party/comments文件夹中 创建 gitalk.swig 文件并添加以下内容 12345678910111213141516171819&#123;% if page.comments &amp;&amp; theme.gitalk.enable %&#125; &lt;link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css"&gt; &lt;script src="https://unpkg.com/gitalk/dist/gitalk.min.js"&gt;&lt;/script&gt; &lt;script src="/js/src/md5.min.js"&gt;&lt;/script&gt; # &lt;— 添加的上述脚本的路径 &lt;script type="text/javascript"&gt; var gitalk = new Gitalk(&#123; clientID: '&#123;&#123; theme.gitalk.ClientID &#125;&#125;', clientSecret: '&#123;&#123; theme.gitalk.ClientSecret &#125;&#125;', repo: '&#123;&#123; theme.gitalk.repo &#125;&#125;', owner: '&#123;&#123; theme.gitalk.githubID &#125;&#125;', admin: ['&#123;&#123; theme.gitalk.adminUser &#125;&#125;'], id: md5(location.pathname), # &lt;— 使用上述脚本中的函数加密 distractionFreeMode: '&#123;&#123; theme.gitalk.distractionFreeMode &#125;&#125;' &#125;) gitalk.render('gitalk-container') &lt;/script&gt;&#123;% endif %&#125; 修改index.swig，在其末尾添加如下内容，将上述文件注册 1&#123;% include 'gitalk.swig' %&#125; 修改主题目录中 /layout/_partials/comments.swig文件，在endif %&#125;```前添加如下内容1234```js&#123;% elseif theme.gitalk.enable %&#125; &lt;div id=&quot;gitalk-container&quot;&gt;&lt;/div&gt; 主题目录下/source/css/_common/components/third-party文件夹中 新建 gitalk.styl，内容如下 1234.gt-header a, .gt-comments a, .gt-popup aborder-bottom: none;.gt-container .gt-popup .gt-action.is--active:beforetop: 0.7em; 修改third-party.styl文件，末尾添加@import &quot;gitalk&quot; if hexo-config(&#39;gitalk.enable&#39;); 重新生成静态网页并推送至GitHub Pages hexo clearn &amp;&amp; hexo g &amp;&amp; hexo d 站点统计使用了不蒜子统计，配置简单，在主题配置文件中设置busuanzi_count的enable的值为true即可。 数学公式支持主题配置文件中，设置mathjax的值为true即可。借助于 MathJax 显示数学公式。 图床在图片较多时，将图片上传至 GitHub Pages 已不再合适，毕竟有300MB的大小限制，此时考虑图床。此处参考国内外部分可用图床推荐对比-YiCH_ 简书，嗯，图片就交给它了-少数派图床推荐等相关资料，选取了腾讯云 COS 做图床，存储空间 50GB，外网下行 10GB，基本够用 - 有防盗链设置。基本满足了我们的需求。 站内搜索服务官网提供了 Swiftype、微搜索、Local Search、Algolia几种，其中 Swiftype 与 Algoliia 开始收费项目，故舍去。 此处选择了 Local Search，两种实现方式，一是本地建立索引；二是采用第三方线上服务。同样修改完重新生成网站并推送。 12345678910111213# 1. 安装 hexo-generator-searchdb，站点根目录下执行如下命令blog_path $ npm install hexo-generator-searchdb --save# 2. 编辑站点配置文件，新增如下内容# Local Searchsearch: path: search.xml field: post format: html limit: 10000# 3. 主题配置文件中，启动本地搜索，且可以修改相关配置# Local searchlocal_search: enable: true Markdown 本地编辑器个人目前博客撰写环境为 MacOS Mojave 10.14.1，因此主要查找尝试了 Mac 下的 Markdown 文本编辑器，参考知乎问题 Mac 上最好的 Markdown 文本编辑器是什么，其中 Typora 推荐数较高，优点是所见即所得，将写作和预览合二为一了。 TyporaTypora 是一款由 Abner Lee 开发的轻量级 Markdown 编辑器，适用于 OS X、Windows 和 Linux 三种操作系统，免费软件。与其他 Markdown 编辑器不同的是采用所见即所得的编辑方式实现了即时预览功能，也可切换至源代码编辑模式。 在编辑时，除了通过传统的 Markdown 代码的方式来实现富文本之外，Typora 支持通过菜单栏或者鼠标右键选取命令的方式来实现富文本，也支持通过快捷键的方式插入。Typora 也支持通过以 TeX 的格式来插入行间公式和行内公式。在完成编辑后导出文件时，Typora 支持以 PDF 或 HTML 的形式导出，如果安装了 Pandoc，也能够以Word、RTF、MediaWiki、LaTeX 等形式导出。Typora 提供有几种主题，并支持通过自定义 CSS 的方式进行个性化定制。 目前博客的编写我使用该软件完成，主要由于该目录独立，又不需要在多台电脑间同步。 印象笔记 &amp; 马克飞象印象笔记是我目前使用的笔记记录工具，两个月前开始支持 Markdown 文法，但整体来说，还处于测试阶段，用户体验有待提高。马克飞象是基于印象笔记的 Markdown 编辑工具，优点是数据存入印象笔记，因此用户可直接通过印象笔记阅读，但无法修改，流畅性好，稳定；缺点马克飞象的维护团队只有一个人，因此相比较与其他产品未完全成熟，如目前不支持 SSL/HTTPS 协议，存在安全隐患，功能添加较为缓慢。马克飞象是付费产品，79元/年。若 Markdown 编辑使用频率高，且需要在不同场合设备间切换，对数据安全性要求没有太高，推荐该产品。 Markdown 写作注意事项此处主要收集记录一些博客撰写过程中，应该注意的事项，可能会不断扩展，错误主要表现为静态网页无法产生。 行内代码引用时尽量使用一个反引号而不是三个反引号，否则容易引起格式混乱；行内代码中不引用百分号或其他转义符时容易引发 Hexo 生成静态网页的错误，如下所示 1234567# 使用一个反引号后转义失败 &#123;%code format error!%&#125; 引发如下错误INFO Start processingFATAL Something's wrong. Maybe you can find the solution here: http://hexo.io/docs/troubleshooting.htmlTemplate render error: (unknown path) unexpected end of file at Object._prettifyError (/Users/CWJ/Documents/Blog/blog_chenwenjia1991/node_modules/nunjucks/src/lib.js:36:11)... 通过 HTML 方式实现文内跳转 12345# 1. 定义 ID&lt;span id="jump"&gt;跳转去的地方&lt;/span&gt;# 2. 使用 Markdown 语法[点击跳转](#jump)eg.. [跳转至博客的需求](#1)效果如下 跳转至博客的需求 后记花费了一周多时间终于完成了本篇博客的撰写。一直在努力避免将一篇博客写的太细太长，不知不觉还是写了很长的篇幅，后续可能考虑拆分博客以为了更好的阅读体验，比如将 Hexo 相关部分独立。 感谢乐于分享的朋友，本文的很多工作也是在参考他们的博客下完成的；感谢读至此的朋友，完成这篇博客的阅读也花费了您不少的时间与耐心。 愿你我共同前行~]]></content>
      <categories>
        <category>DevelopTools</category>
      </categories>
      <tags>
        <tag>GitHub</tag>
        <tag>Hexo</tag>
        <tag>NexT</tag>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为什么开始写博客]]></title>
    <url>%2F2018%2F10%2F25%2F%E4%B8%BA%E4%BB%80%E4%B9%88%E5%BC%80%E5%A7%8B%E5%86%99%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[在2018年的第四个季度，在花费了将近一周的时间之后，最终搭起了自己的个人博客。 这是自己搭起来的第三个博客，前两个完成搭建工作之后逐渐无疾而终，在这次花费最多的时间与精力下，期望本次的博客可以长久不衰~ 为什么要写博客三年前，朋友向我推荐写博客，将自己的最近踩过的坑或者看过学到的知识做一总结分享出来，比如开发架构，比如算法分析，比如机器学习等等。一直觉得所学太浅不值得挂出来，拖拖拉拉许久也未成行。 直至硕士就业将近，完成了自己在 CSDN 的排序算法总结的第一篇博客，至今已有三年有余，500+的访问量。因各种各样的原因，在 CSDN 的只进行了一次就放弃了。第一次尝试，有收获也有失败之处。分享之后曾收到来自朋友的鼓励以及运行代码的朋友给与的反馈和建议，使得自己对于该部分的知识不仅掌握的更加扎实，理解更深入，同时自己的努力也给刚开始学习该部分知识的其他人带去帮助，也是一种喜悦。后来由于自己的原因以及 CSDN 编写博客的不畅，逐渐放弃了这种总结分享的方式。 工作之后，通过 GitHub Pages + Hexo + NexT 方式尝试搭建了个人博客。随工作进行，以及工作学习习惯的原因，更多笔记总结多放于印象笔记中，第二个博客渐渐也冷落许久。 最近随着团队技术积累的建设，团队完成了团队博客的搭建与初步撰写工作。工作以来，积累的很多东西都置于印象笔记中，需要更多的总结回顾与沉淀。因此用一周的时间完成了本博客的搭建及初始化工作。 此致，希望博客可以长久不衰！ 如何写博客什么是博客？个人将其当做记录自身学习成长过程的分享平台。其目的有三：一是回顾总结自身所学所理解认知；二是能给与我情况相似的人以启发，扬长避短；三是时时刻刻可以督促鞭策自己，砥砺前行。 基于此，本人博客基本围绕三点展开：遇到什么问题、如何去解决这个问题的、最终确定的解决方案。 定义问题在学习工作中，很多时候明确问题是很难的一件事情。也许，终其一生，很多事情我们应对之时总觉得哪里不舒服，却找不到问题的根源；也许，有时候发现自认为的问题原因却不是事情的根源所在，解决了问题不能使得事情完美；也许，更多事情，甚至不知道如何去抽象描述问题。 因此，定义问题是我们努力的探索的开始，也是如下工作的基石，问题定义的越明确清晰，解的范围越小越容易获得。 如何解决不同的问题，不同的思考角度，不同的利弊权衡，我们总会选出不同的解决方案。该部分我会尽量介绍自己是如何完成了一个问题解的寻求过程，以及遇到的解有哪些利弊，从哪些角度解决问题的。 解决方案这部分主要完成解的选择理由，以及解决问题的实践过程。当然，有时候这个问题无解也是一种解决方案。人相对于宇宙，未知总是生活的常态。 写博客的好处写博客有很多好处。开始写博客时，可能想到了如下这些，也许在博客的进行过程中，会陆陆续续添加一些新的益处与人分享。 更多的思考首先，写作就是一个深度思考的过程。很多在脑海中的事情，或者已经语言表达过的事情，用文字记录下来会是一个更深刻，更严肃的过程。相比较口语，文字在大多数情况下总有更强的逻辑性与说服力。因此，写博客的过程也是自己对所写内容进行思考回顾的过程。其次，事情完成之后记录下来，站在事情之外去看待问题的定义、解决过程会有更多收获，哪里做的好或者做的不好一目了然。更重要的是分享出来给更多人看到，他人会从更多的角度出发去接受理解这个问题，他们的思考与反馈会给自己更多考虑与帮助。 逼着自己去学习人很多时候都是有惰性的，学习的过程也不总会是顺风顺水开开心心的。因此很多时候人需要逼自己一把，越过高山。博客的撰写除了自己的记录总结，也是接受阅读的朋友们监督推动自己不断前进的过程。 学会坚持坚持做一件事情总是随着坚持时间的加长而愈加困难。学习博客恰是这样一个需要长久坚持的事情，期待自己在这方面可以做的更好更完美。 博客是一份好的简历毋庸置疑，博客相较于简历，更多体现你的知识体系，技能掌握情况等等。与简历相比，内容更丰富。当然，博客可以添加于自己的简历之上，给别人或者工作岗位确认你是否是需要的那个人~获得更匹配的岗位。 知名度提升技术道路上认识更多志同道合的人，除了在碰到一个问题时，多了可以询问咨询解决方案的通道，多了共同探索学习的朋友，你自己也会在技术圈的知名度逐步提升。 是否需要一个个人博客每个人都可以搞一个博客，无论这个博客会对公对私。记录自己的生活学习成长，吸取经验和教训，总是为了让明天的自己成为更好的自己。 后记在开源思潮日益兴盛的今天，博客搭建的难度越来越低，我们可以更关注于我们想做的事情而不是工具的选择。后续，回把自己博客的搭建过程放出来供搭建选择参考。当然也大不必将精力放在博客上太多，更多的我们要关注博客的内容，见证自己的成长。]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>杂记</tag>
      </tags>
  </entry>
</search>
