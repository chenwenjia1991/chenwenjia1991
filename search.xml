<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Pandas 内存优化]]></title>
    <url>%2F2018%2F11%2F30%2FPandas-%E5%86%85%E5%AD%98%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[Pandas 是数据分析中常用的一个 Python 库。本文主要讲述 Pandas 内存优化。其中数据读入存储的部分参考资料 简单又实用的pandas技巧：如何将内存占用降低90%，该部分内容在网上有较多说明与介绍。本文更多篇幅放在 Pandas 的 DataFrame 类型中如何进行新列的添加。 问题描述 对数据分析师或算法工程师而言，pandas 是一个常用的 Python 库，经常用来进行数据读入，清洗等操作。因此，不可避免的，从文件中读入数据或将数据存入 DataFrame 类型的变量中进行处理。这里主要针对于 DataFrame 变量对象的优化，包含数据读入（即 DataFrame 类型变量初始化）及向 DataFrame 对象中添加新列或产生新变换后的 DataFrame。 解决思路 此处，优化内存使用了 pandas 库函数之外的 memory_profiler 库以监控 Python 进程的内存使用情况，可以分析到每一行代码增减的内存状况，也可以以时间为横轴监控 Python 进程的内存使用情况。此处主要以监控代码行。使用了 datetime 模块监控函数运行时间。 优化过程 该部分主要分为从文件中读入数据与数据的处理变换部分（即对应机器学习中的数据读入与数据预处理）。 数据读入 数据读入指将数据从文件读入内存的过程，一般载入内存存入到 DataFrame 类型变量中。因此，数据读入部分的内存优化主要为 DataFrame 存储方式的优化（原因可见 结论 &amp;&amp; 分析 部分）。在读入时候传入数据对应类型，此处使用 DataFrame.info(memory_usage=&#39;deep&#39;) 方法打印与 DataFrame 类型相关的所有信息。通过该方法可以准确的获取其内存使用状况。 模拟数据这里产生单列数据的函数有三个，分别对应 int，float，category 三种类型的特征。generate_data 函数最终产生的数据大小为 data_size，其中包含两列时间值，一列目标值（即特征维度至少为4），其余三种类型的特征约为 1:1:1 进行分配。最终产生 str_df，precision_64_df，precision_32_df，str2category_df 四种 DataFrame 类型变量作为返回值，表示 DataFrame 中全部为 object 类型，数值型转为对应 64 位数值型，数值型降为对应类型最小存储单元，object 转为 category 类型依次降低其对内存的使用。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107import numpyimport pandasimport timedef generate_int_feature(field_name, length=5000): col_arr = numpy.random.randint(0, 10, size=[length]) return field_name, col_arr, "int64"def generate_float_feature(field_name, length=5000): col_arr = numpy.random.rand(length) * 100 return field_name, col_arr, "float64"def generate_categorical_feature(field_name, length=5000, str_size=5): category_names = ["a", "b", "c", "d", "e", "f", "g"] category_names = [name*str_size for name in category_names] category_nums = numpy.random.randint(2, 7) col_arr = numpy.random.randint(0, category_nums, size=[length]) col_arr = [category_names[index] for index in col_arr] return field_name, col_arr, "str"def generate_data(data_size=(5000, 300), str_size=5): """ the normal data on business include times, numbers, categories... :param data_path: the output path of the data :param data_size: the size of data """ length, cols = data_size # save the data description in order the analysis data_describe = dict() # field_name: field_type col_names = list() col_arrs = list() # generate data include 2-times, 1-target and other-features # category features: number features = 1: 2 # generate the 2-times data time_format = "%Y-%m-%d-%H" # Year-Month-Day-Hour register_time = numpy.random.randint( low=time.mktime((2000, 1, 1, 0, 0, 0, 0, 0, 0)), high=time.mktime((2018, 1, 1, 0, 0, 0, 0, 0, 0)), size=[length]).tolist() register_time_str = [time.strftime(time_format, time.localtime(i)) for i in register_time] col_names.append("register_time") col_arrs.append(register_time_str) birth_day = numpy.random.randint( low=time.mktime((1900, 1, 1, 0, 0, 0, 0, 0, 0)), high=time.mktime((2000, 1, 1, 0, 0, 0, 0, 0, 0)), size=[length]).tolist() birth_day_str = [time.strftime(time_format, time.localtime(i)) for i in birth_day] col_names.append("birth_day") col_arrs.append(birth_day_str) # generate the 1-target data: 2-class target_values = numpy.random.randint(0, 2, size=[length]).tolist() col_names.append("target") col_arrs.append(target_values) data_describe["target"] = numpy.int64 # generate features: 1/3 category and 2/3 numbers category_numbs = cols / 3 for i in range(int(category_numbs)): field_name, field_value, field_type = generate_categorical_feature( field_name="category_" + str(i), length=length, str_size=str_size) col_names.append(field_name) col_arrs.append(field_value) data_describe[field_name] = field_type numerical_numbers = cols - 3 - category_numbs for i in range(int(numerical_numbers)): if i % 2 == 0: field_name, field_value, field_type = generate_int_feature( field_name="int_" + str(i), length=length) else: field_name, field_value, field_type = generate_float_feature( field_name="float_" + str(i), length=length) col_names.append(field_name) col_arrs.append(field_value) data_describe[field_name] = field_type str_df = pandas.DataFrame(data=dict(zip(col_names, col_arrs)), dtype=str, columns=col_names) precision_64_df = str_df.copy() for field_name, field_type in data_describe.items(): precision_64_df[field_name] = precision_64_df[field_name].astype( field_type) precision_64_df["register_time"] = pandas.to_datetime( precision_64_df["register_time"], format=time_format) precision_64_df["birth_day"] = pandas.to_datetime( precision_64_df["birth_day"], format=time_format) precision_32_df = precision_64_df.copy() int_names = precision_32_df.select_dtypes(include=['int']).columns.tolist() precision_32_df.loc[:, int_names] = precision_32_df[int_names].apply( pandas.to_numeric, downcast="unsigned") float_names = precision_32_df.select_dtypes(include=["float"]).columns precision_32_df.loc[:, float_names] = precision_32_df[float_names].apply( pandas.to_numeric, downcast="float") str2category_df = precision_32_df.copy() str_names = str2category_df.select_dtypes(include=["object"]).columns.tolist() for field_name in str_names: if field_name in ("register_time", "birth_day"): continue str2category_df[field_name] = str2category_df[field_name].astype( "category") return str_df, precision_64_df, precision_32_df, str2category_df 测试验证测试主要为内存使用大小的信息打印，打印 DataFrame 类型内存使用情况的函数与测试函数如下，其中 category 类型特征产生函数通过 str_size 控制产生的字符串长度。12345678910111213141516def memory_usage(df): print(df.info(memory_usage='deep'))def memory_test(): for i in range(1, 5): print("==============================================================") print("DataSize is %s * %s !" % (5000*i, 300*i)) result = generate_data(data_size=(5000*i, 300*i), str_size=20) print("\nstr_df:") memory_usage(result[0]) print("\nprecision_64_df:") memory_usage(result[1]) print("\nprecision_32_df:") memory_usage(result[2]) print("\nstr2category_df:") memory_usage(result[3]) 分别测试数据量为 (5000*300)、(10000*600)、(15000*900) 及 (20000*1200) 四种情况下字符串长度为 5, 10, 20 的内存使用情况，结果如下 表1 字符串长度为 5 的 DataFrame 类型变量内存使用情况 str_df object(300) precision_64_df datetime64[ns](2) float64(98) int64(100) object(100) precision_32_df datetime64[ns](2) float32(98) uint8(100) object(100) str2category_df datetime64[ns](2) float32(98) uint8(100) category(100) (5000*300) 74.1 MB 48.6 MB 43.4 MB 2.9 MB (10000*600) 294.7 MB 194.5 MB 173.6 MB 11.6 MB (15000*900) 661.8 MB 437.7 MB 390.6 MB 26.0 MB (20000*1200) 1.1 GB 778.2 MB 694.4 MB 46.1 MB 表2 字符串长度为 10 的 DataFrame 类型变量内存使用情况 str_df object(300) precision_64_df datetime64[ns](2) float64(98) int64(100) object(100) precision_32_df datetime64[ns](2) float32(98) uint8(100) object(100) str2category_df datetime64[ns](2) float32(98) uint8(100) category(100) (5000*300) 86.1 MB 60.6 MB 55.4 MB 3.0 MB (10000*600) 342.4 MB 242.2 MB 221.3 MB 11.6 MB (15000*900) 769.1 MB 545.0 MB 497.9 MB 26.0 MB (20000*1200) 1.3 GB 968.9 MB 885.2 MB 46.1 MB 表3 字符串长度为 20 的 DataFrame 类型变量内存使用情况 str_df object(300) precision_64_df datetime64[ns](2) float64(98) int64(100) object(100) precision_32_df datetime64[ns](2) float32(98) uint8(100) object(100) str2category_df datetime64[ns](2) float32(98) uint8(100) category(100) (5000*300) 109.9 MB 84.4 MB 79.2 MB 3.0 MB (10000*600) 437.8 MB 337.6 MB 316.7 MB 11.7 MB (15000*900) 983.7 MB 759.6 MB 712.5 MB 26.1 MB (20000*1200) 1.7 GB 1.3 GB 1.2 GB 46.2 MB 结论 &amp;&amp; 分析结论由如上实验，我们可以得出如下结论： 字符串类型（即 object ）转换为其他对应所需类型时，如 datatime64，float，int； 根据数据精度要求，选择合适的类型存储，如 float64 -&gt; float32，int64 -&gt; uint8，object -&gt; category； category 类型可以有效降低因字符串长度带来的内存增长。 因此，在数据读入时，根据先验知识填入对应字段的数据类型可以有效降低数据在内存中的大小，主要是 dtype （E.g. {&#39;a&#39;: np.float64, &#39;b&#39;: np.int32} ）及 date_parser、parse_dates 三个参数。 pandas.read_csv 分析参考 pandas 中的 read_csv 函数，位于 ”pandas/io/parsers.py“ 文件中。在函数 _make_parser_function 中，先进行了参数的初步转换工作，如下所示。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748def _make_parser_function(name, sep=','): default_sep = sep def parser_f(filepath_or_buffer, sep=sep, delimiter=None, # ignore the others... float_precision=None): # Alias sep -&gt; delimiter. if delimiter is None: delimiter = sep if delim_whitespace and delimiter is not default_sep: raise ValueError("Specified a delimiter with both sep and" " delim_whitespace=True; you can only" " specify one.") if engine is not None: engine_specified = True else: engine = 'c' engine_specified = False if skip_footer != 0: warnings.warn("The 'skip_footer' argument has " "been deprecated and will be removed " "in a future version. Please use the " "'skipfooter' argument instead.", FutureWarning, stacklevel=2) kwds = dict(delimiter=delimiter, engine=engine, dialect=dialect, compression=compression, engine_specified=engine_specified, # ignore the others... skip_blank_lines=skip_blank_lines) return _read(filepath_or_buffer, kwds) parser_f.__name__ = name return parser_fread_csv = _make_parser_function('read_csv', sep=',')# 此处 Appender 与文档相关，对函数功能未产生影响read_csv = Appender(_read_csv_doc)(read_csv) 在 _read 函数中，产生按行读文件的通用 reader，并进行读取文件操作，如下代码所示，最终进行文件读操作的为 TextFileReader。若以迭代方式分批读数据，直接返回该类型对象，否则调用 read 方法获取数据并返回。1234567891011121314151617181920212223242526272829303132333435def _read(filepath_or_buffer, kwds): """Generic reader of line files.""" encoding = kwds.get('encoding', None) if encoding is not None: encoding = re.sub('_', '-', encoding).lower() kwds['encoding'] = encoding # compression 参数确认使用的文件是否为压缩文件 # &#123;‘infer’, ‘gzip’, ‘bz2’, ‘zip’, ‘xz’, None&#125;, default ‘infer’ compression = kwds.get('compression') compression = _infer_compression(filepath_or_buffer, compression) filepath_or_buffer, _, compression = get_filepath_or_buffer( filepath_or_buffer, encoding, compression) kwds['compression'] = compression if kwds.get('date_parser', None) is not None: if isinstance(kwds['parse_dates'], bool): kwds['parse_dates'] = True # Extract some of the arguments (pass chunksize on). iterator = kwds.get('iterator', False) chunksize = _validate_integer('chunksize', kwds.get('chunksize', None), 1) nrows = _validate_integer('nrows', kwds.get('nrows', None)) # Create the parser. parser = TextFileReader(filepath_or_buffer, **kwds) if chunksize or iterator: return parser try: data = parser.read(nrows) finally: parser.close() return data 在 TextFileReader 中数据读入如下所示。读入使用的分析引擎 engine 取值为 {&#39;c&#39;, &#39;python&#39;}，C 引擎更快但 Python 引擎的功能更完备一些。此处发现对文件读取进行了更底层的封装，构造返回的 DataFrame 对象也是以 DataFrame(col_dict, columns=columns, index=index) 方式构造的。数据类型的转换操作发生在解析引擎的底层（若数据类型参数为 None 时会在底层进行尝试解析）。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647class TextFileReader(BaseIterator): # ignore other methods... def read(self, nrows=None): if nrows is not None: if self.options.get('skipfooter'): raise ValueError('skipfooter not supported for iteration') ret = self._engine.read(nrows) if self.options.get('as_recarray'): return ret # May alter columns / col_dict index, columns, col_dict = self._create_index(ret) if index is None: if col_dict: # Any column is actually fine: new_rows = len(compat.next(compat.itervalues(col_dict))) index = RangeIndex(self._currow, self._currow + new_rows) else: new_rows = 0 else: new_rows = len(index) df = DataFrame(col_dict, columns=columns, index=index) self._currow += new_rows if self.squeeze and len(df.columns) == 1: return df[df.columns[0]].copy() return df def __next__(self): try: return self.get_chunk() except StopIteration: self.close() raise def get_chunk(self, size=None): if size is None: size = self.chunksize if self.nrows is not None: if self._currow &gt;= self.nrows: raise StopIteration size = min(size, self.nrows - self._currow) return self.read(nrows=size) 如此处使用 cdef 定义的 TextReader 类中的 _convert_with_dtype 方法用来进行类型的转换。因此在调用该函数时直接传入对应的数据类型可以提升读入的性能。 因此，由上述代码可知，其初始化的 DataFrame 类型使用 dict 传入数据，因此分析 DataFrame 使用 dict 初始化的部分。 pandas.DataFrame 分析参考 pandas 中的 DataFrame 类，位于 ”pandas/core/frame.py“ 文件中。在 data 参数为 dict 类型时，其初始化函数可以简化如下。DataFrame 继承关系为：DataFrame --&gt; NDFrame --&gt; PandasObject; SelectionMixin --&gt; StringMixin --&gt; Object; Object。在 NDFrame 中解释为 &quot;N 维 DataFrame 的类似物，存储多维度的尺寸可变，有标记的数据结构&quot;。其中 data 参数存储为 BlockManager 类型的数据（由上机器之心提到，BlockManager 类负责保留行列索引与实际块之间的映射关系，可以作为一个 API 提供了对底层数据的访问）。如下代码中，可以看到最终构建的为多个块对数据进行管理1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889# pandas.core.frame.pyclass DataFrame(NDFrame): # ignore some methods for understanding better def __init__(self, data=None, index=None, columns=None, dtype=None, copy=False): mgr = self._init_dict(data, index, columns, dtype=dtype) NDFrame.__init__(self, mgr, fastpath=True) def _init_dict(self, data, index, columns, dtype=None): """ Segregate Series based on type and coerce into matrices. Needs to handle a lot of exceptional cases. """ if columns is not None: columns = _ensure_index(columns) # GH10856 # raise ValueError if only scalars in dict if index is None: extract_index(list(data.values())) # prefilter if columns passed data = dict((k, v) for k, v in compat.iteritems(data) if k in columns) if index is None: index = extract_index(list(data.values())) else: index = _ensure_index(index) arrays = [] data_names = [] for k in columns: if k not in data: # no obvious "empty" int column if dtype is not None and issubclass(dtype.type, np.integer): continue if dtype is None: # 1783 v = np.empty(len(index), dtype=object) elif np.issubdtype(dtype, np.flexible): v = np.empty(len(index), dtype=object) else: v = np.empty(len(index), dtype=dtype) v.fill(np.nan) else: v = data[k] data_names.append(k) arrays.append(v) else: keys = list(data.keys()) if not isinstance(data, OrderedDict): keys = _try_sort(keys) columns = data_names = Index(keys) arrays = [data[k] for k in keys] return _arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype)def _arrays_to_mgr(arrays, arr_names, index, columns, dtype=None): """ Segregate Series based on type and coerce into matrices. Needs to handle a lot of exceptional cases. """ # figure out the index, if necessary if index is None: index = extract_index(arrays) else: index = _ensure_index(index) # don't force copy because getting jammed in an ndarray anyway arrays = _homogenize(arrays, index, dtype) # from BlockManager perspective axes = [_ensure_index(columns), _ensure_index(index)] return create_block_manager_from_arrays(arrays, arr_names, axes)# pandas.core.internals.pydef create_block_manager_from_arrays(arrays, names, axes): try: blocks = form_blocks(arrays, names, axes) mgr = BlockManager(blocks, axes) mgr._consolidate_inplace() return mgr except ValueError as e: construction_error(len(arrays), arrays[0].shape, axes, e) 探究可知，DataFrame 在内存中存储为多个块的形式（每个块为 numpy.array）类型，通过 BlockManager 对块进行数据的增删改查操作。每种数据类型有其对应的类，如上 ObjectBlock 表示字符串块，FloatBlock 表示浮点数列块。这些块是连续存储的。因此通过降低单个数值存储的大小有效降低了其内存的使用。 数据预处理 数据分析中，将数据进行清洗变换得到一份新的数据以进行后续分析训练是常见且高频的操作，此处主要产生新数据的方式进行测试分析。 模拟预处理此处代码主要模拟了两种预处理行为，一列变换为一列（如对数变换，归一化变换，异常值缺失值过滤等情况），一列变为多列（如 OneHot 编码，时间序列分解等）。为方便后续插入到 DataFrame 中，返回为 list 类型的列名与列变换后的值。123456789101112131415import numpydef generate_one(field_name, length=5000): col_arr = numpy.random.randint(0, 10, size=[length]) return [field_name], [col_arr]def generate_more_than_one(field_name, length=5000): col_nums = numpy.random.randint(2, 7) col_arr = numpy.random.randint(0, 2, size=[length, col_nums]) result_names = list() result_values = list() for idx in range(col_nums): result_values.append(col_arr[:, idx]) result_names.append("_".join([field_name, str(idx)])) return result_names, result_values 在列数较多时，随机的方式测试效果更平均，接近于真实项目中的情况。 预处理方式实现测试了两种存储预处理之后列数据的方式，分别为 原有 DataFrame 变量中调用 reindex 方法加入到原有 DataFrame 变量中； 原有 DataFrame 变量中逐列加入新列； 保存预处理后的列名与列值，构建列名到列值的映射，调用 DataFrame 初始化方法构建新变量。 代码实现如下12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273import numpyimport pandasdef analog_pre_process_reindex(data_size=(5000, 300)): length, cols = data_size processed_df = pandas.DataFrame() processed_df["target"] = numpy.random.randint(0, 3, size=[length]) # generate the test data length * cols include the target # but result maybe has more cols # default OneHot:Standard=1:1 cur_field_names = processed_df.columns.tolist() for field_index in range(1, cols): field_name = str(field_index) if field_index % 2 == 0: processed_names, processed_values = generate_one( field_name=field_name, length=length) else: processed_names, processed_values = generate_more_than_one( field_name=field_name, length=length) cur_field_names.extend(processed_names) processed_df = processed_df.reindex( columns=cur_field_names, copy=False) processed_df[processed_names] = numpy.array(processed_values).T return processed_dfdef analog_pre_process_one_to_df(data_size=(5000, 300)): length, cols = data_size processed_df = pandas.DataFrame() processed_df["target"] = numpy.random.randint(0, 3, size=[length]) # generate the test data length * cols include the target # but result maybe has more cols # default OneHot:Standard=1:1 for field_index in range(1, cols): field_name = str(field_index) if field_index % 2 == 0: processed_names, processed_values = generate_one( field_name=field_name, length=length) else: processed_names, processed_values = generate_more_than_one( field_name=field_name, length=length) for index in range(len(processed_names)): one_field_name = processed_names[index] one_field_values = processed_values[index] processed_df[one_field_name] = one_field_values return processed_dfdef analog_pre_process_list(data_size=(5000, 300)): length, cols = data_size col_names = ["target"] col_values = [numpy.random.randint(0, 3, size=[length])] # generate the test data length * cols include the target # but result maybe has more cols # default OneHot:Standard=1:1 for field_index in range(1, cols): field_name = str(field_index) if field_index % 2 == 0: processed_names, processed_values = generate_one( field_name=field_name, length=length) else: processed_names, processed_values = generate_more_than_one( field_name=field_name, length=length) col_names.extend(processed_names) col_values.extend(processed_values) fields_num = len(col_names) data_dict = &#123;col_names[idx]: col_values[idx] for idx in range(fields_num)&#125; return pandas.DataFrame(data_dict) 此处，特征数量中一列变换到一列与一列变换到多列的比例为1:1，原有 DataFrame 变量中只有目标分类列（由数据产生方式可看出为三分类数据）。 测试验证测试包含时间与内存消耗两部分，时间测试函数与主函数模块如下123456789101112131415161718from datetime import datetimedef record_time(func, data_size): start_time = datetime.now() res = func(data_size) end_time = datetime.now() return res, (end_time - start_time).total_seconds()if __name__ == "__main__": for i in range(1, 5): print("==============================================================") print("DataSize is %s * %s !" % (5000*i, 300*i)) df, cost_time = record_time( func=analog_pre_process_one_to_df, data_size=(5000 * i, 300 * i)) # func=analog_pre_process_reindex, data_size=(5000 * i, 300 * i)) # func=analog_pre_process_list, data_size=(5000 * i, 300 * i)) del df print("CostTime = %s seconds!" % cost_time) 为避免同时测试对性能的影响，此处对每种方式均单独测试运行，分别测试数据量为 (5000*300)、(10000*600)、(15000*900) 及 (20000*1200) 数据量的情况，时间代价如下（单位为秒）。 表4 保存预处理结果时间消耗统计表 analog_pre_process_reindex analog_pre_process_one_to_df analog_pre_process_list (5000*300) 5.89472 0.292583 0.069674 (10000*600) 48.151761 1.250182 0.215701 (15000*900) 160.599144 3.279289 0.462308 (20000*1200) 763.805195 6.772731 0.771077 内存测试使用 memory_profiler 库函数，对检测函数 func_name 进行内存分析的使用方式如下。此处修饰符 @profile(precision=6) 精度为 6 位小数。123456from memory_profiler import profile@profile(precision=6)def func_name(param...): pass 运行三次分别得到的结果对比如下图所示 此处时间略大于不进行内存测试时的时间，因为使用该模块进行内存监控会降低代码的运行速度，属于正常现象。 结论 &amp;&amp; 分析结论由上述测试结果明显可以得出结论： 将获取到的所有预处理数据统一初始化构造为 DataFrame 对象是比向 DataFrame 变量中插入数据具有更高效的处理方式 在数据预处理中逐次添加一列或多列数据到 DataFrame 均会带来内存的增加与执行速度的降低。 分析由 DataFrame 初始化中源码分析可知，在该类型变量底层是通过 numpy.array 申请的连续内存块存储数据的，因此添加新列需要重新申请内存并释放旧内存，在频繁加入新内存时即带来了内存的额外消耗和性能下降。 环境说明此处实验的计算机硬件配置为1234CPU: CPU Intel(R) Core(TM) i7-4770 CPU @ 3.40GHz# 但并为用到 GPU 加速等相关操作GPU: VGA compatible controller: NVIDIA Corporation GM206 [GeForce GTX 960] (rev a1)内存: 16GB软件环境为12345OS: Ubuntu-16.04Python: 3.5.3numpy: 1.13.1pandas: 0.22.0memory-profiler: 0.54.0 后记最近在做 Python 机器学习项目有关内存优化的工作，本文主要是优化过程中与 Pandas 相关的优化，通过该方式已经有效降低了与此相关的内存使用，效果明显，因此记录下来以供大家参考。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>内存优化</tag>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3.5.6 multiprocessing 用户文档]]></title>
    <url>%2F2018%2F11%2F16%2FPython3-5-6-multiprocessing-%E7%94%A8%E6%88%B7%E6%96%87%E6%A1%A3%2F</url>
    <content type="text"><![CDATA[最近在做 Python 内存优化时，发现 multiprocessing 可以充分利用 CPU 的多核特性，且可以在执行完任务后释放所有的资源，只保留需要的结果即可，可以避免 Python 的垃圾回收过慢的弊病，同时带来执行效率的提升。同时，之前的 Python 资料书籍更关注于 Python 多线程部分，而对多进程的讲解应用较少。于是开始阅读该部分文档和源码，本篇博客是其官网文档 multiprocessing 的翻译（原文地址）。 Introductionmultiprocessing 是一个支持产生新进程的 package，API 使用方式与 threading 模块相似。multiprocessing 可以提供本地/远程的并发，通过使用子进程而不是线程绕开 GIL 的约束（充分利用 CPU 多核提升效率）。因此，multiprocessing 模块可以支持程序充分利用给定计算机上的多个处理器。Unix/Windows 环境均可以运行。 multiprocessing同样引入了 threading中没有的 APIs。一个主要的例子如 Pool，提供了便捷的方法实现跨多个输入值的函数并行执行，可以跨进程分配输入的数据（数据并行）。下面的例子演示Pool在模块中定义这样一个函数的常见用法，以便子进程可以成功导入该模块，实现数据的并行性。 12345678from multiprocessing import Pooldef f(x): return x*xif __name__ == '__main__': with Pool(5) as p: print(p.map(f, [1, 2, 3])) 打印结果如下1[1, 4, 9] Process 类multiprocessing通过创建一个 Process 对象调用其 start() 方法产生子进程。Process 的 API 与 threading.Thread一致，一个多进程的例子如下所示。123456789from multiprocessing import Processdef f(name): print('hello', name)if __name__ == '__main__': p = Process(target=f, args=('bob',)) p.start() p.join() 为了显示每个进程的独立 ID，一个扩展的程序实例如下。123456789101112131415161718from multiprocessing import Processimport osdef info(title): print(title) print('module name:', __name__) print('parent process:', os.getppid()) print('process id:', os.getpid())def f(name): info('function f') print('hello', name)if __name__ == '__main__': info('main line') p = Process(target=f, args=('bob',)) p.start() p.join() 如果读者需要了解 if __name__ == &#39;__main__&#39; 为什么在这里是必须的，可以查看 Programming Guidlines。 Context 与 start 方法依据运行环境的不同（Unix/Windows），multiprocessing 支持三种方式创建子进程，这三种方式是 spawn父进程开启一个全新的 Python 解释器进程。子进程仅继承运行进程对象的 run() 方法所必须的资源。特别的是，不会从父进程中继承不需要的文件描述和文件句柄。与下面两种方法相比，此方法启动比较慢。Unix 与 Windows 环境均存在该方法，Windows 下默认使用该方法。 fork父进程使用 os.fork() 复制 Python 解释器。子进程开始运行时实际是与父进程一样的。父进程的所有资源均会被子进程继承。注意拷贝一个多线程的进程可能会有问题。仅 Unix 环境存在该方法，Unix 默认使用该方法。 forkserver当程序开始运行并选择了 forkserver 方法，将开启一个服务进程。从此时起，每当需要一个新进程时，父进程将连接到服务进程并请求它复制一个新进程。服务进程是单线程的因此使用 os.fork() 更安全。不必要的资源不会被继承。仅支持 Unix 管道通信传输文件描述符的 Unix 平台支持该方法。 在 3.4 版本之后，所有的 Unix 环境支持了spawn 方法，部分 Unix 环境支持了 forkserver 方法，Windows 环境下子进程不再继承父进程的所有可继承的句柄。 Unix 环境下使用 spawn 或 forkserver 方法同样会开启一个信号量跟踪器进程跟踪由程序创建的未连接的命名信号量。当所有的进程退出时，信号量跟踪器取消链接任何剩余的信号量。一般情况下是没有的，但如果一个进程被信号量杀死，可能会有信号量泄漏。（没有链接的命名信号量是一个严重的问题，因为操作系统只允许有限数量的信号量且直至下次重启不会主动取消链接。） 猜测：该方式只拷贝了父进程的信号量而没有拷贝文件描述符，因此是未连接的命名信号量，文件描述符或句柄由父进程的信号量连接。正常情况下，进程结束时，信号跟踪器会负责销毁该信号量，当意外退出或被 kill -9 杀死时，未能完成信号量的回收操作导致信号量泄漏。 可以在主函数的 if __name__ == &#39;__main__&#39; 从句中使用 set_start_method() 方法选择启动方法，如下实例。123456789101112import multiprocessing as mpdef foo(q): q.put('hello')if __name__ == '__main__': mp.set_start_method('spawn') q = mp.Queue() p = mp.Process(target=foo, args=(q,)) p.start() print(q.get()) p.join() 程序中 set_start_method() 方法至多被使用一次，如像上述实例中，使用多次会报错 1RuntimeError: context has already been set. 或者，可以使用 get_context() 获取一个 context 对象，context 对象作为 multiprocessing 模块的一员，具有和该模块同样的 API，并允许在同一程序中使用多次 start() 方法。123456import multiprocessing as m ctx = mp.get_context('spawn')q = ctx.Queue()p = ctx.Process(target=foo, args=(q,))p.start()print(q.get())p.join() 注：与 context 相关的对象可能与不同 context 的进程不兼容。特别是使用 fork() 方法的 context 创建的锁不能传递给使用 spawn 或 forkserver 方法启动的进程。 使用特殊启动方式的库应该使用 get_context() 避免干扰库的使用者对启动方法的选择。 进程间信息交换进程间 multiprocessing 支持两种类型的交流通道：Queue 和 Pipe 。 QueueQueue 差不多克隆自 queue.Queue。代码实例如下1234567891011from multiprocessing import Process, Queuedef f(q): q.put([42, None, 'hello'])if __name__ == '__main__': q = Queue() p = Process(target=f, args=(q,)) p.start() print(q.get()) # prints "[42, None, 'hello']" p.join() Queue 是线程/进程安全的。 PipesPipe() 函数返回一对由管道连接的连接对象（默认是双工的）。代码实例如下123456789101112from multiprocessing import Process, Pipedef f(conn): conn.send([42, None, 'hello']) conn.close()if __name__ == '__main__': parent_conn, child_conn = Pipe() p = Process(target=f, args=(child_conn,)) p.start() print(parent_conn.recv()) # prints "[42, None, 'hello']" p.join() 两个连接对象代表了管道的两端。每一个连接对象都有 send() 和 recv() （彼此间）。 注：如果两个进程（或线程）同时读取或写入管道同一端的数据，则数据可能被损毁。当然，进程使用管道不同端不存在数据损坏的风险。 进程间同步multiprocessing 模块包含 threading 所有同步原语的等价语句。例如我们可以使用 lock 保证同一时刻只有一个进程在进行标准输出。12345678910111213from multiprocessing import Process, Lockdef f(l, i): l.acquire() try: print('hello world', i) finally: l.release()if __name__ == '__main__': lock = Lock() for num in range(10): Process(target=f, args=(lock, num)).start() 如果不使用 lock ，来自不同进程的输出可能会混淆不清。 进程间状态共享如上所述，在进行并发编程时，尽量不使用状态共享，多线程编程时尤其如此。然而，如果你确实需要共享数据，multiprocessing 提供了两种方式实现。 内存共享数据可以以 Value 或者 Array 的方式在内存中共享。代码实例如下123456789101112131415from multiprocessing import Process, Value, Arraydef f(n, a): n.value = 3.1415927 for i in range(len(a)): a[i] = -a[i]if __name__ == '__main__': num = Value('d', 0.0) arr = Array('i', range(10)) p = Process(target=f, args=(num, arr)) p.start() p.join() print(num.value) print(arr[:]) 打印结果如下123.1415927[0, -1, -2, -3, -4, -5, -6, -7, -8, -9] 创建变量 num 和 arr 时使用的参数 &#39;d&#39; 和 &#39;i&#39; 指定数据类型的代码：&#39;d&#39; 表示双精度浮点型，&#39;i&#39; 表示有符号整型。共享对象是进程/线程安全的。 为了更灵活地使用内存共享，可以使用 multiprocessing.sharedctypes 模块，该模块支持从共享内存分配出来的任意 ctypes 对象。 服务进程Manager() 返回的 manager 对象管理一个服务进程持有 Python 对象且允许其他进程使用代理操作这些对象，支持 list, dict, Namespace, Lock, RLock, Semaphore, BoundedSemaphore, Condition, Event, Barrier, Queue, Value 和 Array。代码示例如下1234567891011121314151617from multiprocessing import Process, Managerdef f(d, l): d[1] = '1' d['2'] = 2 d[0.25] = None l.reverse()if __name__ == '__main__': with Manager() as manager: d = manager.dict() l = manager.list(range(10)) p = Process(target=f, args=(d, l)) p.start() p.join() print(d) print(l) 打印结果如下12&#123;0.25: None, 1: '1', '2': 2&#125;[9, 8, 7, 6, 5, 4, 3, 2, 1, 0] 使用服务进程 manager 比使用内存共享更灵活，因为 manager 可以支持任意对象类型。此外，单个 manager 可以通过网络在不同计算机上实现进程共享。但是，manager 方式比使用内存共享慢。 使用 Pool workersPool 对象表示一个进程池，其允许以几种不同的方式将任务装载到进程池。代码示例如下 123456789101112131415161718192021222324252627282930313233from multiprocessing import Pool, TimeoutErrorimport timeimport osdef f(x): return x*xif __name__ == '__main__': # start 4 worker processes with Pool(processes=4) as pool: # print "[0, 1, 4,..., 81]" print(pool.map(f, range(10))) # print same numbers in arbitrary order for i in pool.imap_unordered(f, range((10)): print(i) # evaluate "f(20)" asynchroously res = pool.apply_async(f, (20,)) # runs in *only* one process print(res.get(timeout=1)) # prints "400" # evaluate "os.getpid()" asynchronously res = pool.apply_async(os.getpid, ()) # runs in *only* one process print(res.get(timeout=1)) # prints the PID of that process # launching multiple evaluations asynchronously *may* use more processes multiple_results = [pool.apply_async(os.getpid, ()) for i in range(4)] print([res.get(timeout=1) for res in multiple_results]) # make a single worker sleep for 10 secs res = pool.apply_async(time.sleep, (10,)) try: print(res.get(timeout=1)) except TimeoutError: print("We lacked patience and got a multiprocessing.TimeoutError") print("For the moment, the pool remains available for more work") # exiting the 'with'-block has stopped the pool print("Now the pool is closed and no longer available") 注：Pool 的方法只能由创建它的进程使用。Tips: 该包中的功能要求 __main__ 模块可由子项导入。在 Programming Guidelines 中有所涉及，但值得在此指出。这意味着某些例子，如下 multiprocessing.pool.Pool 的实例在交互式解释器中不起作用。 1234567891011121314​from multiprocessing import Pool def f(x): return x*xp = Pool(5)p.map(f, [1,2,3])# Process PoolWorker-1:# Process PoolWorker-2:# Process PoolWorker-3:# Traceback (most recent call last):# AttributeError: 'module' object has no attribute 'f'# AttributeError: 'module' object has no attribute 'f'# AttributeError: 'module' object has no attribute 'f'如果你尝试运行该代码，会以半随机的方式输出三个完整的回溯，然后不能不以某种方式停止主进程。 Referencemultiprocessing 模块主要复制了 threading 模块的 API。 Process &amp;&amp; Exceptionsmultiprocessing.Process(group=None, target=None, name=None, args=(), kwargs={}, **, daemon=None*) Process 对象表示另一个进程中的活动。Process 类具有 threading.Thread 的所有方法的等价方法。 应该始终使用关键字参数调用构造函数。参数 group 应该始终是 None，它仅为了与 threading.Thread 构造参数一致。参数 target 是 run() 方法的调用函数，默认为 None，即不调用任何内容。参数 name 是进程的名字，详细情况见下参数部分。参数 kwargs 是目标函数调用的参数关键字字典，若提供了该参数，将关键字参数 daemon 设置为 True/False，若为默认值 None，则从创建该进程的进程继承该参数。默认情况下，不会将任何参数传递给 target。 如果子类重写构造函数，必须确保在对进程执行任何动作之前先调用 Process.__init__() 方法。 版本 3.3 后添加了 daemon 参数。 构造函数源码如下所示。1234567891011121314151617# cpython 3.5.2 /Lib/multiprocessing/process BaseProcess 类def __init__(self, group=None, target=None, name=None, args=(), kwargs=&#123;&#125;, *, daemon=None):​ assert group is None, 'group argument must be None for now'​ count = next(_process_counter)​ self._identity = _current_process._identity + (count,)​ self._config = _current_process._config.copy()​ self._parent_pid = os.getpid()​ self._popen = None​ self._target = target​ self._args = tuple(args)​ self._kwargs = dict(kwargs)​ self._name = name or type(self).__name__ + '-' + \ ':'.join(str(i) for i in self._identity)​ if daemon is not None:​ self.daemon = daemon​ _dangling.add(self) 通过该函数我们可以清晰了解到进程的命名规则，如下有详细介绍。 run()表示进程动作（即进程的执行任务）的方法。分别按顺序使用 args 和 kwargs 参数中的关键字参数。 start()进程启动的方法。该方法在每一个进程中至多被调用一次，其在一个单独进程中调用 Process 对象的 run() 方法。 join([timeout])若可选参数 timeout 为 None （默认值），该方法会阻塞进程直至调用该方法的进程终止；若可选参数 timeout 为整数 N，该方法会阻塞进程 N 秒；注：若方法进程终止或超时，返回 None。可以通过检查进程的 exitcode 属性值确认进程是否终止。 name进程名称。name 是字符串类型，仅用于识别目的，没有语义。多个进程可以使用相同的名字。初识名字由构造器设置，若构造器没有提供显示名字，按照 &quot;Process-{N1:N2:...:Nk}&quot; 形式构造名字，其中 &quot;Nk&quot; 表示父进程的第 Nk 个子进程。 is_alive()返回进程是否活着。粗略的说，从进程调用 start() 方法到进程终止之间进程状态时活着的。 daemon进程守护进程运行标志，Boolean 类型。必须在调用 start() 方法之前设置 - 这样可以变为守护进程。初始值继承自父进程（即创建进程）。当进程退出时，它会尝试其终止其守护的所有子进程。 注：守护进程不允许创建子进程。否则一个后台进程退出时会终止其子进程。当然，这些子进程不是 Unix 守护进程或服务，它们是可以正常终止的进程（不调用 join() 方法阻塞等待）。 除了 threading.Thread API 之外，Process 还支持如下属性或方法pid返回进程 ID，进程生成之前返回 None。 exitcode子进程退出码。None 表示进程尚未终止；返回负整数 -N 表示进程被信号 N 终止；返回0表示没有错误正常退出；返回正数表示进程有错误，并以错误码为状态码退出。 authkey程序的身份认证秘钥是一个 byte 类型的字符串。multiprocessing 初始化时，会使用 os.urandom() 方法为主进程分配一个随机字符串。当创建 Process 对象时，它会继承父进程的 authkey，可以通过设置 authkey 来改变它。 sentinel系统对象的数字句柄，当进程结束时状态变为 ready。一次等待多个事件时可以使用 multiprocessing.connection.wait()，否则使用 join() 方法会更简单。 Windows 环境下这是一个系统句柄调用 WaitForSingleObject 和 WaitForMultipleObjects API 族；Unix 环境下是一个文件描述符，与 select 模块中的原语配合使用。 terminate()终止进程。Unix 环境下使用 SIGTERM 信号量完成；Windows 环境下调用 TerminatedProcess() 方法实现。但不会执行退出句柄和剩余子句。注：进程的后裔进程不会被终止，后裔进程将称为孤儿进程。 警告：如果相互关联的进程正在使用管道或队列时，调用了该方法，管道或队列可能会被损毁且无法被其他进程使用。类似的，如果进程已获得了锁或者信号量，终止它可能导致其他进程的死锁。 注：start()，join()，is_alive()，terminate() 和 exitcode 这些方法应该仅由父进程对象调用。 如下是一个 Process 方法的使用实例。 12345678910111213import multiprocessing, time, signalp = multiprocessing.Process(target=time.sleep, args=(1000,))print(p, p.is_alive())# &lt;Process(Process-1, initial)&gt; Falsep.start()print(p, p.is_alive())# &lt;Process(Process-1, started)&gt; Truep.terminate()time.sleep(0.1)print(p, p.is_alive())# &lt;Process(Process-1, stopped[SIGTERM])&gt; Falsep.exitcode == -signal.SIGTERM# True 文档中这里介绍了 multiprocessing 中四种异常。exception multiprocessing.ProcessErrormultiprocessing 异常类型的基类。 exception multiprocessing.BufferTooShort当提供的缓存对读取的消息来说太小时，由 Connection.recv_bytes_into() 方法抛出的异常。 exception multiprocessing.AuthenticationError认证失败抛出的异常。 exception multiprocessing.TimeoutError超时抛出的异常。 Pipes &amp;&amp; Queues通常多进程之间通信应该使用消息传递，尽量避免使用任何同步原语（如锁）。 消息传递可以使用 Pipe() （两个进程间通信）或 queue（多个生产者和消费者之间通信）。Queue、SimpleQueue 和 JoinableQueue 类型是基于标准库中 queue.Queue 类的基础上构建的多生产者，多消费者 FIFO 队列。不同之处在于 Queue 与 queue.Queue 相比缺少 task_done() 和 join() 方法（该方法在 Python 2.5 之后的版本引入）。 如果使用 JoinableQueue 则一定要为从队列中删除的每个任务调用 JoinableQueue.task_done()，否则可能导致用于统计未完成任务数量的信号量溢出，从而引发异常。 注：也可以通过 Manager 对象创建一个共享队列。 Tip：multiprocessing 使用 queue.Empty 和 queue.Full 异常发出超时信号。这些不能在 multiprocessing 的命名空间中找到，因此需要从 queue 中引用进来。 Tip：当一个对象被放入队列时，对象是被序列化了的，之后后台线程会把序列化之后的数据缓冲到一个底层管道。这可能是一个令人惊讶的结果，但不会造成任何困难。如果这个操作会影响到你的任务，可以使用 manager。将一个对象放入到空队列后，可能有一个无限小的延迟队列的 empty() 方法返回 True 。get_nowait() 方法可以在不抛出 queue.Empty 异常的情况下返回队列状态。1234567891011121314151617181920212223242526272829303132# CPython 3.5.2 /Lib/multiprocessing/queues.py## Queue type using a pipe, buffer and thread#class Queue(object): # ... ignore other methods def get(self, block=True, timeout=None): if block and timeout is None: with self._rlock: res = self._recv_bytes() self._sem.release() else: if block: deadline = time.time() + timeout if not self._rlock.acquire(block, timeout): raise Empty try: if block: timeout = deadline - time.time() if timeout &lt; 0 or not self._poll(timeout): raise Empty elif not self._poll(): raise Empty res = self._recv_bytes() self._sem.release() finally: self._rlock.release() # unserialize the data after having released the lock return ForkingPickler.loads(res) def get_nowait(self): return self.get(False)如果多个进程同时向队列中放入对象，在另一端接收到的对象很可能是无序的。但是，由同一个进程放入队列中的对象始终按预期的顺序相互关联。 警告：如果一个进程正在使用一个队列的时候通过使用 Process.terminate() 或 os.kill() 终止了进程，队列中的数据很可能被损毁。这可能导致任何尝试使用该队列中数据的进程抛出异常。 警告：如上所述，子进程向队列中放入信息（且没有使用 JoinableQueue.cancel_join_thread 方法），进程在将数据缓冲到管道之前不会终止。 这意味着如果调用进程的 join() 方法在队列中仍有数据时可能造成死锁。类似地，若子进程是非后台进程，当父进程尝试调用 join() 方法等待它的非后台子进程终止时，父进程可能在退出时挂起。 通过使用 manager 创建的队列不会存在该问题。参与 Programming guidelines。 有关使用队列在进程间通信的代码实例，可以查看 Examples 部分。 multiprocessing.Pipe([duplex])返回一对代表 connection 对象通道两端的变量 (conn1, conn2)。若参数 dumplex 为 True 通道是双工的（默认为 True），若该参数为 False 则通道是单向的，conn1 只能用来接收消息，conn2 只能用来发送消息。 multiprocessing.Queue([maxsize])返回一个进程，其使用管道和一些锁/信号量实现了共享队列。当进程首次将数据放入队列时，会启动将数据对象从缓存传输到队列的进给线程。标准库 queue 模块中的 queue.Empty 和 queue.Full 异常会引发超时信号。Queue 实现了 queue.Queue 中除 task_done() 和 join() 外的所有方法。 qsize()返回队列的近似大小。因为多线程/多进程影响，该数字是不可靠的。注：该方法在像 MacOSX 这样未实现 sem_getvalue() 的 Unix 平台可能引发 NotImplementedError 异常。 empty()如果队列为空返回 True 否则返回 False。由于多线程/多进程影响，返回值不可靠。 full()如果队满返回 True 否则返回 False。由于多线程/多进程影响，返回值不可靠。 put(obj[, block[, timeout]])将对象放入队列。若可选参数 block 是 True（默认值）且 timeout 参数为 None（默认值），若没有空间插入对象会发生阻塞直到可以插入对象 。若 timeout 参数是一个正整数 N，其会至多阻塞 N 秒还没有空间可以插入时抛出 queue.Full异常。若可选参数 block 是 False 会直接向队列中放入元素，若没有空间直接抛出 queue.Full 异常（该情况下 timeout 参数被忽略）。 put_nowait(obj)等价于 put(obj, False) 方法。 get([block[, timeout]])从队列中删除并返回一个数据对象。 get_nowait()等价于 get(False) 方法。 multiprocessing.Queue 还有一些 queue.Queue 中没有的方法。这些方法在大多数情况下用不到。 close()表示当前进程不会再将数据放入该队列。一旦后台线程将所有的数据缓冲到管道中，后台线程退出。当 Queue 实例被垃圾回收时自动调用该方法。 join_thread()join 后台线程。该方法只能在调用 close() 方法后使用，将阻塞至后台线程的退出以确保缓存的所有数据均缓冲到管道中。默认情况下，非 Queue 创建者的进程退出时会调用该方法，可以通过调用 cancel_join_thread() 使该操作失效。 cancel_join_thread()阻止 join_thread() 方法阻塞。特别的是，可以阻止后台线程在进程中退出时进程对线程的 join 操作 - 参考上面 join_thread() 方法。allow_exit_without_flush() 的名字更适合该方法，其可能会导致进入队列中的数据消失，因此基本不需要使用该方法。只有在当前进程需要立即退出，并不关心丢失的数据，不再等待后台线程将缓存数据缓冲到底层管道的情况下，使用该方法。 注：该类的方法依赖于主机操作系统支持共享信号量功能。若不支持，该类中的方法将被禁用，尝试实例化 Queue 将会导致 ImportError 异常。参阅 bpo-3770 获取更详细的信息。对下面列出的其他特殊 Queue 类型具有同样的要求。 multiprocessing.SimpleQueue类似于加锁 Pipe 类型的 Queue 简单实现。 empty()若队列为空返回 True 否则返回 False。 get()从队列中移除并返回数据对象。 put(item)将数据对象放入队列。 multiprocessing.JoinableQueue([maxsize])Queue 类型的子类，添加了 task_done() 和 join() 方法。 task_done()表明之前排队过的任务已经完成。由队列的消费者使用。对于每个使用 get() 方法获取任务的消费者后续调用该方法告知队列任务完成。若调用 join() 方法，会阻塞至队列中的所有任务均被完成（即调用 put() 方法放入队列中的每个任务都收到了 task_done 回调）。若该方法的调用次数超过了队列中放置的任务数量，会引发 ValueError 异常。 join()阻塞至放入到队列中的所有任务均被处理完成。每当有新的任务加入到队列中，未完成的任务数量就会增加。每当消费者调用了 task_done() 告知队列任务完成，未完成的任务数量就会下降。当未完成任务数量的计数将至零时，该方法的阻塞不再阻塞。 Miscellaneousmultiprocessing.active_children()返回当前进程活跃的子进程列表。使用该函数会对已经结束的子进程调用 poll() 方法的副作用。1234567# CPython 3.5.2 /Lib/multiprocessing/queues.pydef active_children(): children = current_process()._children for p in list(children): if not p.is_alive(): children.pop(p, None) return list(children) multiprocessing.cpu_count()返回系统的 CPU 数量，可能抛出 NotImplementedError。可同时参阅 os.cpu_count() multiprocessing.current_process()返回与当前进程一致的进程对象。与 threading.current_thread() 类似。 multiprocessing.freeze_support()添加了冻结程序的多进程的支持以产生 Windows 可执行文件。（已使用 py2exe，PyInstaller 和 cx_Freeze 进行过测试。）需要在主函数的 if __name__ == &#39;__main__&#39;: 代码后调用该函数，实例如下12345678from multiprocessing import Process, freeze_supportdef f(): print('hello world!')if __name__ == '__main__': freeze_support() Process(target=f).start() 若省略了 freeze_support() 行代码去执行冻结的可执行文件会抛出 RuntimeError 异常。在 Windows 系统外任何其他系统上运行该函数无效；若 Windows 环境下 Python 解释器正常运行模块（程序没有被冻结），该函数无效。 multiprocessing.get_all_start_methods()返回所有支持的 start() 方式列表，第一个是默认方式。可能的方式有 fork、spawn 和 forkserver。Windows 系统下只支持 spawn 方式。Unix 系统下支持 fork 和 spawn 方式，默认是 fork 方式。版本 3.4 后添加。 multiprocessing.get_context(method=None)返回与 multiprocessing 模块相同属性的 context 对象。若参数 method 为 None，返回默认 context；否则参数 method 应该为 fork，spawn，forkserver 中一个。若 start() 的方式系统不支持会抛出 ValueError 异常。版本 3.4 后添加。 multiprocessing.get_start_method(allow_none=False)返回启动进程方式的名称。若 start() 方式未确定且参数 allow_none 是 False，start() 启动方式被设置为默认值并返回默认方式的名称。若 start() 方式没有被确定且参数 allow_none 是 True 返回 None。返回值可以是 fork，spawn，forkserver 或 None。Unix 默认 fork，Windows 默认 spawn。版本 3.4 后添加。 multiprocessing.set_executable()设置启动子进程时 Python 解释器路径。（默认使用 sys.executable）。嵌入式在子进程创建之前，很可能需要做类似于如下的事情。1set_executable(os.path.join(sys.exec_prefix, 'pythonw.ext')) 版本 3.4 之后 Unix 支持使用 spawn 方式启动子进程。 multiprocessing.set_start_method(method)设置子进程的 start() 方式，可以是 fork，spawn 或者 forkserver。注意，该函数至多被调用一次，且应该在主模块的 if __name__ == &#39;__main__&#39;: 子句中进行保护。版本 3.4 后添加。 注：multiprocessing 不包含如下类似方法 threading.active_count()、threading.enumerate()、threading.settrace()、threading.setprofile()、threading.Timer 和 threading.local。 Connection ObjectsConnection 对象可以发送和接收序列化的对象或字符串，它们可以被看做是面向消息的套接字。Connection 对象一般使用 Pipe() 方法创建 - 参阅 Listeners &amp;&amp; Clients class multiprocessing.Connectionsend(obj)向连接的另一端发送一个对象，另一端应该使用 recv() 接收。传输的对象一定可以被序列化。过大的序列化对象可能抛出 ValueError 异常（大约 32 MB+，取决于操作系统）。 recv()返回由连接另一端使用 send() 方法发送来的对象。如果没有对象传输过来调用该方法会产生阻塞直至接收到对象。若没有任何对象发送给该端接收而另一发送端关闭，会抛出 EOFError 异常。 fileno()返回连接使用的文件描述符或句柄。 close()关闭连接。连接被垃圾回收时被自动调用。 poll([timeout])返回是否有可以被读取的数据。若 timeout 没有指定，则立即返回；若 timeout 是数字 N，则指定阻塞的时间最长为 N 秒；若 timeout 为 None，则阻塞的时间为无限长。注：可以使用 multiprocessing.connection.wait() 方法一次轮训多个连接对象。 send_bytes(buffer[, offset[, size])将字节类型对象作为完整消息发送。若设置了 offset 参数将会缓存中该偏移位置读取数据。若设置了 size 参数将会从缓存中读取多个字节。太大的缓存区（约为 32MB+，取决于操作系统）可能抛出 ValueError 异常。 recv_bytes([maxlength])将从连接另一端发送的字节类型的数据对象作为字符串返回。阻塞至有数据对象收到。若没有剩余可接收数据对象且连接发送端已经关闭会抛出 EOFError 异常。若指定了 maxlength 参数且消息对象的长度大于该参数数值，将抛出 OSError 异常，且该连接不再可读。版本 3.3 后变更：该方法引发的 IOError 异常是 OSError 异常的别称。 recv_bytes_into(buffer[, offset])读入从连接发送端发送的字节类型数据对象完整消息并返回数据对象的字节数。阻塞至有数据对象收到。若没有剩余可接收数据对象且连接发送端已经关闭会抛出 EOFError 异常。参数 buffer 必须是可写的字节类型对象。若给定 offset 参数，将会从缓存中该偏移位置写入数据，该参数是小于 buffer 长度的非负整数。若缓冲区太小会引发 BufferTooShort 异常，且完整的消息可以从异常实例 e.args[0] 获取。 版本 3.3 后改动：Connection 对象可以在进程间使用 Connection.send() 和 Connection.recv() 传输。版本 3.3 后添加：Connection 对象支持上下文管理协议。 Python 交互界面中，示例如下12345678910111213141516from multiprocessing import Pipea, b = Pipe()a.send([1, 'hello', None])b.recv()# [1, 'hello', None]b.send_bytes(b'thank you')a.recv_bytes()# b'thank you'import arrayarr1 = array.array('i', range(5))arr2 = array.array('i', [0] * 10)a.send_bytes(arr1)count = b.recv_bytes_into(arr2)assert count == len(arr1) * arr1.itemsizearr2# array('i', [0, 1, 2, 3, 4, 0, 0, 0, 0, 0]) 警告：connection.recv() 方法会自动反序列化接收的数据对象，这可能带来安全风险，所以要确认发送消息的进程是值得信任的。因此，除使用 Pipe() 生成的连接对象，均应该进行身份验证后再使用 recv() 和 send() 方法。参与 Authentication keys。警告：杀死正在从管道中读取数据或向管道中写入数据的进程可能导致管道中的数据被破坏，因为无法确定数据边界。 Synchronization primitives通常，同步原语在多进程程序中不像多线程程序中那么必要。参阅 threading 模块文档。可以通过是使用 manager 对象创建同步原语 - 参考 Manager。 class multiprocessing.Barrier(parties[, action[, timeout]])克隆自 threading.Barrier 的屏障对象。版本 3.3 后添加。 class multiprocessing.BoundedSemaphore([value])类似于 threading.BoundedSemaphore 的屏障信号量。与类似对象 threading.BoundedSemaphore 的细微不同：其 acquire 方法第一个参数是与 Lock.acquire() 方法一致的 block。MacOSX 系统上该对象与 Semaphore 无法区分，因为该平台未实现 sem_getvalue() 方法。 class multiprocessing.Condition([lock])类似于 threading.Condition 的条件变量。若指定了 lock 参数，参数应该是来自 multiprocessing 模块的 Lock 或 RLock 对象实例。版本 3.3 后添加了 wait_for() 方法。 class multiprocessing.Event克隆自 threading.Event。 class multiprocessing.Lock类似于 threading.Lock 的非递归锁。一旦进程/线程获得了锁，后续其他进程/线程尝试获取锁时会阻塞至锁被释放。任何进程/线程均可以释放它。除非另有说明，threading.Lock 对于线程的概念和行为同样适用于 multiprocessing.Lock 对于进程/线程。Lock 对象支持上下文管理协议，因此可以与 with 语句一起使用。 acquire(block=True, timeout=None)获取一个阻塞/非阻塞的锁。block 参数被设置为 True（默认值）该方法会阻塞至锁处于解锁状态，将其设置为上锁状态返回 True。注意此处第一个参数的名字与 threading.Lock.acquire() 方法中的第一个参数名字不同。若 block 参数设置为 False，该方法不会阻塞。若锁被上锁，返回 Fasle，否则将锁上锁返回 True。当使用正的浮点型数值 F 设置 timeout 参数时，其阻塞至获得锁的最长等待时间 F 秒。F 为负数时等效于为 0 的调用。若该参数设置为 None（默认值），即阻塞时间没有上限。注意此处对超时的负值或 None 的表现形式已与 threading.Lock.acquire() 不同。若 block 参数被设置为 False 该参数因没有实际意义将被忽略。若获得锁返回 True 否则阻塞时间超时时返回 False。 release()解锁。该方法不仅可以由当前占有锁的进程/线程调用，也可以由其他进程/线程调用。除在未上锁的锁上调用该方法引发 ValueError 异常外，其行为与 threading.Lock.release() 一致。 class multiprocessing.RLock类似于 threading.RLock 的递归锁。递归锁必须由获取它的进程/线程释放它。一旦进程/线程已经获取了它，同一进程/线程可以不阻塞的再次获得它；该进程/线程每次获取它均需要释放。注意 RLock 实际上是一个工厂函数，其返回使用默认上下文初始化的 multiprocessing.synchronize.RLock 实例。RLock 对象支持上下文管理协议，因此可以与 with 语句一起使用。 acquire(block=True, timeout=None)获取一个阻塞/非阻塞的锁。block 参数被设置为 True（默认值）该方法会阻塞至锁处于解锁状态，除非锁被当前进程/线程所有。当前进程/线程获得了锁的所有权（若之前还没有所有权），锁的内部递归级别加一并返回 True。注意，与 threading.RLock.acquire() 的实现相比，第一个参数名称不同，这里是函数本身。参数 block 设置为 False 时不阻塞。若锁已经被另一个进程/线程获取（即拥有），则当前进程/线程不会获得所有权，锁的内部递归级别也不会更改，返回 False。若锁处于解锁状态，则当前进程/线程获取锁的所有权且锁的内部递归级别加一，返回 True。timeout 参数的用法和行为与 Lock.acquire() 相同。注意，这里的超时行为与 threading.RLock.acquire() 的实际表现略有不同。 release()释放锁并将内部递归级别减一。在锁的递归级别变为零后，解锁（锁不为任何进程/线程拥有）。若有其他进程/线程在阻塞等待该锁，仅允许其中一个获得锁的所有权。若递归级别不为零，则锁保持上锁状态，并由当前调用进程/线程拥有。仅在锁的拥有者的进程/线程调用该方法。由锁的非拥有者调用该方法或锁处于非锁定状态（无主状态）时调用该方法会引发 AssertionError 异常。注意，该情况下抛出的异常类型与 threading.RLock.release() 实现的行为不同。 class multiprocessing.Semaphore([value])类似于 threading.Semaphore 的信号量对象。差异在于其 acquire 方法的第一个参数名是与 Lock.acquire() 一样的 block。注：MacOS X 系统不支持 sem_timedwait 函数，因此带有 timeout 参数调用 acquire() 方法将使用休眠循环模拟该行为。 注：若 Ctrl-C 信号量到达，而主线程在调用 BoundedSemaphore.acquire()、Lock.acquire()、RLock.acquire()、Semaphore.acquire()、Condition.acquire() 或 Condition.wait() 产生阻塞，会立即中断并抛出 KeyboardInterrupt 异常。这里与 threading 模块中行为不同，threading 模块会忽略 SIGINT 信号。注：该模块中的部分功能需要系统支持共享信号量。不支持的系统上会禁用 multiprocessing.synchronize 模块，继续引用会抛出 ImportError 异常。详细信息参阅 bpo-3770。 Shared ctypes Objects可以使用共享内存创建共享对象，共享内存可以由子进程继承。 multiprocessing.Value(typecode_or_type, *args, lock=True)返回从共享内存中分配的 ctypes 对象。默认返回的实际是对象的同步包装器。可以通过 Value 的 value 属性获取对象本身。typecode_or_type 决定了返回对象的类型：它是 ctypes 类型或是由 array 模块使用的类型字符串。 *args 被传递给对应类型的构造函数。若参数 lock 为 True（默认值），会创建一个递归锁以同步对该值的访问。如果 lock 是 Lock 或 RLock 对象类型，将用于对同步值权限的保护。若 lock 是 False，则锁不会自动保护对返回对象的访问，因此将不是进程安全的。例如像 += 这样涉及读写的操作不是原子性的。因此，想以原子性方式递增共享值仅以如下方式将不能实现。 1counter.value += 1加入相关锁是递归锁（默认情况）可以改为 12with counter.get_lock(): counter.value += 1 注意 lock 仅是关键字参数。 multiprocessing.Array(typecode_or_type, size_or_initializer, , lock=True*)返回从共享内存中分配的 ctypes 类型的数组。默认情况下，返回值实际是数组的同步包装器。typecode_or_type 决定了返回对象的类型：它是 ctypes 类型或是由 array 模块使用的类型字符串。若 size_or_initializer 是一个整数，它决定数组长度，且数组初识值为零。否则， size_or_initializer 是一个用于初始化数组的序列，其长度决定了数组的长度。若参数 lock 为 True（默认值），会创建一个锁以同步对该值的访问。如果 lock 是 Lock 或 RLock 对象类型，将用于对同步值权限的保护。若 lock 是 False，则锁不会自动保护对返回对象的访问，因此将不是进程安全的。注意 lock 仅是关键字参数。注意 ctypes.c_char 具有值和原始属性，允许用户存储和检测字符串。 multiprocessing.sharedctypes 模块multiprocessing.sharedctypes 模块提供了从共享内存分配 ctypes 类型对象的功能，这些对象可以被子进程继承。注：虽然可以在内存中存储指针，但要注意这将引用特定进程的地址空间中的位置。这样在其他进程的上下文环境中该指针可能无效，尝试在其他进程取消指针引用也可能导致进程的崩溃。 multiprocessing.sharedctypes.RawArray(type_or_type, size_or_initializer)返回从共享内存中分配的 ctypes 数组。 typecode_or_type 决定了返回数组的元素类型，其是一个 ctypes 类型或者 array 模块使用的一个类型字符串。若 size_or_initializer 是一个整数，它决定数组长度，且数组初识值为零。否则， size_or_initializer 是一个用于初始化数组的序列，其长度决定了数组的长度。 注：读写元素是非原子性的 - 使用 Array() 通过锁确保其实原子性的。 multiprocessing.sharedctypes.RawValue(typecode_or_type, *args)返回从共享内存中分配的 ctypes 类型对象。 typecode_or_type 决定了返回数组的元素类型，其是一个 ctypes 类型或者 array 模块使用的一个类型字符串。*args 参数将传递给该类型对象的构造函数。 注：读写元素是非原子性的 - 使用 Value() 通过锁确保其实原子性的。 注：ctypes.c_char 数组具有 value 和 raw 属性，可以通过该属性存储和检索字符串。 multiprocessing.sharedctypes.Array(typecode_or_type, size_or_initializer, , lock=True*)除需依赖锁类型变量确保线程安全，可以返回进程安全的修饰器而不是原始 ctypes 数组外，与 RawArray() 一样。 若参数 lock 为 True（默认值），会创建一个锁以同步对该值的访问。如果 lock 是 Lock 或 RLock 对象类型，将用于对同步值权限的保护。若 lock 是 False，则锁不会自动保护对返回对象的访问，因此将不是进程安全的。 注意 lock 仅是关键字参数。 multiprocessing.sharedctypes.Value(typecode_or_type, *args, lock=True)除需依赖锁类型变量确保线程安全，可以返回进程安全的修饰器而不是原始 ctypes 类型对象外，与 RawValue() 一样。 若参数 lock 为 True（默认值），会创建一个锁以同步对该值的访问。如果 lock 是 Lock 或 RLock 对象类型，将用于对同步值权限的保护。若 lock 是 False，则锁不会自动保护对返回对象的访问，因此将不是进程安全的。 注意 lock 仅是关键字参数。 multiprocessing.sharedctypes.copy(obj)返回从共享内存分配的 ctypes 类型对象，共享内存中的对象是对象 obj 的一个副本。 multiprocessing.sharedctypes.synchronized(obj[, lock])返回一个 ctypes 类型的进程安全包装器对象，使用了 lock 同步访问权限。若 lock 是 None（默认值）将自动创建 multiprocessing.RLock 类型对象。 同步的包装器除包装的对象外还有两个方法：get_obj() 返回包装过的对象；get_lock() 返回用于同步的锁对象。注：通过包装器访问 ctypes 对象比访问原始 ctypes 对象慢很多。版本 3.5 后同步对象支持上下文管理协议。 下表比较了使用不同 ctypes 语法从共享内存创建 ctypes 类型对象的语法。（表中 MyStruct 是 ctypes.Structure 的子类。 ctypes sharedctypes using type sharedctypes using typecode c_double(2.4) RawValue(c_double, 2.4) RawValue(&#39;d&#39;, 2.4) MyStruct(4, 6) RawValue(MyStruct, 4, 6) (c_short * 7)() RawArray(c_short, 7) RawArray(&#39;h&#39;, 7) (c_int * 3)(9, 2, 8) RawArray(c_int, (9, 2, 8)) RawArray(&#39;i&#39;, (9, 2, 8)) 下面是一个子进程改变 ctypes 类型数值的代码实例。1234567891011121314151617181920212223242526272829from multiprocessing import Process, Lockfrom multiprocessing.sharedctypes import Value, Arrayfrom ctypes import Structure, c_doubleclass Point(Structure): _fields_ = [('x', c_double), ('y', c_double)]def modify(n, x, s, A): n.value **= 2 x.value **= 2 s.value = s.value.upper() for a in A: a.x **= 2 a.y **= 2if __name__ == '__main__': lock = Lock() n = Value('i', 7) x = Value(c_double, 1.0/3.0, lock=False) s = Array('c', b'hello world', lock=lock) A = Array(Point, [(1.875,-6.25), (-5.75,2.0), (2.375,9.5)], lock=lock) p = Process(target=modify, args=(n, x, s, A)) p.start() p.join() # print result print(n.value) print(x.value) print(s.value) print([(a.x, a.y) for a in A]) 打印结果如下。1234490.1111111111111111HELLO WORLD[(3.515625, 39.0625), (33.0625, 4.0), (5.640625, 90.25)] ManagerManager 提供了一种在不同进程间共享创建的数据的方式，包括在不同计算机运行的进程之间通过网络共享。Manager 对象管理一个共享对象的服务进程，其他进程通过使用代理获取共享对象。 Multiprocessing.Manager()返回一个 SyncManager 对象，可以用来在进程间共享对象。返回的 manager 对象对应一个生成的子进程，该子进程会创建共享对象并返回相应的代理方法。manager 进程一旦被垃圾回收或其父进程退出就会关闭。manager 类的定义位于 multiprocessing.managers 模块。 multiprocessing.managers.BaseManager([address[, authkey]])创建 BaseManager 对象。创建后应该调用 start() 或 get_server().serve_forever() 方法保证 manager 对象引用的进程开启。address 是 manager 进程监听的新连接的地址，若该参数为 None 则系统随机分配一个对应于某些空闲端口号的地址。authkey 身份认证密钥，用于检查连接到服务进程的有效性。若该参数为 None 则使用 current_process().authkey。若使用该参数，其类型是 byte 字符串类型。 start([initializer[, initargs]])manager 开启子进程。若构造函数不为空，启动时调用构造函数 initializer(*initargs)。 get_server()返回 manager 对象控制下的真实服务器 Server，该类型支持 serve_forever() 方法。 1234from multiprocessing.managers import BaseManagermanager = BaseManager(address=('', 50000), authkey=b'abc')server = manaager.get_server()server.serve_forever() `Server` 类型具有 `address` 属性，由 `manager` 对象使用的地址。 **Connect**() 连接一个本地的 `manager` 对象到远程 `manager` 进程。 123from multiprocessing.managers import BaseManagerm = BaseManager(address=('127.0.0.1', 5000), authkey=b'abc')m.connect() shutdown()终止 manager 进程。该方法只有在服务进程执行过 start() 方法后才有效。该方法可以被多次调用。 register(typeid[, callable[, proxytype[, exposed[, method_to_typeid[, create_method]]]]])类方法，用于注册类型或调用 manager。 typeid 是类型标识符，用于识别共享对象的类型，字符串类型。 callable 是用于创建该类型对象的调用。若 manager 实例使用 connect() 方法连接到服务器或设置 create_method 参数为 False 该参数可以设置为 None。 proxytype 是 BaseProxy 的子类，用于为该类型的共享对象创建代理。若该参数为 None，则自动创建代理。 exposed 用于指定允许使用 BaseProxy.__callmethod() 方法访问该类型共享对象的一系列方法的名字。（若该参数为 None 且 proxytype._exposed_ 存在则使用后者替换前者。）若未指定，则共享的对象的所有公开方法均可以被访问。（这里公开方法指 __call__() 方法与其他不以 &#39;_&#39; 为名字开头的方法。） method_to_typeid 是指定了返回代理的公开方法应该返回的数据类型的映射。它将方法的名字映射到类型字符串。（若该参数为 None 且 proxytype._method_to_typeid_ 存在使用后者替换前者。）若方法的名字在该参数中不存在或者该参数为 None 则方法返回的对象按值复制。 create_method 决定是否构造方法用于通知服务进程创建共享对象并返回代理方法，默认为 True。 BaseManager 实例有一个只读属性： address由 manager 使用的地址。版本 3.3 后的变化：manager 对象支持上下文管理协议。 multiprocessing.managers.SyncManagerBaseManager 的子类，用于进程间同步。multiprocessing.Manager() 返回的对象类型。同样支持创建共享 List 和 Dict。 Barrier(parties[, action[, timeout]])创建共享 threading.Barrier 对象并返回其代理。3.3 版本后引入。 BoundedSemaphore([value])创建共享 threading.BoundedSemaphore 对象并返回其代理。 Condition([lock])创建共享 threading.Condition 对象并返回其代理。若创建的共享对象类型包含 lock则返回值是 threading.Lock 或 threading.RLock 对象。3.3 版本后添加了 wait_for() 方法。 Event()创建共享 threading.Event 对象并返回其代理。 Lock()创建共享 threading.Lock 对象并返回其代理。 Namespace()创建共享 Namespace 对象并返回其代理。 Queue([maxsize])创建共享 queue.Queue 对象并返回其代理。 RLock()创建共享 threading.RLock 对象并返回其代理。 Semaphore([value])创建共享 threading.Semaphore 对象并返回其代理。 Array(typecode, sequence)创建一个 Array 并返回其代理。 Value(typecode, value)创建带有可写属性 value 的对象并返回其代理。 dict() &amp; dict(mapping) &amp; dict(sequence)创建共享 dict 对象并返回其代理。 list() &amp; list(sequence)创建共享 list 对象并返回其代理。注：对 dict 或 list 的代理中可变值或可变项的修改可能不会通过 manager 传播，因为代理无法知道其值或项在何时被修改的。因此，要修改此类值或项，需要将修改后的对象重新赋值给 manager 包含的代理。 12345678910# create a list proxy and append a mutable object (a dictionary)lproxy = manager.list()lproxy.append(&#123;&#125;)# now mutate the dictionaryd = lproxy[0]d['a'] = 1d['b'] = 2# at this point, the changes to d are not yet synced, but by# reassigning the dictionary, the roxy is notified of the changelproxy[0] = d mulltiprocessing.managers.Namespace可以注册 SyncManager 的类型。一个 Namespace 对象没有公开方法，但有可写属性，其代表它属性的值。然后，当使用 Namespace 对象的代理时，其以 &#39;_&#39; 开头名字的属性是代理的属性而不是引用对象的属性。 1234567manager = multiprocessing.Manager()Global = manager.Namespace()Global.x = 10Global.y = 'hello'Global._z = 12.3 # this is an attribute of the proxyprint(Global)# Namespace(x=10, y='hello') 定制 manager可以通过创建 BaseManager 的子类创建自己的定制化 manager，使用 register() 类方法注册新类型或调用该类。实例如下 123456789101112131415161718from multiprocessing.managers import BaseManagerclass MathsClass: def add(self, x, y): return x + y def mul(self, x, y): return x * yclass MyManager(BaseManager): passMyManager.register('Maths', MathsClass)if __name__ == '__main__': with MyManager() as manager: maths = manager.Maths() print(maths.add(4, 3)) # prints 7 print(maths.mul(7, 8)) # prints 56 使用远程 manger可以在一台计算机上运行 manager 服务，另一台计算机上使用客户端使用该服务（假设防火墙允许）。 运行如下代码创建一个 manager 服务建立一个允许远程客户端访问的共享队列。 123456789from multiprocessing.managers import BaseManagerimport queuequeue = queue.Queue()class QueueManager(BaseManager): passQueueManager.r.Queue()class QueueManager(BaseManager): passQueueManager.regiress=('', 50000), authkey=b'abracadabra')s = m.get_server()s.serve_forever() 客户端获取服务实例代码如下 1234567from multiprocessing.managers import BaseManagerclass QueueManager(BaseManager): passQueueManager.register('get_queue')m = QueueManager(address=('foo.bar.org', 50000), authkey=b'abracadabra')m.connect()queue = m.get_queue()queue.put('hello') 另一个客户端可以获取上述客户端放入到队列中的信息，如下代码所示 12345678from multiprocessing.managers import BaseManagerclass QueueManager(BaseManager): passQueueManager.register('get_queue')m = QueueManager(address=('foo.bar.org', 50000), authkey=b'abracadabra')m.connect()queue = m.get_queue()queue.get()# output: 'hello' 本地也可以使用上述客户端的代码远程访问该队列 1234567891011121314151617181920212223from multiprocessing import Process, Queuefrom multiprocessing.managers import BaseManagerclass Worker(Process): def __init__(self, q): self.q = q super(Worker, self).__init__() def run(self): self.q.put('local hello')queue = Queue()w = Worker(queue)w.start()class QueueManager(BaseManager): passQueueManager.register('get_queue', callable=lambda: queue)m = QueueManager(address=('', 50000), authkey=b'abracadabra')s = m.get_server()s.serve_forever() Proxy代理是一个对象，其指向不同进程间（可能）存在的共享对象。共享对象被认为是代理指向的对象。多个代理对象可能指向同一个共享对象。代理对象有调用其引用对象方法的相应方法（虽然不是引用对象的所有方法均需通过代理调用）。代理通常可以如同引用对象一样使用，如下实例所示。1234567891011from multiprocessing import Managermanager = Manager()l = manager.list([i*i for i in range(10)])print(l)# [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]print(repr(l))# &lt;ListProxy object, typeid 'list' at 0x...&gt;l[4]# 16l[2:5]# [4, 9, 16] 注意使用 str() 返回的是饮用对象的，而 repr() 返回的是代理对象自身的。代理对象的一个重要特征是可序列化的因此可以在不同进程间传输。但是，如果将代理发送到对应的 manager 的进程反序列化将取消引用对象本身。这意味这一个共享对象可以包含第二个共享对象。12345678a = manager.list()b = manager.list()a.append(b) # referent of a now contains referent of bprint(a, b)# [[]] []b.append('hello')print(a, b)# [['hello']] ['hello'] multiprocessing 中的代理类型不支持值比较。因此对如下实例有1manager.list([1, 2, 3]) == [1, 2, 3] # False 因此进行比较操作时应使用引用对象的拷贝。 class multiprocessing.managers.BaseProxy代理对象是 BaseProxy 子类的实例。 _callmethod(methodname[, args[, kwds]])调用引用对象方法并返回执行结果。proxy 是一个引用对象是 obj 的代理，表达式 1proxy._callmethod(methodname, args, kwds) 等价于 manager 进程中的表达式 1getattr(obj, methodname)(*args, **kwds) 返回值是调用结果的拷贝或者新共享对象的代理 - method_to_typeid 参数参阅 BaseManager.register()。 若调用引发异常，将由 _call_method 方法重新引发该异常。若在 manger 进程中引发了其他异常，将其转换为 RemoteError 异常并由 _callmethod() 引发。 特别注意：若参数 methodname 是引用对象的非公开方法将抛出异常。 使用该方法的代码实例。 123456789l = manager.list(range(10))l._callmethod('__len__')# 10l._callmethod('__getitem__', (slice(2, 7),)) # equivalent to l[2:7]# [2, 3, 4, 5, 6]l._callmethod('__getitem__', (20,)) # equivalent to l[20]# Traceback (most recent call last):# ...# IndexError: list index out of range _getvalue()返回引用对象的拷贝。若引用对象不可以反序列化将引发异常。 repr()返回代理对象的表示。 str()返回引用对象的表示。 Cleanup代理对象使用的弱引用回调，当被垃圾回收时，会从拥有其引用的 manager 中注销自己。共享对象不再被任何代理对象引用时，从 manager 进程中删除。 Process Pool创建进程池，可以执行使用 Pool 类提交给它的任务。 class multiprocessing.pool.Pool([processes[, initializer[, initargs[, maxtasksperchild[, context]]]]])进程池对象，可以控制提交任务的工作进程池，支持异步获取超时或回调结果，具有并行映射的实现。processes 是使用的工作进程数量。若该参数为 None 使用 os.cpu_count() 返回的数字。若 initializer 不是 None 则在每个进程 start() 时调用该 initializer(*initargs) 函数。maxtasksperchild 是一个工作进程在退出或使用新工作进程取代之前可以完成的任务数，以释放不再使用的资源。默认值是 None 意味着工作进程生命周期与进程池一样。context 可用于指定工作进程 start() 的 context。通常通过使用 multiprocessing.Pool() 或者 context 对象的 Pool() 方法创建进程池。两种情景均可以使用 context 设置。注：进程池对象的方法仅可以由创建进程池的进程调用。版本 3.2 后添加了 maxtasksperchild。版本 3.4 后添加了 context。 注：工作进程的生命周期通常与进程池生命周期一直。在其他系统（如 Apache，mode_wsgi 等）存在频繁切换进程时，在进程退出或者新进程取代旧进程前，释放用于工作进程所拥有的资源的模式。maxtasksperchild 参数向用户公开了进程池的这种能力。 apply(func[, args[, kwds]])使用参数 args 和 关键字参数 kwds 调用 func 函数。会阻塞至任务执行结果就绪。apply_async() 方法更适合阻塞下的并行执行任务。此外，func 仅在进程池中的一个工作进程中执行。 apply_async(func[, args[, kwds[, callback[, error_callback]]]])apply() 方法的一个变体，返回一个结果对象。若指定了参数 callback，该参数值应该是带有一个参数的可调用函数。当结果就绪时，对结果调用该函数。若该情况下调用失败，error_callback 异常取代返回结果。若指定了参数 error_callback，该参数值是带有一个参数的可调用函数。当目标函数执行实行，则异常实例作为参数调用该函数。回调应该是很快完成的，否则处理结果线程将会阻塞。 map(func, iterable[, chunksize])内置 map() 的并行等价物（只支持一个 iterable 参数）。阻塞至结果就绪。该方法将迭代对象切分为多个块并将其作为单独任务提交到进程池。可以通过参数 chunksize 设置为正整数指定这些块的（近似）大小。 map_async(func, iterable[, chunksize[, callback[, error_callback]]])map() 方法的一个变种，返回一个结果对象。若指定了参数 callback，该参数值应该是带有一个参数的可调用函数。当结果就绪时，对结果调用该函数。若该情况下调用失败，error_callback 异常取代返回结果。若指定了参数 error_callback，该参数值是带有一个参数的可调用函数。当目标函数执行实行，则异常实例作为参数调用该函数。回调应该是很快完成的，否则处理结果线程将会阻塞。 imap(func, iterable[, chunksize])一个懒惰版本的 map()。参数 chunksize 与 map() 方法的使用相同。对于非常长的迭代对象使用一个较大的 chunksize 可以使任务比使用默认为 1 完成的快很多。此外，若 chunksize 为 1，则该方法返回的迭代器 next() 具有可选参数 timeout，若在超时时间内（单位为秒）没有返回，next(timeout) 将抛出 multiprocessing.TimeoutError 异常。 imap_unordered(func, iterable[, chunksize])除返回的迭代器被认为是无序的外，与 imap() 方法相同。（只有当只有一个工作进程时才能保证顺序是正确的） starmap(func, iterable[, chunksize])除 iterable 的非解包元素作为参数迭代外，与 map() 方法相同。因此，[(1, 2), (3, 4)] 的 iterable 参数值迭代为 [func(1, 2), func(3, 4)]。版本 3.3 后添加。 starmap_async(func, iterable[, chunksize[, callback[, error_back]]])starmap() 和 map_async() 方法的组合。迭代 iterable 中的非解包元素并调用 func 返回结果对象。版本 3.3 后添加。 close()防止更多任务被提交到进程池。提交到进程池的所有任务完成后，工作进程退出。 terminate()立即终止工作进程而不再继续执行未完成的任务。进程池对象被垃圾回收时直接调用该方法。 join()等待工作进程退出。必须在 close() 和 terminate() 方法调用前调用该方法。 版本 3.3 后进程池对象开始支持上下文管理协议。 class multiprocessing.pool.AsyncResult由 Pool.apply_async() 和 Pool.map_async() 返回结果的类。 get([timeout])返回到达时的结果。若 timeout 非 None 且结果没有在 timeout 时间内返回将抛出 multiprocessing.TimeoutError 异常。若远程调用引发了异常，将由该方法重新抛出该异常。 wait([timeout])等待至结果返回或超出 timeout 时间。 ready()返回调用是否完成。 successfult()返回调用完成是否抛出了异常。若结果还未就绪将抛出 AssertionError 异常。 下面的实例说明如何使用 Pool。1234567891011121314151617181920212223242526272829303132from multiprocessing import Poolimport timedef f(x): return x*xif __name__ == '__main__': # start 4 worker processes with Pool(processes=4) as pool: # evaluate "f(10)" asynchronously in a single process result = pool.apply_async(f, (10,)) # prints "100" unless your computer is *very* slow print(result.get(timeout=1)) # prints "[0, 1, 4,..., 81]" print(pool.map(f, range(10))) # prints "[0, 1, 4,..., 81]" it = pool.imap(f, range(10)) # prints "0" print(next(it)) # prints "1" print(next(it)) # prints "4" unless your computer is *very* slow print(it.next(timeout=1)) result = pool.apply_async(time.sleep, (10,)) # raises multiprocessing.TimeoutError print(result.get(timeout=1)) Listener &amp;&amp; Client通常进程间传递消息使用队列或由 Pipe() 方法返回的 Connection 对象。 此外，multiprocessing.connection 模块有更多的灵活性。它可以处理来自处理套接字或者 Windows 管道等 API 的更高级别消息。同样支持使用 hmac 模块进行数字认证，支持同时轮询多个连接。 multiprocessing.connection.deliver_challenge(connection, authkey)向另一端发送一个随机产生的消息并等待回复。若返回的回复使用秘钥作为键匹配数字认证，则向另一端发送欢迎消息。否则抛出 AuthenticationError 异常。 multiprocessing.connection.answer_challenge(connection, authkey)接收消息，然后使用 authkey 作为键计算出消息摘要发送回去。若没有收到欢迎消息将抛出 AuthenticationError 异常。 multiprocessing.connection.Client(address[, family[, authenticate[, authkey]]])使用 address 地址尝试建立到 Listener 的连接，返回 Connection 对象。连接的类型由参数 family 决定，但通常可以省略，因为可以通过地址的格式推断出来（参阅 Address Formats）若 authenticate 是 True 或者 authkey 是一个字节字符串，将使用摘要身份认证。用于认证的秘钥是 authkey（若参数为 None 则使用 current_process().authkey）。若认证失败抛出 AuthenticationError 异常。参阅 Authentication keys。 class multiprocessing.connection.Listener([address[, family[, backlog[, authenticate[, authkey]]]]])正在监听连接的包装器，绑定在套接字或 Windows 管道上。 address 是 Listener 对象用来绑定套接字或管道的地址。 地址 &#39;0.0.0.0&#39; 地址不能是 Windows 的可连接端点，应使用 &#39;127.0.0.1&#39;。 family 是使用的套接字（或管道）的类型，如下所示 &#39;AF_INET&#39; 对应 TCP 套接字； &#39;AF_UNIX&#39; 对应 Unix 套接字； &#39;AF_PIPE&#39; 对应 Windows 管道。 其中只有第一个是保证可用的。若该参数为 None 则会从地址格式推断。若 address 也是 None 选择默认值。默认值被假定为可以最快获取的。参阅 Address Formats。注意，若 family 是 &#39;AF_UNIX&#39; 且 address 为 None 将使用 tempfile.mkstemp() 在私有临时目录中创建套接字。若 Listener 对象使用套接字，一旦绑定该套接字，将向套接字的 listen() 方法发送 backlog（默认为1）。若 authenticate 为 True（默认为 False）或 authkey 不是 None 将使用摘要身份认证。若 authkey 是一个字节字符串，其将被用作认证秘钥；否则一定为 None。若 authkey 为 None 且 authenticate 为 True 将使用 current_process().authkey 作为认证秘钥。若 authkey 为 None 且 authenticate 为 False 将不会做认证。若认证失败抛出 AuthenticationError 异常。参阅 Authentication keys。 accept()尝试连接到 Listener 对象绑定的套接字或管道上，返回 Connection 对象。若尝试认证且认证失败，将抛出 AuthenticationError 异常。 close()关闭 Listener 对象绑定的套接字或管道。Listener 对象被垃圾回收时自动调用该方法。但是建议明确调用该方法。 Listener 对象还具有如下只读属性。addressListener 对象使用的地址。 last_accepted上次连接成功使用的地址。若该地址不可用为 None。 版本 3.3 后 Listener 对象支持上下文管理协议。 multiprocessing.connection.wait(object_list, timeout=None)等待 object_list 中的对象就绪。返回列表中准备就绪的对象的列表。若 timeout 是浮点数 F 该调用至多阻塞 F 秒。若 timeout 为 None 则阻塞时间没有上限。负数的参数等效于零。对 Unix 和 Windows 而言，出现在 object_list 中的对象需满足如下要求 可读 connection 对象； 可连接、可读的 socket.socket 对象； Process 对象的 sentinel 属性。当连接或套接字对象中的数据可以获取时，或者另一端已经关闭时，认为连接或套接字对象已就绪。 Unix：wait(object_list, timeout) 几乎等价于 select.select(object_list, [], [], timeout)。不同之处在于，select.select() 可以被信号中断，抛出错误码为 EINTR 的 OSError 异常，而 wait() 不会。Windows：object_list 中的对象必须是一个可以等待的整数句柄（根据 Win32 函数 WaitForMultipleObjects() 使用文档中的定义）或者是一个带有 fileno() 方法（该方法返回一个套接字句柄或管道句柄）的对象。（注意管道句柄和套接字句柄是不是可等待的句柄。） 版本 3.3 后添加。 Examples下面的服务端代码创建了一个 listener，其使用 secret_password 作为秘钥。之后等待连接并向客户端发送数据。1234567891011from multiprocessing.connection import Listenerfrom array import arrayaddress = ('localhost', 6000) # family is deduced to be 'AF_INET'with Listener(address, authkey=b'secret password') as listener: with listener.accept() as conn: print('connection accepted from', listener.last_accepted) conn.send([2.25, None, 'junk', float]) conn.send_bytes(b'hello') conn.send_bytes(array('i', [42, 1729])) 下面的代码代表客户端连接到服务端并从服务端接收数据。1234567891011from multiprocessing.connection import Clientfrom array import arrayaddress = ('localhost', 6000)with Client(address, authkey=b'secret password') as conn: print(conn.recv()) # =&gt; [2.25, None, 'junk', float] print(conn.recv_bytes()) # =&gt; 'hello' arr = array('i', [0, 0, 0, 0, 0]) print(conn.recv_bytes_into(arr)) # =&gt; 8 print(arr) # =&gt; array('i', [42, 1729, 0, 0, 0]) 下面的代码使用 wait() 方法同时等待来自多个进程的消息。1234567891011121314151617181920212223242526272829import time, randomfrom multiprocessing import Process, Pipe, current_processfrom multiprocessing.connection import waitdef foo(w): for i in range(10): w.send((i, current_process().name)) w.close()if __name__ == '__main__': readers = [] for i in range(4): r, w = Pipe(duplex=False) readers.append(r) p = Process(target=foo, args=(w,)) p.start() # We close the writable end of the pipe now to be sure that # p is the only process which owns a handle for it. This # ensures that when p closes its handle for the writable end, # wait() will promptly report the readable end as being ready. w.close() while readers: for r in wait(readers): try: msg = r.recv() except EOFError: readers.remove(r) else: print(msg) Address Formats AF_INET 地址是一个 (hostname, port) 格式的元组，hostname 是一个字符串，port 是一整数。 AF_UNIX 地址是代表文件系统中一个文件名字的字符串。 AF_PIPE 地址是一个 r&#39;\\.\pipe\PipeName 形式的字符串。使用 Client() 连接到远程计算机中名为 ServerName 的管道应使用 r&#39;\\ServerName\pipe\PipeName 替换。 注意任何以两个反斜杠开始的字符串会被默认为 AF_PIPE 地址而不是 AF_UNIX 地址。 Authentication keys当使用 Connection.recv 时，接收到的数据将会自动反序列化。不幸地是当反序列化的数据来自于非信任源时就会有安全风险。因此，Listener 和 Client() 使用 hmac 模块提供摘要认证。 一个可以当做密码的秘钥是一个字节类型的字符串：一旦连接建立，连接的双方均需要提供对方知道的秘钥。（即时两端使用相同的秘钥验证也不涉及通过连接发送秘钥） 若要求身份认证但没有提供秘钥，将使用当前进程的秘钥，该值自动继承创建该对象的当前进程的秘钥。这意味着（默认）所有的多进程程序的进程间通信使用同一个秘钥。 合适的秘钥可以通过使用 os.urandom() 产生。 Logging提供了部分日志支持。但要注意，logging 模块不使用进程共享锁，所以来自不同进程的信息可能混淆（取决于处理程序类型）。 multiprocessing.get_logger() 返回一个 multiprocessing 模块使用的日志记录器。若有必要，将会新建一个。 首次创建的日志记录器日志级别为 logging.NOTSET 且没有默认处理程序。日志信息默认不会传递到根日志记录器。 注：Windows 环境下子进程只会继承父进程的日志级别而不会继承任何其他定义。 multiprocessing.log_to_stderr() 该方法返回一个添加了日志处理程序的 get_logger() 方法调用，该日志处理程序使用 &quot;[%(levelname)s/%(processName)s] %(message)s&quot; 格式将消息发送到 sys.stderr。 下面是打开日志记录的一个代码实例。 123456789101112import multiprocessing, logginglogger = multiprocessing.log_to_stderr()logger.setLevel(logging.INFO)logger.warning('doomed')# [WARNING/MainProcess] doomedm = multiprocessing.Manager()# [INFO/SyncManager-...] child process calling self.run()# [INFO/SyncManager-...] created temp directory /.../pymp-...# [INFO/SyncManager-...] manager serving at '/.../listener-...'del m# [INFO/MainProcess] sending shutdown message to manager# [INFO/SyncManager-...] manager exiting with exitcode 0 有关日志记录级别的详细信息，请参阅 logging。 multiprocessing.dummy该模块复制了 multiprocessing 的 API，仅仅是 threading 模块的修饰器。 Programming Guidelines如下是使用 multiprocessing 模块时的一些指导原则和习惯用法。 Start()如下原则适用于所有的 start() 方法。 避免内存共享尽可能的避免在进程之间传递大量数据。进程间通信最好严格使用 queue 或 pipe 而不是使用级别较低的同步原语。 序列化保证代理方法的参数是可以序列化的。（可以被 pickle） 代理是线程安全的若代理对象来自多个线程，应该使用锁进行保护。（确保使用同一个代理的不同进程不会出现问题） 僵尸进程的 join() 方法Unix 中，当一个子进程完成退出没有调用 join() 方法而父进程在继续运行时，子进程成为僵尸进程。因为每一个新进程 start() （或调用 active_children() 方法），所有没有调用 join() 方法的终止进程将会 join()，所以僵尸进程的数量不会特别多。同时，调用一个已终止进程的 Process.is_alive 将会调用 join() 方法。即使如此，开始时明确进程的执行流程也是一个好习惯。 个人解读join() 方法具有清除僵尸进程的作用。通过下述源码我们可以发现，该方法通过子父进程的串行进程执行方式清除僵尸进程。join() 方法主要做了两件事情：① 通知父进程调用 wait() 方法；② 等待子进程终止或超时后，移除子进程。不带参数的 join() 方法若调用带有参数的 join(timeout) 方法，在代码中父进程等待 timeout 时长后，开始唤醒父进程，此时子父进程开始同时执行，父进程首先执行join(timeout) 方法，若此时子进程还未结束，则变量 res 获取的进程退出信息为空，即不会清除子进程，然后继续执行父进程的后续逻辑，此时子父进程在并行执行。若子进程再次终止后父任务还未结束，则父进程因无法获取到子进程的退出信息而导致子进程沦为僵尸进程。123456789101112131415161718# CPython 3.5.2 /Lib/multiprocessing/process.py BaseProcess 类class BaseProcess(object): ''' Process objects represent activity that is run in a separate process The class is analogous to `threading.Thread` ''' # ... ignore other methods def _Popen(self): raise NotImplementedError def join(self, timeout=None): ''' Wait until child process terminates ''' assert self._parent_pid == os.getpid(), 'can only join a child process' assert self._popen is not None, 'can only join a started process' res = self._popen.wait(timeout) if res is not None: _children.discard(self)12345678910111213141516171819202122232425262728293031# CPython 3.5.2 /Lib/multiprocessing/popen_fork.py Popen 类class Popen(object): method = 'fork' # ... ignore other methods def poll(self, flag=os.WNOHANG): if self.returncode is None: while True: try: pid, sts = os.waitpid(self.pid, flag) except OSError as e: # Child process not yet created. See #1731717 # e.errno == errno.ECHILD == 10 return None else: ​ break if pid == self.pid: if os.WIFSIGNALED(sts): ​ self.returncode = -os.WTERMSIG(sts) else: ​ assert os.WIFEXITED(sts) ​ self.returncode = os.WEXITSTATUS(sts) return self.returncode def wait(self, timeout=None): if self.returncode is None: if timeout is not None: from multiprocessing.connectionimport wait if not wait([self.sentinel], timeoout): ​ return None # This shouldn't block if wait() returned successfully. return self.poll(os.WNOHANG if timeout == 0.0 else 0) return self.returncode消除僵尸进程的方法：创建两次子进程即将父进程创建子进程的方式改为父进程先创建子进程代理进程，由代理进程创建子进程执行任务。代理进程在完成子进程的创建任务后，调用代理进程的 join() 方法消除（因为代理进程的执行时间很短）；同时，执行任务的子进程变成了孤儿进程，无论执行多久，最终被 init 进程（即进程号为 1 的进程）所收养，由 init 进程对其完成状态收集工作，避免了僵尸进程的产生。利用系统信号清除僵尸进程基于 Linux 信号方式添加代码 signal.signal(signal.SIGCHLD, signal.SIG_IGN)。signal.signal() 函数定义在收到信号时执行自定义的处理程序。此处是指忽略创建的子进程的退出信号，signal.SIGCHLD 在子进程状态改变后会产生此信号。sinal.SIG_IGN 是标准信号处理程序，简单忽略给定信号；默认使用 sinal.SIG_DFL，代表不理会给定信号，但也不丢弃。修改后不保存子进程的状态使之成为孤儿进程被 init 进程回收。此外，父进程退出时创建的僵尸进程也会被清除。 继承比使用 pickle/unpickle 更好使用 spawn 或者 forkserver 方式调用 start() 方法时，multiprocessing 中的许多类型需要被序列化给子进程使用它们。但是，通常应该避免使用 pipe 或 queue 将共享对象发送给其他进程。应该合理安排程序，使得需要访问其他进程创建的共享资源的进程通过从祖先进程继承的方式去访问。 避免主动终止进程使用 Process.terminate 方法可以终止进程，但可能导致进程当前使用的资源（如锁、信号量、管道和队列）被破坏或者不能被其他进程使用。因此，最好只有没有使用任何共享资源的进程上调用 Process.terminate 方法。 使用 queue 通信的进程调用 join()请记住，当一个进程调用 join() 方法时，Python 会检测被放入到 queue 中数据是否已经被全部删除（如 queue.get()，若没有被完全删除，进程将会一直等待，直到所有缓冲的数据由“feeder”线程将数据提供给底层管道。（子进程可以通过调用 Queue.cancel_join_thread 方法取消该行为。）这意味着，无论在什么时候使用 queue 都需要确保进程调用 join() 方法前放入到队列中的数据被全部清除。否则，无法确保将数据放入到队列中的进程已经终止。切记，非后台进程将会自动调用 join() 方法。下面是一个会造成死锁的代码示例。 123456789from multiprocessing import Process, Queuedef f(q): q.put('X' * 100000100000100000100000 if __name__ == '__main__': queue = Q = Process(target=f, args=(queue,)) p.start() p.join() # this deadlocks obj = queue.get() 一个改进办法是交换上述代码的最后两行（或简单删除 p.join() ） 将资源明确的传递给子进程在使用 fork 方式调用 start() 方法的 Unix 环境中，子进程可以使用父进程创建的全局资源中的所有共享资源，但最好将共享的对象作为参数传递给子进程的构造函数。这样除了可以使得代码（可能）兼容 Windows 平台和其他调用 start() 方法的方式外，同时还可以确保共享的对象在子进程仍处于活动状态时不会在父进程被垃圾回收。如果父进程垃圾回收时释放某些共享资源，这种方式就非常重要。如下代码示例应该被改写为后者。 123456789101112131415161718192021# 有问题的方式from multiprocessing import Process, Lockdef f(): # ... do something using "lock" ...if __name__ == '__main__': lock = Lock() for i in range(10): Process(target=f).start()# 更好的写法from multiprocessing import Process, Lockdef f(l): # ... do something using "l" ...if __name__ == '__main__': lock = Lock() for i in range(10): Process(target=f, args=(lock,)).start() 用文件对象取代 sys.stdin 时要小心multiprocessing 模块最初是无约束的调用 os.close(sys.stdin.fileno())。在 multiprocessing.Process._bootstrap() 方法中这样会导致子进程出现问题，修改如下 12sys.stdin.close()sys.stdin = open(os.open(os.devnull, os.O_RDONLY), closefd=False) 这样解决了进程相互冲突导致的文件描述符错误的基本问题，但对于带有输出缓冲的文件对象取代 sys.stdin() 的应用程序引入了潜在危险。危险是：如果在多个进程中调用同一个文件对象的 close() 方法多次，可能导致相同的数据被多次刷新到文件对象中，从而导致了数据损毁。如果需要自己实现文件对象及其缓存，则可以通过对缓存附加进程 PID 并在 PID 发生改变时丢弃缓存使其成为拷贝安全的。一个实现实例如下所示。 1234567@propertydef cache(self): pid = os.getpid() if pid != self._pid: self._pid = pid self._cache = [] return self._cache 更详细的信息可以阅读 bpo-5155，pbo-5313 和 bpo-5331。 spawn方式 &amp;&amp; forkserver方式此外还有些不适用于 fork 方式 start() 的其他约束。 更多序列化要求保证传给 Process.__init__() 方法的参数都是可以序列化的。同时也应该保证 Process 子类调用 Process.start() 方法时其实例也可以被序列化。 全局变量请记住，如果在子进程中尝试访问全局变量，则子进程看到的变量值（如果有）可能与父进程中调用 Process.start() 方法时的值不同。 main 模块的引用安全确保 Python 解释器可以安全的导入主模块而不会导致意外发生（如启动一个新进程时）。例如，使用 spawn 或 forkserver 方式调用 start() 方法运行下面的代码实例会导致 RuntimeError。 1234567from multiprocessing import Processdef foo(): print('hello')p = Process(target=foo)p.start() 应使用 if __name__ == &#39;__main__&#39;: 来包含程序入口，如下代码所示。 12345678910from multiprocessing import Process, freeze_support, set_start_methoddef foo(): print('hello')if __name__ == '__main__': freeze_support() set_start_method('spawn') p = Process(target=foo) p.start() （若程序实体正常运行而不需要冻结可以省略 freeze_support() 行代码。）这允许新生成的 Python 解释器可以安全导入模块，然后运行模块中 foo() 函数。在主模块中创建了 Pool 或 Manager 具有类似约束。 Examplesmanager如下是一个创建使用 manager 及其代理的实例。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980from multiprocessing import freeze_supportfrom multiprocessing.managers import BaseManager, BaseProxyimport operatorclass Foo: def f(self): print('you called Foo.f()') def g(self): print('you called Foo.g()') def _h(self): print('you called Foo._h()')# A simple generator functiondef baz(): for i in range(10): yield i*i# Proxy type for generator objectsclass GeneratorProxy(BaseProxy): _exposed_ = ['__next__'] def __iter__(self): return self def __next__(self): return self._callmethod('__next__')# Function to return the operator moduledef get_operator_module(): return operatorclass MyManager(BaseManager): pass# register the Foo class; make `f()` and `g()` accessible via proxyMyManager.register('Foo1', Foo)# register the Foo class; make `g()` and `_h()` accessible via proxyMyManager.register('Foo2', Foo, exposed=('g', '_h'))# register the generator function baz; use `GeneratorProxy` to make proxiesMyManager.register('baz', baz, proxytype=GeneratorProxy)# register get_operator_module(); make public functions accessible via proxyMyManager.register('operator', get_operator_module)def test(): manager = MyManager() manager.start() print('-' * 20) f1 = manager.Foo1() f1.f() f1.g() assert not hasattr(f1, '_h') assert sorted(f1._exposed_) == sorted(['f', 'g']) print('-' * 20) f2 = manager.Foo2() f2.g() f2._h() assert not hasattr(f2, 'f') assert sorted(f2._exposed_) == sorted(['g', '_h']) print('-' * 20) it = manager.baz() for i in it: print('&lt;%d&gt;' % i, end=' ') print() print('-' * 20) op = manager.operator() print('op.add(23, 45) =', op.add(23, 45)) print('op.pow(2, 94) =', op.pow(2, 94)) print('op._exposed_ =', op._exposed_)if __name__ == '__main__': freeze_support() test() Pool使用 Pool 的实例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149import multiprocessingimport timeimport randomimport sys## Functions used by test code#def calculate(func, args): result = func(*args) return '%s says that %s%s = %s' % ( multiprocessing.current_process().name, func.__name__, args, result )def calculatestar(args): return calculate(*args)def mul(a, b): time.sleep(0.5 * random.random()) return a * bdef plus(a, b): time.sleep(0.5 * random.random()) return a + bdef f(x): return 1.0 / (x - 5.0)def pow3(x): return x ** 3def noop(x): pass## Test code#def test(): PROCESSES = 4 print('Creating pool with %d processes\n' % PROCESSES) with multiprocessing.Pool(PROCESSES) as pool: # # Tests # TASKS = [(mul, (i, 7)) for i in range(10)] + \ [(plus, (i, 8)) for i in range(10)] results = [pool.apply_async(calculate, t) for t in TASKS] imap_it = pool.imap(calculatestar, TASKS) imap_unordered_it = pool.imap_unordered(calculatestar, TASKS) print('Ordered results using pool.apply_async():') for r in results: print('\t', r.get()) print() print('Ordered results using pool.imap():') for x in imap_it: print('\t', x) print() print('Unordered results using pool.imap_unordered():') for x in imap_unordered_it: print('\t', x) print() print('Ordered results using pool.map() --- will block till complete:') for x in pool.map(calculatestar, TASKS): print('\t', x) print() # # Test error handling # print('Testing error handling:') try: print(pool.apply(f, (5,))) except ZeroDivisionError: print('\tGot ZeroDivisionError as expected from pool.apply()') else: raise AssertionError('expected ZeroDivisionError') try: print(pool.map(f, list(range(10)))) except ZeroDivisionError: print('\tGot ZeroDivisionError as expected from pool.map()') else: try: print(list(pool.imap(f, list(range(10))1110))ro print('\tGot ZeroDiviionEionErrionErrrrionErriorrrrionError as expected from list(pool.imap())') else: raise AssertionError('expected ZeroDivisionError') it = pool.imap(f, list(range(10))) for i in range(10): try: x = next(it) except ZeroDivisionError: if i == 5: pass except StopIteration: break else: if i == 5: raise AssertionError('expected ZeroDivisionError') assert i == 9 print('\tGot ZeroDivisionError as expected from IMapIterator.next()') print() # # Testing timeouts # print('Testing ApplyResult.get() with timeout:', end=' ') res = pool.apply_async(calculate, TASKS[0]) while 1: sys.stdout.flush() try: sys.stdout.write('\n\t%s' % res.get(0.02)) break except multiprocessing.TimeoutError: sys.stdout.write('.') print() print() print('Testing IMapIterator.next() with timeout:', end=' ') it = pool.imap(calculatestar, TASKS) while 1: sys.stdout.flush() try: sys.stdout.write('\n\t%s' % it.next(0.02)) except StopIteration: break except multiprocessing.TimeoutError: sys.stdout.write('.') print() print()if __name__ == '__main__': multiprocessing.freeze_support() test() queue 下面的实例介绍了如何使用 queue 将任务分配给多个任务进程并收集任务结果。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677import timeimport randomfrom multiprocessing import Process, Queue, current_process, freeze_support## Function run by worker processes#def worker(input, output): for func, args in iter(input.get, 'STOP'): result = calculate(func, args) output.put(result)## Function used to calculate result#def calculate(func, args): result = func(*args) return '%s says that %s%s = %s' % \ (current_process().name, func.__name__, args, result)## Functions referenced by tasks#def mul(a, b): time.sleep(0.5*random.random()) return a * bdef pluus(a, b): time.sleep(0.5*random.random()) return a + b##def test(): NUMBER_OF_PROCESSES = 4 TASKS1 = [(mul, (i, 7)) for i in range(20)] TASKS2 = [(plus, (i, 8)) for i in range(10)] # Create queues task_queue = Queue() done_queue = Queue() # Submit tasks for task in TASKS1: task_queue.put(task) # Start worker processes for i in range(NUMBER_OF_PROCESSES): Process(target=worker, args=(task_queue, done_queue)).start() # Get and print results print('Unordered results:') for i in range(len(TASKS1)): print('\t', done_queue.get()) # Add more tasks using `put()` for task in TASKS2: task_queue.put(task) # Get and print some more results for i in range(len(TASKS2)): print('\t', done_queue.get()) # Tell child processes to stop for i in range(NUMBER_OF_PROCESSES): task_queue.put('STOP')if __name__ == '__main__': freeze_support() test() 后记在完成内存优化工作的尾声才完成了这篇文档的翻译工作。原以为较为简单的工作，进行中发现读懂的基础上翻译为较为合适的中文还是有不小的难度及工作量的。首先，需要通读完成理解统一概念之后才能对整篇文档有所了解；其次，翻译过程中，很多细节的概念或不常见的参数还需要花费时间查阅资料或阅读源码；最终初稿完成的基础上仍需多次阅读校正（该步骤还未完成，因此欢迎大家支持本文翻译的不当之处或错误，感谢~）。因此还是花费了较长的时间编辑本篇文档。当然，该工作对自己了解 Python 的一些机制尤其是与进程相关的一些设计思路，功能考虑等有了更深的认知和理解，若有暇可在后续整理出来供大家参考，共同讨论和进步。 注该篇博客参考文档为 Python 3.5.6 版本，源码版本为 CPython 3.5.2。 致谢本篇工作的完成还来自很多朋友的支持和帮助，如僵尸进程，孤儿进程等相关资料来自于网上博客的参考，CSDN 平台的 @逸辰杳；不同系统平台的子进程开启方式部分受到同事 @linquanisaac @Zhiya Zang @Mtax 的帮助；以及更多同事朋友的支持帮助~再次感谢他们。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>documentation</tag>
        <tag>Translation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GitHub Pages + Hexo 搭建博客]]></title>
    <url>%2F2018%2F11%2F01%2FGitHub-Pages-Hexo-%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[在决定写博客之后，我们首要面临的问题就是如何完成个人博客的初始化工作。此处，个人选择了 GitHub Pages + Hexo 的博客搭建方案。对博客方案选择过程有兴趣的或者有自身明确需求的读客可以阅读 博客的需求 和 博客搭建的选择 两部分以决定是否阅读本博客的方案，若决定使用与本博客同样的解决方案，可以直接阅读本博客的方案。 博客的需求对个人博客而言，我将需要具备的能力按重要性排列如下： 方便快捷编辑修改的能力 数据的存放 内容获取的便利性 其他方面 方便快捷编辑修改的能力我将方便快捷编辑修改的能力放置于个人博客需求的首位。 因为与其他网站不同，个人博客在追求形式美的同时，平时更多关注于内容的撰写与发表。具有较为理想的快速编辑与修改能力就显的很有必要，也减少了编写博客的阻力（编写能力弱无形中增加了写博客的惰性）。 其次，博客的内容在于他人的沟通交流或者自身理解程度的加深，需要回去修改或增加内容，所以快速定位博客及编辑修改的能力也要突出。 最后，编写功能操作越简单越好，推送发布的操作越少越好。降低每次写新博客或者编辑旧博客的复杂程度。 数据的存放对个人博客而言，数据存放是一件严肃认真的事情。数据的存放会包含很多方面，比如数据存放的位置，存放的平台，存放的格式等等；存放完还有数据是否自己可以完全掌控，是否容易迁移等等各种各样的问题。 对个人博客而言，一般而言，以文字为主，佐以少量图片或语音调色（可使用外链），因此一般数据量不大。访问的频率也不会太高。但写博客是一件长期的事情，所以要考虑数据存储的稳定性，数据备份迁移等行为不宜频率太高。 此处，我个人倾向于选择数据可以完全自己掌握的存储方式，如博客园，CSDN等博客网站，或如知乎，公众号等平台，首先会对你发表的内容进行审核，因此发表修改编辑有环节，不能所写所见；其次，数据导出或迁移甚至在无网络情况下，本地无法浏览更改；最后，这些数据的所有权归属于平台，因此在博客选择时我排查了上述类型的博客撰写方式。 内容获取的便利性个人博客的撰写目的当然是为了方便他人的阅读与交流，因此要方便他人的阅读，方便交流。当然，也可以通过获取方式对阅读人群进行初步筛选，如常见的新浪微博偏娱乐碎片化的信息记录交流；公众号稍微长一点，也是碎片化的信息；各种课程网站就比较长且完整。 其他方面除上述之外，可能还有很多原因或考虑促成你选择适合自己的博客。比如美观时尚要求，比如内容要求，经常分享自己的拍照视频等等各种各样的需求。按需选择适合自己的博客是促使可以长期坚持下去的首要条件，若是不合适的博客，在几次尝试撰写博客，或者在各大流行的博客间转换几次之后失去了兴趣，也就失去了写博客的初衷。因此，愿每个勇于尝试博客搭建的人都能很快选出适合的博客并坚持下去。 博客搭建的选择该部分大部分内容会参考自ONEGEE 博客 - 怎么选择和快速搭建个人博客 ，本人尝试过的方式会进行额外说明与补充。 该博主将博客按发布形式分为了三种：个人主页注册、静态网站生成与内容管理系统。 个人主页注册个人主页注册是指在现有的博客网站、论坛或社区上注册个人主页。优点是没有技术门槛，注册即用；拥有成熟的平台支撑，方便推广。缺点是风格单一，自定义程度低，还有许多形式与内容的限制。适合嫌麻烦不喜折腾而又不反感条条框框，对数据存储无感且不轻易迁移数据的人。 SegmentFault中文领域最大的编程问答交流社区平台，其所属杭州堆栈科技有限公司创立于2012年，目标是覆盖和服务中国软件开发者和 IT 信息从业者，充分利用在各个平台所能获得的各种技术创新机会为其开发产品应用和服务。 可以理解为中文的 StackOverFlow 社区，技术交流平台成熟。网站提供了文章专栏板块，有审核机制，支持 Markdown 文法、标签、评论、智能目录等。颜值中等的简洁风格。 因此，整体上 SegmentFault 平台计算机相关专业为主要阅读者。 简书简书自身定位为国内最优质的创作社区，2013年4月上线公测版本，正式开放注册。任何人可以在其上进行创作，相互交流。内容偏重于文字。同支持 Markdown 文法、标签、评论功能。颜值中等干净。 简书创始人简书个人主页，知乎ID简叔。对简书或其CEO个人有兴趣的可以去关注。 知乎中文互联网知名知识社交平台，创立于2011年1月26日，产品形态模仿自美国类似网站 Quora。用户通过问答等交流方式建立连接，偏娱乐化和专业知识，目前该公司包括知乎、知乎群组、知乎日报与公益壹点通四款应用。 提供的文章版本可以用作于博客撰写，提供 Markdown 文法、标签、评论功能。颜值中等偏大气的风格。 对知乎用户获取较为方便，但非知乎用户获取困难。大部分互联网人士了解该平台。 CSDNCSDN 全称是 Chinese Software Developer Network，创建于1999年，自身定位是中国专业 IT 社区，为中国的软件开发者提供知识传播、在线学习、职业发展等全生命周期服务。官方数据，截止2018年6月，CSDN 拥有2500+ 万技术会员，论坛发帖数 1000+ 万，技术资源 700+ 万，博客文章1300+ 万，新媒体矩阵粉丝数量 430+ 万。 老牌技术论坛，支持 Markdown 文法、标签、评论功能，文章管理方式传统。但目前 CSDN 的交互体验比较差，常被吐槽。颜值正常偏下水平。 博客园博客园创立于2004年1月，面向开发者的知识分享社区。自身定位为开发者打造纯净的技术交流区，推动并帮助开发者通过互联网分享知识。 老牌技术论坛，申请博客需人工审核，上班时间10分钟左右。支持 Markdown 文法、标签、评论、RSS、相册、文件等功能，文章管理方式传统。Logo 有多种选择，颜值正常，老式网站风格。 其他注册形式的博客还有很多，如网易(很快将迁移至 LOFTER)、新浪、搜狐博客等，甚至还有已经停止维护的博客网站，且大多数定位也不是技术类博客，此处没有介绍。此外，如微信订阅号、公众号，知乎问答、StackOverFlow或Quora、甚至百度贴吧等以问答形式完成博客的撰写也是不错的形式。 静态网站生成通常是指由 Jekyll、Hugo 或 Hexo 等技术生成静态网站，然后上传至 GitHub Pages、Coding Pages 等托管平台免费展示。具有一定的技术门槛，需要了解 Markdown 文法，简单 Linux 命令，域名解析，对要托管的平台如GitHub 或 Coding 有一定了解。 该类型的博客撰写或修改流程大致如下 本地以特定表头格式写博客，放于指定文件夹中 执行命令快速生成完整的静态网站 通过 Git 管理工具将文件上传至代码托管平台 该种博客搭建方式具有搭建快速、自定义程度高、主题丰富、技术更新迭代快、社区活跃的优势，同时具有一定的入坑门槛，适合有一定技术基础或喜欢折腾的用户，不同技术配置间迁移成本低。 Hexo一种基于 Node.js 的快速、简洁、高效的博客框架，GitHub代码库有 24k+ 的 Star（截止2018年11月12日）。安装过程顺利，配置、发布人性化，社区活跃，对技术不熟英文不好的人同样友善。主题多，选择空间大。可以通过插件形式支持博客所需功能。 Hugo一种基于 Go 语言实现的站点生成器，GitHub代码库有30K+的 Star（截止2018年11月12日）。安装过程较为顺利，中文社区不是很活跃。主题多，适合有一定技术基础有更高品味要求的用户。 JekyllGitHub 官方推荐的将纯文本转化为静态网站和博客的站点生成器。GitHub 联合创始人 Tom Preston-Werner 使用 Ruby 语言编写，在 GitHub代码库有35K+的 Star（截止2018年11月12日）。官方的加持，使得可以不依赖本地环境配置，直接在网站生成，本地环境配置较为麻烦。主题较好，相较于 Hexo 与 Hugo 较少。 内容管理系统内容管理系统指带有后台管理的博客系统，需要配置服务器、数据库以及域名管理，在此基础上安装内容管理系统。相较于静态网站生成而言，是动态博客，有前台后台之分，后台负责写作、发布、系统配置等。 这种方式具有贴心的后台管理功能，意味着具有出色的文章管理，相册管理，文件管理，而且在数据库基础上可以实现用户管理以及高清大图上传等，可以内置搜索、评论等常用功能。但同时，丰富的管理功能背后需要用户较高的技术基础，如 Web 相关的服务器知识，数据库知识等。与当下用户体验当道和扁平化时代相比，丰富又臃肿。 因此，对个人用户而言，使用该方式搭建博客，安全稳定，一次上手之后基本无需迁移，但可能需要有服务器的开销花费。若有多人维护，频繁更新的需求可以考虑该方式搭建博客。 WordPress一个开源的基于 PHP 和 MySQL 的个人发布系统。根源和开发可以追溯到2001年，社区活跃，遵循 GPL 协议。 具有较高的市场占有率，博客只是其功能之一，可以搭建企业级网站。中文友好，中文社区活跃。 ghost基于Node.js 实现的开源，旨在为新闻媒体构建开源解决方案。社区活跃，相比于 WordPress 而言，简洁大气，专为写作生产力的极致博客系统，便捷，可以随时随地撰写编辑博客，尤其在不同电脑上。WordPress 良好替换品，有一定搭建门槛。颜值在所有例子中最高（为颜值牺牲了一些功能）。 建议新手村指南：如果是新手，对于以上的技术门槛一窍不通，但是又想要主题精美的个人博客网站，建议从Markdown语言开始学起（半天入门，一天出师）。之后可以选择现有平台，简单上手，也可以稍微了解一些基本的命令行知识和 Git 操作，跟随各种教程，从生成静态网站入门快速搭建博客，完全不花钱。 对与我情况类似的读者，出身计算机相关专业或从事相关工作，可以考虑自己动手搭建。 首推 hexo。性价比最高，中文友好，快速上线，贴心配置，免费高颜值。 其次 WordPress。满足多人维护，资料繁多等需求，虽然门槛高较高，体量较大，且有额外花销，但稳定，可以对网址数据全掌握。 最后，内容高于形式，入坑需谨慎 。 个人博客最终选择了GitHub Pages + Hexo + NexT 的博客解决方案，博客地址：码农驿站 本博客的方案个人搭建技术博客，对颜值要求中等，期望数据完全掌握，由于一个人维护，又不希望维护的成本过高，因此选定了 Hexo 技术方案。其丰富的插件支持足以满足我对个人博客的需求。 在托管平台选取时，天然选择了 GitHub，即程序员交友网站，方便阅读和沟通交流。 GitHub Pages + Hexo + NexT 的方案搭建过程如下，其中可能遇到的问题我会在相应位置提及，但整体而言，较少遇到，搭起来很快。 参考了较多他人的博客与官方文档，资料如下 GitHub + Hexo 搭建个人网站详细教程 - From 知乎 Hexo 官网 环境准备这里的环境准备包括两部分，GitHub 的托管平台配置与本地环境的配置。与博客相关度不大的部分在这里介绍将会较为简略。 GitHub 配置GitHub 是目前最流行的代码仓库，得到了很多大公司与项目的青睐，为使得项目更方便的被人理解，需要项目的介绍页面甚至完整的技术文档，于是 GitHub Pages 服务应运而生，其不仅可以方便的为项目建立介绍站点，也可以用来建立个人博客。 GitHub Pages 属于轻量级的博客系统，配置简单；支持 Markdown 文法，编辑简单迅速；无需自己搭建服务器，GitHub 给每个站免费提供了 300MB 的空间（对文字而言足够）；可以绑定自己的域名。 配置流程大致如下： 购买、绑定独立域名 配置和使用 GitHub 注册账号 本地安装 Git Bash 配置 SSH Keys 设置实现免密登陆 测试联通成功并添加账号等相关信息 GitHub Pages 建立博客 - 注个人博客必须使用与 GitHub 用户名一样的名字，格式为 GitHubName.github.io 绑定域名到GitHub Pages 详细 GitHub Pages 配置过程可参考 使用 GitHub Pages 建立独立博客 - beiyunyun的博客，若与我情况类似，重新建立了新的 GitHub 账号以搭建博客，可能需要了解多个 GitHub 账号配置 SSH Key。 Hexo 本地环境配置Hexo 是基于 Node.js 的博客框架，因此需要首先安装 Node.js，Node.js 的安装包下载地址 安装成功后，通过命令行测试结果如下 1234~ $ node -vv8.4.0~ $ npm -v6.1.0 Hexo 安装较为简单，命令如下所示 1~ $ npm install hexo-cli -g 如上操作成功后，本地的环境基本准备完成，后续进行 Hexo 相关配置说明。 Hexo 配置Hexo 官网技术文档详细说明了如何上手，强烈推荐快速浏览一遍，可以对 Hexo 使用有一个大概了解，知道有哪些功能，方便后续功能的添加维护和更新。 此处基本按照 Hexo 搭建的流程进行配置。 建站安装 Hexo 完成后，执行如下命令，Hexo 将在指定的文件夹中新建所需文件 123~ $ hexo init &lt;blog-folder&gt; # 初始化名为 blog 的博客，可自行设置博客名~ $ cd &lt;blog-folder&gt;&lt;blog-folder&gt; $ npm install 新建完成后，指定文件夹下的目录结构如下所示 12345678.├── _config.yml # 网站的配置信息├── package.json # 应用程序信息，默认安装了 EJS，Stylus 和 Markdown Renderer，可自由移除├── scaffolds # 模板文件夹，Hexo 根据该文件建立文件├── source # 存放用户资源的地方，Markdown 和 HTML 文件被解析并放到 public 文件夹，其他被拷贝过去| ├── _drafts| └── _posts└── themes # 主题文件夹 网站的配置在 _config.yml 文件中，在此可以配置大部分的参数，常用及此次修改位置有网站、网址、目录、文章、分类&amp;标签、日期时间格式、分页、扩展（包括主题）等几个部分，配置文件中对应修改如下。 网站123456789# Sitetitle: 码农驿站 # 网站名，会在标签页上显示subtitle: 一枚码农的自述以供交流娱乐 # 副标题，网站名下面description: Just For Fun # 描述，主要用于SEO，告诉搜索引擎一个关于您站点的简单描述keywords: Codeauthor: 陈文嘉language: zh-Hans# 应填写 Asia/Shanghai，填写 CN 会报错 TypeError: Cannot read property 'utcOffset' of nulltimezone: Asia/Shanghai 网址123456# URL## If your site is put in a subdirectory, set url as 'http://yoursite.com/child' and root as '/child/'url: https://chenwenjia1991.github.ioroot: /permalink: :year/:month/:day/:title/permalink_defaults: 目录123456789# Directorysource_dir: source # 资源文件夹public_dir: public # 公共文件夹，用于存放生成的站点文件tag_dir: tags # 标签文件夹archive_dir: archives # 归档文件夹category_dir: categories # 分类文件夹code_dir: downloads/code # Include Code 文件夹i18n_dir: :lang # 国际化文件夹skip_render: # 跳过指定文件的渲染，可使用 glob表达式来匹配路径 Tips: 刚接触 Hexo，此部分一般不做修改。 文章12345678910111213141516171819# Writing# 新文章的文件名称，此处修改问 年-月-日-title.md 的方式方便检索new_post_name: :year-:month-:day-:title.md # File name of new posts# 预设布局default_layout: post# 中英文间加入空格 - 建议文章中自己添加titlecase: false # Transform title into titlecaseexternal_link: true # Open external links in new tabfilename_case: 0render_drafts: falsepost_asset_folder: falserelative_link: falsefuture: true# 代码块设置highlight: enable: true line_number: true auto_detect: false tab_replace: 分类 &amp; 标签博客添加分类和标签页，参考了 Hexo 使用攻略 - 添加分类及标签 From linlif 博客，添加新的页面也是如此，此处以添加 “categories” 页面为例，主要流程为 创建 “categories” 页面并添加 type 属性 1&lt;blog_folder&gt; $ hexo new page categories 成功后提示 1INFO Created: ~/Documents/blog/source/categories/index.md 正该 index.md 文件中，添加字段 type: &quot;categories&quot; 1234567&lt;blog_folder&gt; $ cat source/categories/index.md---title: categoriesdate: 2018-10-25 23:22:17comments: falsetype: "categories"--- 文章中添加 “categories” 属性即可，如下 123456---title: blog_namedate: ****categories:- category_name--- 对于今后的文章，基本都会在撰写时填写分类&amp;标签，可以通过修改 scaffolds/post.md 文件，在 tags: 上添加 categories: 后保存，之后执行 hexo new ** 产生的新文件就有分类选项了。 该文件是产生新博客时的模板，可以通过此文件设置默认的博客页面。 可能产生的问题 问题描述：GitHub Pages 结构混乱，而本地正常 解决方案：tag、category. 在主题的配置中必须选至少一个，即文档中存在的页面需要在配置文件打开，未打开出现上述异常。 扩展这里主要指主题，此处选择了 NexT 主题 NexT 主题配置NexT 主题现已支持十种语言，有四种外观，五套代码高亮主题，配置简单而丰富，已支持多种常见第三方服务，社区较为活跃，GitHub代码库有13K+的 Star（截止2018年11月13日）。官网的文档足够全面，基本可以满足个人博客的所有需求。 NexT 安装Hexo 安装主题的方式简单粗暴，只需将主题文件拷贝至站点目录 themes 目录下，然后修改配置文件中的 theme 配置即可。 从刚才的 GitHub代码库 位置拉取最新代码，或下载稳定版代码解压缩到站点 themes 目录下，并将解压后的文件夹名改为 next。 NexT 启用与其他 Hexo 主题启用方式一致，在 Hexo 配置文件中，theme 字段值修改为 next。 1234# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: next 切换主题后验证主题是否正确启用之前，最好使用 hexo clean 命令清除 Hexo 缓存。 验证主题启动 Hexo 本地站点并开启调试模式，命令是 hexo s --debug 。 服务启动过程中，注意观察命令行输出是否有任何异常信息，若碰到问题，这些信息可帮助更好的定位问题。 当命令行输出如下提示时，可以使用浏览器访问http://localhost:4000检查站点是否正常运行。 1INFO Hexo is running at http://0.0.0.0:4000/. Press Ctrl+C to stop. 当看到的站点外观与下图所示类似时说明已成功安装 NexT 主题。这是默认的 Schema -- Muse。 选择 SchemaSchema 是 NexT 提供的一种特性，以提供多种不同的外观，几乎所有配置均可以在不同 Scheme 之间功用。目前支持了四种 Scheme，在希望启用的 scheme 前去掉注释 #即可。 12345678# ---------------------------------------------------------------# Scheme Settings# ---------------------------------------------------------------# Schemes#scheme: Muse # 默认主题，NexT 的最初版，黑白主调，大量留白#scheme: Mist # Muse 的紧凑版本，整洁有序的单栏外观scheme: Pisces # 双栏，小家碧玉似的清新，此处我采用的外观#scheme: Gemini # 与 Pisces 类似 菜单设置 设定菜单内容，对应 menu 字段，设置格式为 item name: link || icon name。其中item name是名称，并不会直接显示在页面上，而是用于匹配图标及翻译。link目标连接。icon name是 FontAwesome Icon 的名字。 通过下述我的配置，即打开了home、tags、categories 与 archives，其他部分待打开进行设置。 1234567891011121314151617# ---------------------------------------------------------------# Menu Settings# ---------------------------------------------------------------# When running the site in a subdirectory (e.g. domain.tld/blog), remove the leading slash from link value (/archives -&gt; archives).# Usage: `Key: /link/ || icon`# Key is the name of menu item. If translate for this menu will find in languages - this translate will be loaded; if not - Key name will be used. Key is case-senstive.# Value before `||` delimeter is the target link.# Value after `||` delimeter is the name of FontAwesome icon. If icon (with or without delimeter) is not specified, question icon will be loaded.menu: home: / || home # 主页 # about: /about/ || user # 关于页面 tags: /tags/ || tags # 标签页 categories: /categories/ || th # 分类页 archives: /archives/ || archive # 归档页 # schedule: /schedule/ || calendar # sitemap: /sitemap.xml || sitemap # 站点地图 # commonweal: /404/ || heartbeat # 公益 404 设置菜单的显示文本。上述步骤1中的名称并不会用于界面上的展示。Hexo 在生成时实用该名字查找对应的翻译，并提取显示文本。这些翻译文本放置在 NexT 主题目录下的 languages/{language}.yml（{language}为自己所使用的语言）。 12345678910menu: home: 首页 archives: 归档 categories: 分类 tags: 标签 about: 关于 search: 搜索 schedule: 日程表 sitemap: 站点地图 commonweal: 公益 404 设定菜单项的图标，对应字段是menu_settings。 1234# Enable/Disable menu icons / item badges.menu_settings: icons: true badges: false 注：在菜单图标开启的情况下，如果菜单项与菜单未匹配（没有设置或者无效的 Font Awesome 图标名字） 的情况下，NexT 将会使用 ? 作为图标。 图标配置&amp;头像修改主题配置文件中的favicon 中，内容如下 12345678910111213141516# ---------------------------------------------------------------# Site Information Settings# ---------------------------------------------------------------# To get or check favicons visit: https://realfavicongenerator.net# Put your favicons into `hexo-site/source/` (recommend) or `hexo-site/themes/next/source/images/` directory.# Default NexT favicons placed in `hexo-site/themes/next/source/images/` directory.# And if you want to place your icons in `hexo-site/source/` root directory, you must remove `/images` prefix from pathes.# For example, you put your favicons into `hexo-site/source/images` directory.# Then need to rename &amp; redefine they on any other names, otherwise icons from Next will rewrite your custom icons in Hexo.favicon: small: /images/blog_logos/16x16.png medium: /images/blog_logos/32x32.png apple_touch_icon: /images/blog_logos/apple-icon-180x180.png safari_pinned_tab: /images/blog_logos/logo.svg android_manifest: /images/blog_logos/manifest.json ms_browserconfig: /images/blog_logos/browserconfig.xml 主题配置文件中的avatar 中，内容如下 123456789101112# Sidebar Avataravatar: # in theme directory(source/images): /images/avatar.gif # in site directory(source/uploads): /uploads/avatar.gif # You can also use other linking images. url: /images/blog_logos/avatar.gif # If true, the avatar would be dispalyed in circle. rounded: true # The value of opacity should be choose from 0 to 1 to set the opacity of the avatar. opacity: 1 # If true, the avatar would be rotated with the cursor. rotated: true 其图像建议放入博客的 source/images 中，这样将来修改或更换该图片均不会失效。 网站 Logo、头像等的制作网上有很多工具，此处不做过多说明。 集成评论模块Hexo NexT 主题下，支持较多的评论系统，具体可参看NexT 第三方服务集成。除评论系统外，还可以通过第三方服务增加数据统计与分析功能，内容分享服务，搜索服务，数学公式显示，Facebook SDK 支持，Google 站点管理工具等服务。 官方推荐的评论模块有 DISQUES、Facebook Comments、HyperComments、网易云跟帖、LiveRe几种。参考了知乎问题Hexo(NexT主题)评论系统哪个好？，根据其推荐，选择了 Gitalk 作为此处评论系统。 Gitalk 是一个基于 GitHub Issue 和 Preact 开发的评论插件，使用 GitHub 登陆，支持多语言，支持个人或组织，无干扰模式（设置 distractionFreeMode 为 true 开启），支持快捷键（cmd|ctrl + enter）提交。 此处工作参考了博客Hexo NexT 主题中集成 Gitalk 评论系统 - From asdfv1929‘s Home，配置流程如下 GitHub 中注册新应用，注册链接，填写内容如下 1234Application name # 应用名称Homepage URL # 网址，此处填 https://chenwenjia1991.github.io/Application description # 应用描述~Authorization callback URL # 授权回调网址， https://chenwenjia1991.github.io/ 点击注册后，保存 Client ID 和 Client Secret，在后面的配置中会用到 主题配置文件中配置 Gitalk，具体参数含义可参考详细参数列表 1234567891011# Gitalk# Introduction: https://github.com/gitalk/gitalk/blob/master/readme-cn.md gitalk: enable: true githubID: chenwenjia1991 # GitHub 账号 repo: Gitalk-Comment # 存储评论的仓库，可新建或使用旧项目，只写项目名称即可，项目需开启 issue ClientID: ** # 上述记录的 Client ID ClientSecret: ** # 上述 Client Secret adminUser: chenwenjia1991 #指定可初始化评论账户 perPage: 15 # 每页显示的最大评论数 distractionFreeMode: true # 全屏遮罩 通过 MD5 加密 ID 以解决 Label 长度不能超过50的问题，出现的具体问题描述及解决方案参考 Gitalk Issues 115，将JS脚本文件拷贝至 source/js/src/md5.min.js。 主题中进行相关配置 主题目录下/layout/_third-party/comments文件夹中 创建 gitalk.swig 文件并添加以下内容 12345678910111213141516171819&#123;% if page.comments &amp;&amp; theme.gitalk.enable %&#125; &lt;link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css"&gt; &lt;script src="https://unpkg.com/gitalk/dist/gitalk.min.js"&gt;&lt;/script&gt; &lt;script src="/js/src/md5.min.js"&gt;&lt;/script&gt; # &lt;— 添加的上述脚本的路径 &lt;script type="text/javascript"&gt; var gitalk = new Gitalk(&#123; clientID: '&#123;&#123; theme.gitalk.ClientID &#125;&#125;', clientSecret: '&#123;&#123; theme.gitalk.ClientSecret &#125;&#125;', repo: '&#123;&#123; theme.gitalk.repo &#125;&#125;', owner: '&#123;&#123; theme.gitalk.githubID &#125;&#125;', admin: ['&#123;&#123; theme.gitalk.adminUser &#125;&#125;'], id: md5(location.pathname), # &lt;— 使用上述脚本中的函数加密 distractionFreeMode: '&#123;&#123; theme.gitalk.distractionFreeMode &#125;&#125;' &#125;) gitalk.render('gitalk-container') &lt;/script&gt;&#123;% endif %&#125; 修改index.swig，在其末尾添加如下内容，将上述文件注册 1&#123;% include 'gitalk.swig' %&#125; 修改主题目录中 /layout/_partials/comments.swig文件，在 endif 语句前前添加如下内容 12&#123;% elseif theme.gitalk.enable %&#125;&lt;div id="gitalk-container"&gt;&lt;/div&gt; 主题目录下/source/css/_common/components/third-party文件夹中 新建 gitalk.styl，内容如下 1234.gt-header a, .gt-comments a, .gt-popup aborder-bottom: none;.gt-container .gt-popup .gt-action.is--active:beforetop: 0.7em; 修改third-party.styl文件，末尾添加如下代码 1@import "gitalk" if hexo-config('gitalk.enable'); 重新生成静态网页并推送至GitHub Pages hexo clean &amp;&amp; hexo g &amp;&amp; hexo d 站点统计使用了不蒜子统计，配置简单，在主题配置文件中设置busuanzi_count的enable的值为true即可。 数学公式支持主题配置文件中，设置mathjax的值为true即可。借助于 MathJax 显示数学公式。 图床在图片较多时，将图片上传至 GitHub Pages 已不再合适，毕竟有300MB的大小限制，此时考虑图床。此处参考国内外部分可用图床推荐对比-YiCH_ 简书，嗯，图片就交给它了-少数派图床推荐等相关资料，选取了腾讯云 COS 做图床，存储空间 50GB，外网下行 10GB，基本够用 - 有防盗链设置。基本满足了我们的需求。 站内搜索服务官网提供了 Swiftype、微搜索、Local Search、Algolia几种，其中 Swiftype 与 Algoliia 开始收费项目，故舍去。 此处选择了 Local Search，两种实现方式，一是本地建立索引；二是采用第三方线上服务。同样修改完重新生成网站并推送。 12345678910111213# 1. 安装 hexo-generator-searchdb，站点根目录下执行如下命令blog_path $ npm install hexo-generator-searchdb --save# 2. 编辑站点配置文件，新增如下内容# Local Searchsearch: path: search.xml field: post format: html limit: 10000# 3. 主题配置文件中，启动本地搜索，且可以修改相关配置# Local searchlocal_search: enable: true Markdown 本地编辑器个人目前博客撰写环境为 MacOS Mojave 10.14.1，因此主要查找尝试了 Mac 下的 Markdown 文本编辑器，参考知乎问题 Mac 上最好的 Markdown 文本编辑器是什么，其中 Typora 推荐数较高，优点是所见即所得，将写作和预览合二为一了。 TyporaTypora 是一款由 Abner Lee 开发的轻量级 Markdown 编辑器，适用于 OS X、Windows 和 Linux 三种操作系统，免费软件。与其他 Markdown 编辑器不同的是采用所见即所得的编辑方式实现了即时预览功能，也可切换至源代码编辑模式。 在编辑时，除了通过传统的 Markdown 代码的方式来实现富文本之外，Typora 支持通过菜单栏或者鼠标右键选取命令的方式来实现富文本，也支持通过快捷键的方式插入。Typora 也支持通过以 TeX 的格式来插入行间公式和行内公式。在完成编辑后导出文件时，Typora 支持以 PDF 或 HTML 的形式导出，如果安装了 Pandoc，也能够以Word、RTF、MediaWiki、LaTeX 等形式导出。Typora 提供有几种主题，并支持通过自定义 CSS 的方式进行个性化定制。 目前博客的编写我使用该软件完成，主要由于该目录独立，又不需要在多台电脑间同步。 印象笔记 &amp; 马克飞象印象笔记是我目前使用的笔记记录工具，两个月前开始支持 Markdown 文法，但整体来说，还处于测试阶段，用户体验有待提高。马克飞象是基于印象笔记的 Markdown 编辑工具，优点是数据存入印象笔记，因此用户可直接通过印象笔记阅读，但无法修改，流畅性好，稳定；缺点马克飞象的维护团队只有一个人，因此相比较与其他产品未完全成熟，如目前不支持 SSL/HTTPS 协议，存在安全隐患，功能添加较为缓慢。马克飞象是付费产品，79元/年。若 Markdown 编辑使用频率高，且需要在不同场合设备间切换，对数据安全性要求没有太高，推荐该产品。 Markdown 写作注意事项此处主要收集记录一些博客撰写过程中，应该注意的事项，可能会不断扩展，错误主要表现为静态网页无法产生。 行内代码引用时尽量使用一个反引号而不是三个反引号，否则容易引起格式混乱；行内代码中不引用百分号或其他转义符时容易引发 Hexo 生成静态网页的错误，如下所示 1234567# 使用一个反引号后转义失败 &#123;%code format error!%&#125; 引发如下错误INFO Start processingFATAL Something&apos;s wrong. Maybe you can find the solution here: http://hexo.io/docs/troubleshooting.htmlTemplate render error: (unknown path) unexpected end of file at Object._prettifyError (/Users/CWJ/Documents/Blog/blog_chenwenjia1991/node_modules/nunjucks/src/lib.js:36:11)... 通过 HTML 方式实现文内跳转 12345# 1. 定义 ID&lt;span id="jump"&gt;跳转去的地方&lt;/span&gt;# 2. 使用 Markdown 语法[点击跳转](#jump)eg.. [跳转至博客的需求](#1)效果如下 跳转至博客的需求 Markdown 引用（quote）中添加代码异常，如下所示会发现代码冲出现了引用符号，引用块中代码下方的引用块分离。Markdown渲染正常，在 Hexo 生成的页面中不一致，可通过如下方法解决(第二种方式更优雅方便)。同时，若此处出现类初始化函数的下划线会被 Hexo 渲染器识别为 HTML 元素再传递，Math 公式中同样浮现该问题，建议将渲染器更换。详情 Hexo-Theme-Next Issues 826。 12npm uninstall hexo-renderer-marked --savenpm install hexo-renderer-kramed --save 将代码块结尾的三个反引号与下一行文字之间的空行删除，用四个空格实现 使用 Hexo 官网给定的另一种插入代码块的方式，代码示例12345678&#123;% blockquote %&#125;上述代码在 Quote 中效果如下&#123;% codeblock lang:python %&#125;# test for code in quoteprint(&quot;Hello World!&quot;)&#123;% endcodeblock %&#125;&#123;% endblockquote %&#125;注：Hexo quote 中引用代码块，注意其 Markdown 解析器的不一致。 上述代码在 Quote 中效果如下 12# test for code in quoteprint("Hello World!") 注：Hexo quote 中引用代码块，注意其解析的不一致。 渲染器更换后，其渲染器配置改为在 &#39;_config.yml&#39; 文件的 kramed 域中进行。默认开启的智能引号转换在中文环境下会将英文引号自动转换为中文的，若不期望该行为发生，配置修改如下。参考 Hexo Issuer 1981。123# Kramed configkramed: smartypants: false 后记花费了一周多时间终于完成了本篇博客的撰写。一直在努力避免将一篇博客写的太细太长，不知不觉还是写了很长的篇幅，后续可能考虑拆分博客以为了更好的阅读体验，比如将 Hexo 相关部分独立。 感谢乐于分享的朋友，本文的很多工作也是在参考他们的博客下完成的；感谢读至此的朋友，完成这篇博客的阅读也花费了您不少的时间与耐心。 愿你我共同前行~]]></content>
      <categories>
        <category>DevelopTools</category>
      </categories>
      <tags>
        <tag>GitHub</tag>
        <tag>Hexo</tag>
        <tag>NexT</tag>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为什么开始写博客]]></title>
    <url>%2F2018%2F10%2F25%2F%E4%B8%BA%E4%BB%80%E4%B9%88%E5%BC%80%E5%A7%8B%E5%86%99%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[在2018年的第四个季度，在花费了将近一周的时间之后，最终搭起了自己的个人博客。 这是自己搭起来的第三个博客，前两个完成搭建工作之后逐渐无疾而终，在这次花费最多的时间与精力下，期望本次的博客可以长久不衰~ 为什么要写博客三年前，朋友向我推荐写博客，将自己的最近踩过的坑或者看过学到的知识做一总结分享出来，比如开发架构，比如算法分析，比如机器学习等等。一直觉得所学太浅不值得挂出来，拖拖拉拉许久也未成行。 直至硕士就业将近，完成了自己在 CSDN 的排序算法总结的第一篇博客，至今已有三年有余，500+的访问量。因各种各样的原因，在 CSDN 的只进行了一次就放弃了。第一次尝试，有收获也有失败之处。分享之后曾收到来自朋友的鼓励以及运行代码的朋友给与的反馈和建议，使得自己对于该部分的知识不仅掌握的更加扎实，理解更深入，同时自己的努力也给刚开始学习该部分知识的其他人带去帮助，也是一种喜悦。后来由于自己的原因以及 CSDN 编写博客的不畅，逐渐放弃了这种总结分享的方式。 工作之后，通过 GitHub Pages + Hexo + NexT 方式尝试搭建了个人博客。随工作进行，以及工作学习习惯的原因，更多笔记总结多放于印象笔记中，第二个博客渐渐也冷落许久。 最近随着团队技术积累的建设，团队完成了团队博客的搭建与初步撰写工作。工作以来，积累的很多东西都置于印象笔记中，需要更多的总结回顾与沉淀。因此用一周的时间完成了本博客的搭建及初始化工作。 此致，希望博客可以长久不衰！ 如何写博客什么是博客？个人将其当做记录自身学习成长过程的分享平台。其目的有三：一是回顾总结自身所学所理解认知；二是能给与我情况相似的人以启发，扬长避短；三是时时刻刻可以督促鞭策自己，砥砺前行。 基于此，本人博客基本围绕三点展开：遇到什么问题、如何去解决这个问题的、最终确定的解决方案。 定义问题在学习工作中，很多时候明确问题是很难的一件事情。也许，终其一生，很多事情我们应对之时总觉得哪里不舒服，却找不到问题的根源；也许，有时候发现自认为的问题原因却不是事情的根源所在，解决了问题不能使得事情完美；也许，更多事情，甚至不知道如何去抽象描述问题。 因此，定义问题是我们努力的探索的开始，也是如下工作的基石，问题定义的越明确清晰，解的范围越小越容易获得。 如何解决不同的问题，不同的思考角度，不同的利弊权衡，我们总会选出不同的解决方案。该部分我会尽量介绍自己是如何完成了一个问题解的寻求过程，以及遇到的解有哪些利弊，从哪些角度解决问题的。 解决方案这部分主要完成解的选择理由，以及解决问题的实践过程。当然，有时候这个问题无解也是一种解决方案。人相对于宇宙，未知总是生活的常态。 写博客的好处写博客有很多好处。开始写博客时，可能想到了如下这些，也许在博客的进行过程中，会陆陆续续添加一些新的益处与人分享。 更多的思考首先，写作就是一个深度思考的过程。很多在脑海中的事情，或者已经语言表达过的事情，用文字记录下来会是一个更深刻，更严肃的过程。相比较口语，文字在大多数情况下总有更强的逻辑性与说服力。因此，写博客的过程也是自己对所写内容进行思考回顾的过程。其次，事情完成之后记录下来，站在事情之外去看待问题的定义、解决过程会有更多收获，哪里做的好或者做的不好一目了然。更重要的是分享出来给更多人看到，他人会从更多的角度出发去接受理解这个问题，他们的思考与反馈会给自己更多考虑与帮助。 逼着自己去学习人很多时候都是有惰性的，学习的过程也不总会是顺风顺水开开心心的。因此很多时候人需要逼自己一把，越过高山。博客的撰写除了自己的记录总结，也是接受阅读的朋友们监督推动自己不断前进的过程。 学会坚持坚持做一件事情总是随着坚持时间的加长而愈加困难。学习博客恰是这样一个需要长久坚持的事情，期待自己在这方面可以做的更好更完美。 博客是一份好的简历毋庸置疑，博客相较于简历，更多体现你的知识体系，技能掌握情况等等。与简历相比，内容更丰富。当然，博客可以添加于自己的简历之上，给别人或者工作岗位确认你是否是需要的那个人~获得更匹配的岗位。 知名度提升技术道路上认识更多志同道合的人，除了在碰到一个问题时，多了可以询问咨询解决方案的通道，多了共同探索学习的朋友，你自己也会在技术圈的知名度逐步提升。 是否需要一个个人博客每个人都可以搞一个博客，无论这个博客会对公对私。记录自己的生活学习成长，吸取经验和教训，总是为了让明天的自己成为更好的自己。 后记在开源思潮日益兴盛的今天，博客搭建的难度越来越低，我们可以更关注于我们想做的事情而不是工具的选择。后续，回把自己博客的搭建过程放出来供搭建选择参考。当然也大不必将精力放在博客上太多，更多的我们要关注博客的内容，见证自己的成长。]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>杂记</tag>
      </tags>
  </entry>
</search>
